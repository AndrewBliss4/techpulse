[
    {
        "id": "2503.11619v1",
        "title": "Tit-for-Tat: Safeguarding Large Vision-Language Models Against Jailbreak Attacks via Adversarial Defense",
        "authors": [
            "Shuyang Hao",
            "Yiwei Wang",
            "Bryan Hooi",
            "Ming-Hsuan Yang",
            "Jun Liu",
            "Chengcheng Tang",
            "Zi Huang",
            "Yujun Cai"
        ],
        "published": "2025-03-14T17:39:45Z",
        "summary": "Deploying large vision-language models (LVLMs) introduces a unique vulnerability: susceptibility to malicious attacks via visual inputs. However, existing defense methods suffer from two key limitations: (1) They solely focus on textual defenses, fail to directly address threats in the visual domain where attacks originate, and (2) the additional processing steps often incur significant computational overhead or compromise model performance on benign tasks. Building on these insights, we propose ESIII (Embedding Security Instructions Into Images), a novel methodology for transforming the visual space from a source of vulnerability into an active defense mechanism. Initially, we embed security instructions into defensive images through gradient-based optimization, obtaining security instructions in the visual dimension. Subsequently, we integrate security instructions from visual and textual dimensions with the input query. The collaboration between security instructions from different dimensions ensures comprehensive security protection. Extensive experiments demonstrate that our approach effectively fortifies the robustness of LVLMs against such attacks while preserving their performance on standard benign tasks and incurring an imperceptible increase in time costs.",
        "field": "Quantum Computing",
        "link": "http://arxiv.org/abs/2503.11619v1"
    },
    {
        "id": "2503.11573v1",
        "title": "Synthesizing Access Control Policies using Large Language Models",
        "authors": [
            "Adarsh Vatsa",
            "Pratyush Patel",
            "William Eiers"
        ],
        "published": "2025-03-14T16:40:25Z",
        "summary": "Cloud compute systems allow administrators to write access control policies that govern access to private data. While policies are written in convenient languages, such as AWS Identity and Access Management Policy Language, manually written policies often become complex and error prone. In this paper, we investigate whether and how well Large Language Models (LLMs) can be used to synthesize access control policies. Our investigation focuses on the task of taking an access control request specification and zero-shot prompting LLMs to synthesize a well-formed access control policy which correctly adheres to the request specification. We consider two scenarios, one which the request specification is given as a concrete list of requests to be allowed or denied, and another in which a natural language description is used to specify sets of requests to be allowed or denied. We then argue that for zero-shot prompting, more precise and structured prompts using a syntax based approach are necessary and experimentally show preliminary results validating our approach.",
        "field": "Quantum Computing",
        "link": "http://arxiv.org/abs/2503.11573v1"
    },
    {
        "id": "2503.11404v1",
        "title": "Towards A Correct Usage of Cryptography in Semantic Watermarks for Diffusion Models",
        "authors": [
            "Jonas Thietke",
            "Andreas Müller",
            "Denis Lukovnikov",
            "Asja Fischer",
            "Erwin Quiring"
        ],
        "published": "2025-03-14T13:45:46Z",
        "summary": "Semantic watermarking methods enable the direct integration of watermarks into the generation process of latent diffusion models by only modifying the initial latent noise. One line of approaches building on Gaussian Shading relies on cryptographic primitives to steer the sampling process of the latent noise. However, we identify several issues in the usage of cryptographic techniques in Gaussian Shading, particularly in its proof of lossless performance and key management, causing ambiguity in follow-up works, too. In this work, we therefore revisit the cryptographic primitives for semantic watermarking. We introduce a novel, general proof of lossless performance based on IND\\$-CPA security for semantic watermarks. We then discuss the configuration of the cryptographic primitives in semantic watermarks with respect to security, efficiency, and generation quality.",
        "field": "Quantum Computing",
        "link": "http://arxiv.org/abs/2503.11404v1"
    },
    {
        "id": "2503.11216v1",
        "title": "Cross-Platform Benchmarking of the FHE Libraries: Novel Insights into SEAL and Openfhe",
        "authors": [
            "Faneela",
            "Jawad Ahmad",
            "Baraq Ghaleb",
            "Sana Ullah Jan",
            "William J. Buchanan"
        ],
        "published": "2025-03-14T09:08:30Z",
        "summary": "The rapid growth of cloud computing and data-driven applications has amplified privacy concerns, driven by the increasing demand to process sensitive data securely. Homomorphic encryption (HE) has become a vital solution for addressing these concerns by enabling computations on encrypted data without revealing its contents. This paper provides a comprehensive evaluation of two leading HE libraries, SEAL and OpenFHE, examining their performance, usability, and support for prominent HE schemes such as BGV and CKKS. Our analysis highlights computational efficiency, memory usage, and scalability across Linux and Windows platforms, emphasizing their applicability in real-world scenarios. Results reveal that Linux outperforms Windows in computation efficiency, with OpenFHE emerging as the optimal choice across diverse cryptographic settings. This paper provides valuable insights for researchers and practitioners to advance privacy-preserving applications using FHE.",
        "field": "Quantum Computing",
        "link": "http://arxiv.org/abs/2503.11216v1"
    },
    {
        "id": "2503.11185v1",
        "title": "Align in Depth: Defending Jailbreak Attacks via Progressive Answer Detoxification",
        "authors": [
            "Yingjie Zhang",
            "Tong Liu",
            "Zhe Zhao",
            "Guozhu Meng",
            "Kai Chen"
        ],
        "published": "2025-03-14T08:32:12Z",
        "summary": "Large Language Models (LLMs) are vulnerable to jailbreak attacks, which use crafted prompts to elicit toxic responses. These attacks exploit LLMs' difficulty in dynamically detecting harmful intents during the generation process. Traditional safety alignment methods, often relying on the initial few generation steps, are ineffective due to limited computational budget. This paper proposes DEEPALIGN, a robust defense framework that fine-tunes LLMs to progressively detoxify generated content, significantly improving both the computational budget and effectiveness of mitigating harmful generation. Our approach uses a hybrid loss function operating on hidden states to directly improve LLMs' inherent awareness of toxity during generation. Furthermore, we redefine safe responses by generating semantically relevant answers to harmful queries, thereby increasing robustness against representation-mutation attacks. Evaluations across multiple LLMs demonstrate state-of-the-art defense performance against six different attack types, reducing Attack Success Rates by up to two orders of magnitude compared to previous state-of-the-art defense while preserving utility. This work advances LLM safety by addressing limitations of conventional alignment through dynamic, context-aware mitigation.",
        "field": "Quantum Computing",
        "link": "http://arxiv.org/abs/2503.11185v1"
    },
    {
        "id": "2503.06279v1",
        "title": "Mitigating Blockchain extractable value (BEV) threats by Distributed Transaction Sequencing in Blockchains",
        "authors": [
            "Xiongfei Zhao",
            "Hou-Wan Long",
            "Zhengzhe Li",
            "Jiangchuan Liu",
            "Yain-Whar Si"
        ],
        "published": "2025-03-08T16:55:52Z",
        "summary": "The rapid growth of Blockchain and Decentralized Finance (DeFi) has introduced new challenges and vulnerabilities that threaten the integrity and efficiency of the ecosystem. This study identifies critical issues such as Transaction Order Dependence (TOD), Blockchain Extractable Value (BEV), and Transaction Importance Diversity (TID), which collectively undermine the fairness and security of DeFi systems. BEV-related activities, including Sandwich attacks, Liquidations, and Transaction Replay, have emerged as significant threats, collectively generating $540.54 million in losses over 32 months across 11,289 addresses, involving 49,691 cryptocurrencies and 60,830 on-chain markets. These attacks exploit transaction mechanics to manipulate asset prices and extract value at the expense of other participants, with Sandwich attacks being particularly impactful. Additionally, the growing adoption of Blockchain in traditional finance highlights the challenge of TID, where high transaction volumes can strain systems and compromise time-sensitive operations. To address these pressing issues, we propose a novel Distributed Transaction Sequencing Strategy (DTSS), which combines forking mechanisms and the Analytic Hierarchy Process (AHP) to enforce fair and transparent transaction ordering in a decentralized manner. Our approach is further enhanced by an optimization framework and the introduction of the Normalized Allocation Disparity Metric (NADM), which ensures optimal parameter selection for transaction prioritization. Experimental evaluations demonstrate that DTSS effectively mitigates BEV risks, enhances transaction fairness, and significantly improves the security and transparency of DeFi ecosystems. This work is essential for protecting the future of decentralized finance and promoting its integration into global financial systems.",
        "field": "Decentralized Finance (DeFi)",
        "link": "http://arxiv.org/abs/2503.06279v1"
    },
    {
        "id": "2503.04850v2",
        "title": "Slow is Fast! Dissecting Ethereum's Slow Liquidity Drain Scams",
        "authors": [
            "Minh Trung Tran",
            "Nasrin Sohrabi",
            "Zahir Tari",
            "Qin Wang",
            "Xiaoyu Xia"
        ],
        "published": "2025-03-06T02:24:35Z",
        "summary": "We identify the slow liquidity drain (SLID) scam, an insidious and highly profitable threat to decentralized finance (DeFi), posing a large-scale, persistent, and growing risk to the ecosystem. Unlike traditional scams such as rug pulls or honeypots (USENIX Sec'19, USENIX Sec'23), SLID gradually siphons funds from liquidity pools over extended periods, making detection significantly more challenging. In this paper, we conducted the first large-scale empirical analysis of 319,166 liquidity pools across six major decentralized exchanges (DEXs) since 2018. We identified 3,117 SLID affected liquidity pools, resulting in cumulative losses of more than US$103 million. We propose a rule-based heuristic and an enhanced machine learning model for early detection. Our machine learning model achieves a detection speed 4.77 times faster than the heuristic while maintaining 95% accuracy. Our study establishes a foundation for protecting DeFi investors at an early stage and promoting transparency in the DeFi ecosystem.",
        "field": "Decentralized Finance (DeFi)",
        "link": "http://arxiv.org/abs/2503.04850v2"
    },
    {
        "id": "2503.01944v1",
        "title": "Protecting DeFi Platforms against Non-Price Flash Loan Attacks",
        "authors": [
            "Abdulrahman Alhaidari",
            "Balaji Palanisamy",
            "Prashant Krishnamurthy"
        ],
        "published": "2025-03-03T18:18:05Z",
        "summary": "Smart contracts in Decentralized Finance (DeFi) platforms are attractive targets for attacks as their vulnerabilities can lead to massive amounts of financial losses. Flash loan attacks, in particular, pose a major threat to DeFi protocols that hold a Total Value Locked (TVL) exceeding \\$106 billion. These attacks use the atomicity property of blockchains to drain funds from smart contracts in a single transaction. While existing research primarily focuses on price manipulation attacks, such as oracle manipulation, mitigating non-price flash loan attacks that often exploit smart contracts' zero-day vulnerabilities remains largely unaddressed. These attacks are challenging to detect because of their unique patterns, time sensitivity, and complexity. In this paper, we present FlashGuard, a runtime detection and mitigation method for non-price flash loan attacks. Our approach targets smart contract function signatures to identify attacks in real-time and counterattack by disrupting the attack transaction atomicity by leveraging the short window when transactions are visible in the mempool but not yet confirmed. When FlashGuard detects an attack, it dispatches a stealthy dusting counterattack transaction to miners to change the victim contract's state which disrupts the attack's atomicity and forces the attack transaction to revert. We evaluate our approach using 20 historical attacks and several unseen attacks. FlashGuard achieves an average real-time detection latency of 150.31ms, a detection accuracy of over 99.93\\%, and an average disruption time of 410.92ms. FlashGuard could have potentially rescued over \\$405.71 million in losses if it were deployed prior to these attack instances. FlashGuard demonstrates significant potential as a DeFi security solution to mitigate and handle rising threats of non-price flash loan attacks.",
        "field": "Decentralized Finance (DeFi)",
        "link": "http://arxiv.org/abs/2503.01944v1"
    },
    {
        "id": "2502.11521v1",
        "title": "DeFiScope: Detecting Various DeFi Price Manipulations with LLM Reasoning",
        "authors": [
            "Juantao Zhong",
            "Daoyuan Wu",
            "Ye Liu",
            "Maoyi Xie",
            "Yang Liu",
            "Yi Li",
            "Ning Liu"
        ],
        "published": "2025-02-17T07:45:03Z",
        "summary": "DeFi (Decentralized Finance) is one of the most important applications of today's cryptocurrencies and smart contracts. It manages hundreds of billions in Total Value Locked (TVL) on-chain, yet it remains susceptible to common DeFi price manipulation attacks. Despite state-of-the-art (SOTA) systems like DeFiRanger and DeFort, we found that they are less effective to non-standard price models in custom DeFi protocols, which account for 44.2% of the 95 DeFi price manipulation attacks reported over the past three years. In this paper, we introduce the first LLM-based approach, DeFiScope, for detecting DeFi price manipulation attacks in both standard and custom price models. Our insight is that large language models (LLMs) have certain intelligence to abstract price calculation from code and infer the trend of token price changes based on the extracted price models. To further strengthen LLMs in this aspect, we leverage Foundry to synthesize on-chain data and use it to fine-tune a DeFi price-specific LLM. Together with the high-level DeFi operations recovered from low-level transaction data, DeFiScope detects various DeFi price manipulations according to systematically mined patterns. Experimental results show that DeFiScope achieves a high precision of 96% and a recall rate of 80%, significantly outperforming SOTA approaches. Moreover, we evaluate DeFiScope's cost-effectiveness and demonstrate its practicality by helping our industry partner confirm 147 real-world price manipulation attacks, including discovering 81 previously unknown historical incidents.",
        "field": "Decentralized Finance (DeFi)",
        "link": "http://arxiv.org/abs/2502.11521v1"
    },
    {
        "id": "2502.13167v1",
        "title": "SmartLLM: Smart Contract Auditing using Custom Generative AI",
        "authors": [
            "Jun Kevin",
            "Pujianto Yugopuspito"
        ],
        "published": "2025-02-17T06:22:05Z",
        "summary": "Smart contracts are essential to decentralized finance (DeFi) and blockchain ecosystems but are increasingly vulnerable to exploits due to coding errors and complex attack vectors. Traditional static analysis tools and existing vulnerability detection methods often fail to address these challenges comprehensively, leading to high false-positive rates and an inability to detect dynamic vulnerabilities. This paper introduces SmartLLM, a novel approach leveraging fine-tuned LLaMA 3.1 models with Retrieval-Augmented Generation (RAG) to enhance the accuracy and efficiency of smart contract auditing. By integrating domain-specific knowledge from ERC standards and employing advanced techniques such as QLoRA for efficient fine-tuning, SmartLLM achieves superior performance compared to static analysis tools like Mythril and Slither, as well as zero-shot large language model (LLM) prompting methods such as GPT-3.5 and GPT-4. Experimental results demonstrate a perfect recall of 100% and an accuracy score of 70%, highlighting the model's robustness in identifying vulnerabilities, including reentrancy and access control issues. This research advances smart contract security by offering a scalable and effective auditing solution, supporting the secure adoption of decentralized applications.",
        "field": "Decentralized Finance (DeFi)",
        "link": "http://arxiv.org/abs/2502.13167v1"
    },
    {
        "id": "2503.09823v1",
        "title": "Data Traceability for Privacy Alignment",
        "authors": [
            "Kevin Liao",
            "Shreya Thipireddy",
            "Daniel Weitzner"
        ],
        "published": "2025-03-12T20:42:23Z",
        "summary": "This paper offers a new privacy approach for the growing ecosystem of services--ranging from open banking to healthcare--dependent on sensitive personal data sharing between individuals and third-parties. While these services offer significant benefits, individuals want control over their data, transparency regarding how their data is used, and accountability from third-parties for misuse. However, existing legal and technical mechanisms are inadequate for supporting these needs. A comprehensive approach to the modern privacy challenges of accountable third-party data sharing requires a closer alignment of technical system architecture and legal institutional design. In order to achieve this privacy alignment, we extend traditional security threat modeling and analysis to encompass a broader range of privacy notions than has been typically considered. In particular, we introduce the concept of covert-accountability, which addresses adversaries that may act dishonestly but face potential identification and legal consequences. As a concrete instance of this design approach, we present the OTrace protocol, designed to provide traceable, accountable, consumer-control in third-party data sharing ecosystems. OTrace empowers consumers with the knowledge of where their data is, who has it, what it is being used for, and whom it is being shared with. By applying our alignment framework to OTrace, we demonstrate that OTrace's technical affordances can provide more confident, scalable regulatory oversight when combined with complementary legal mechanisms.",
        "field": "Artificial Intelligence (AI) in Banking",
        "link": "http://arxiv.org/abs/2503.09823v1"
    },
    {
        "id": "2503.09586v1",
        "title": "Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot",
        "authors": [
            "Andrew Crossman",
            "Andrew R. Plummer",
            "Chandra Sekharudu",
            "Deepak Warrier",
            "Mohammad Yekrangian"
        ],
        "published": "2025-03-12T17:54:18Z",
        "summary": "We present Auspex - a threat modeling system built using a specialized collection of generative artificial intelligence-based methods that capture threat modeling tradecraft. This new approach, called tradecraft prompting, centers on encoding the on-the-ground knowledge of threat modelers within the prompts that drive a generative AI-based threat modeling system. Auspex employs tradecraft prompts in two processing stages. The first stage centers on ingesting and processing system architecture information using prompts that encode threat modeling tradecraft knowledge pertaining to system decomposition and description. The second stage centers on chaining the resulting system analysis through a collection of prompts that encode tradecraft knowledge on threat identification, classification, and mitigation. The two-stage process yields a threat matrix for a system that specifies threat scenarios, threat types, information security categorizations and potential mitigations. Auspex produces formalized threat model output in minutes, relative to the weeks or months a manual process takes. More broadly, the focus on bespoke tradecraft prompting, as opposed to fine-tuning or agent-based add-ons, makes Auspex a lightweight, flexible, modular, and extensible foundational system capable of addressing the complexity, resource, and standardization limitations of both existing manual and automated threat modeling processes. In this connection, we establish the baseline value of Auspex to threat modelers through an evaluation procedure based on feedback collected from cybersecurity subject matter experts measuring the quality and utility of threat models generated by Auspex on real banking systems. We conclude with a discussion of system performance and plans for enhancements to Auspex.",
        "field": "Artificial Intelligence (AI) in Banking",
        "link": "http://arxiv.org/abs/2503.09586v1"
    },
    {
        "id": "2503.00070v1",
        "title": "Systematic Review of Cybersecurity in Banking: Evolution from Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain, Policies and Practice",
        "authors": [
            "Tue Nhi Tran"
        ],
        "published": "2025-02-27T14:17:06Z",
        "summary": "Throughout the history from pre-industry 4.0 to post-industry 4.0, cybersecurity at banks has undergone significant changes. Pre-industry 4.0 cyber security at banks relied on individual security methods that were highly manual and had low accuracy. When moving to post-industry 4.0, cybersecurity at banks had a major turning point with security methods that combined different technologies such as Artificial Intelligence (AI), Blockchain, IoT, automating necessary processes and significantly increasing the defence layer for banks. However, along with the development of new technologies, the current challenge of cybersecurity at banks lies in scalability, high costs and resources in both money and time for R&D of defence methods along with the threat of high-tech cybercriminals growing and expanding. This report goes from introducing the importance of cybersecurity at banks, analyzing their management, operational and business objectives, evaluating pre-industry 4.0 technologies used for cybersecurity at banks to assessing post-industry 4.0 technologies focusing on Artificial Intelligence and Blockchain, discussing current policies and practices and ending with discussing key advantages and challenges for 4.0 technologies and recommendations for further developing cybersecurity at banks.",
        "field": "Artificial Intelligence (AI) in Banking",
        "link": "http://arxiv.org/abs/2503.00070v1"
    },
    {
        "id": "2503.10644v1",
        "title": "Combined climate stress testing of supply-chain networks and the financial system with nation-wide firm-level emission estimates",
        "authors": [
            "Zlata Tabachová",
            "Christian Diem",
            "Johannes Stangl",
            "András Borsos",
            "Stefan Thurner"
        ],
        "published": "2025-02-25T20:25:47Z",
        "summary": "On the way towards carbon neutrality, climate stress testing provides estimates for the physical and transition risks that climate change poses to the economy and the financial system. Missing firm-level CO2 emissions data severely impedes the assessment of transition risks originating from carbon pricing. Based on the individual emissions of all Hungarian firms (410,523), as estimated from their fossil fuel purchases, we conduct a stress test of both actual and hypothetical carbon pricing policies. Using a simple 1:1 economic ABM and introducing the new carbon-to-profit ratio, we identify firms that become unprofitable and default, and estimate the respective loan write-offs. We find that 45% of all companies are directly exposed to carbon pricing. At a price of 45 EUR/t, direct economic losses of 1.3% of total sales and bank equity losses of 1.2% are expected. Secondary default cascades in supply chain networks could increase these losses by 300% to 4000%, depending on firms' ability to substitute essential inputs. To reduce transition risks, firms should reduce their dependence on essential inputs from supply chains with high CO2 exposure. We discuss the implications of different policy implementations on these transition risks.",
        "field": "Artificial Intelligence (AI) in Banking",
        "link": "http://arxiv.org/abs/2503.10644v1"
    },
    {
        "id": "2502.14766v2",
        "title": "Multi-Layer Deep xVA: Structural Credit Models, Measure Changes and Convergence Analysis",
        "authors": [
            "Kristoffer Andersson",
            "Alessandro Gnoatto"
        ],
        "published": "2025-02-20T17:41:55Z",
        "summary": "We propose a structural default model for portfolio-wide valuation adjustments (xVAs) and represent it as a system of coupled backward stochastic differential equations. The framework is divided into four layers, each capturing a key component: (i) clean values, (ii) initial margin and Collateral Valuation Adjustment (ColVA), (iii) Credit/Debit Valuation Adjustments (CVA/DVA) together with Margin Valuation Adjustment (MVA), and (iv) Funding Valuation Adjustment (FVA). Because these layers depend on one another through collateral and default effects, a naive Monte Carlo approach would require deeply nested simulations, making the problem computationally intractable. To address this challenge, we use an iterative deep BSDE approach, handling each layer sequentially so that earlier outputs serve as inputs to the subsequent layers. Initial margin is computed via deep quantile regression to reflect margin requirements over the Margin Period of Risk. We also adopt a change-of-measure method that highlights rare but significant defaults of the bank or counterparty, ensuring that these events are accurately captured in the training process. We further extend Han and Long's (2020) a posteriori error analysis to BSDEs on bounded domains. Due to the random exit from the domain, we obtain an order of convergence of $\\mathcal{O}(h^{1/4-\\epsilon})$ rather than the usual $\\mathcal{O}(h^{1/2})$. Numerical experiments illustrate that this method drastically reduces computational demands and successfully scales to high-dimensional, non-symmetric portfolios. The results confirm its effectiveness and accuracy, offering a practical alternative to nested Monte Carlo simulations in multi-counterparty xVA analyses.",
        "field": "Artificial Intelligence (AI) in Banking",
        "link": "http://arxiv.org/abs/2502.14766v2"
    },
    {
        "id": "2412.04051v1",
        "title": "How to design a Public Key Infrastructure for a Central Bank Digital Currency",
        "authors": [
            "Makan Rafiee",
            "Lars Hupel"
        ],
        "published": "2024-12-05T10:41:38Z",
        "summary": "Central Bank Digital Currency (CBDC) is a new form of money, issued by a country's or region's central bank, that can be used for a variety of payment scenarios. Depending on its concrete implementation, there are many participants in a production CBDC ecosystem, including the central bank, commercial banks, merchants, individuals, and wallet providers. There is a need for robust and scalable Public Key Infrastructure (PKI) for CBDC to ensure the continued trust of all entities in the system. This paper discusses the criteria that should flow into the design of a PKI and proposes a certificate hierarchy, together with a rollover concept ensuring continuous operation of the system. We further consider several peculiarities, such as the circulation of offline-capable hardware wallets.",
        "field": "Central Bank Digital Currencies (CBDCs)",
        "link": "http://arxiv.org/abs/2412.04051v1"
    },
    {
        "id": "2411.06362v1",
        "title": "Will Central Bank Digital Currencies (CBDC) and Blockchain Cryptocurrencies Coexist in the Post Quantum Era?",
        "authors": [
            "Abraham Itzhak Weinberg",
            "Pythagoras Petratos",
            "Alessio Faccia"
        ],
        "published": "2024-11-10T05:05:55Z",
        "summary": "This paper explores the coexistence possibilities of Central Bank Digital Currencies (CBDCs) and blockchain-based cryptocurrencies within a post-quantum computing landscape. It examines the implications of emerging quantum algorithms and cryptographic techniques such as Multi-Party Computation (MPC) and Oblivious Transfer (OT). While exploring how CBDCs and cryptocurrencies might integrate defenses like post-quantum cryptography, it highlights the substantial hurdles in transitioning legacy systems and fostering widespread adoption of new standards. The paper includes comprehensive evaluations of CBDCs in a quantum context. It also features comparisons to alternative cryptocurrency models. Additionally, the paper provides insightful analyses of pertinent quantum methodologies. Examinations of interfaces between these methods and blockchain architectures are also included. The paper carries out considered appraisals of quantum threats and their relevance for cryptocurrency schemes. Furthermore, it features discussions of the influence of anticipated advances in quantum computing on algorithms and their applications. The paper renders the judicious conclusion that long-term coexistence is viable provided challenges are constructively addressed through ongoing collaborative efforts to validate solutions and guide evolving policies.",
        "field": "Central Bank Digital Currencies (CBDCs)",
        "link": "http://arxiv.org/abs/2411.06362v1"
    },
    {
        "id": "2408.06956v1",
        "title": "PayOff: A Regulated Central Bank Digital Currency with Private Offline Payments",
        "authors": [
            "Carolin Beer",
            "Sheila Zingg",
            "Kari Kostiainen",
            "Karl Wüst",
            "Vedran Capkun",
            "Srdjan Capkun"
        ],
        "published": "2024-08-13T15:15:06Z",
        "summary": "The European Central Bank is preparing for the potential issuance of a central bank digital currency (CBDC), called the digital euro. A recent regulatory proposal by the European Commission defines several requirements for the digital euro, such as support for both online and offline payments. Offline payments are expected to enable cash-like privacy, local payment settlement, and the enforcement of holding limits. While other central banks have expressed similar desired functionality, achieving such offline payments poses a novel technical challenge. We observe that none of the existing research solutions, including offline E-cash schemes, are fully compliant. Proposed solutions based on secure elements offer no guarantees in case of compromise and can therefore lead to significant payment fraud. The main contribution of this paper is PayOff, a novel CBDC design motivated by the digital euro regulation, which focuses on offline payments. We analyze the security implications of local payment settlement and identify new security objectives. PayOff protects user privacy, supports complex regulations such as holding limits, and implements safeguards to increase robustness against secure element failure. Our analysis shows that PayOff provides strong privacy and identifies residual leakages that may arise in real-world deployments. Our evaluation shows that offline payments can be fast and that the central bank can handle high payment loads with moderate computing resources. However, the main limitation of PayOff is that offline payment messages and storage requirements grow in the number of payments that the sender makes or receives without going online in between.",
        "field": "Central Bank Digital Currencies (CBDCs)",
        "link": "http://arxiv.org/abs/2408.06956v1"
    },
    {
        "id": "2407.13776v1",
        "title": "Offline Digital Euro: a Minimum Viable CBDC using Groth-Sahai proofs",
        "authors": [
            "Leon Kempen",
            "Johan Pouwelse"
        ],
        "published": "2024-07-01T09:55:14Z",
        "summary": "Current digital payment solutions are fragile and offer less privacy than traditional cash. Their critical dependency on an online service used to perform and validate transactions makes them void if this service is unreachable. Moreover, no transaction can be executed during server malfunctions or power outages. Due to climate change, the likelihood of extreme weather increases. As extreme weather is a major cause of power outages, the frequency of power outages is expected to increase. The lack of privacy is an inherent result of their account-based design or the use of a public ledger. The critical dependency and lack of privacy can be resolved with a Central Bank Digital Currency that can be used offline. This thesis proposes a design and a first implementation for an offline-first digital euro. The protocol offers complete privacy during transactions using zero-knowledge proofs. Furthermore, transactions can be executed offline without third parties and retroactive double-spending detection is facilitated. To protect the users' privacy, but also guard against money laundering, we have added the following privacy-guarding mechanism. The bank and trusted third parties for law enforcement must collaborate to decrypt transactions, revealing the digital pseudonym used in the transaction. Importantly, the transaction can be decrypted without decrypting prior transactions attached to the digital euro. The protocol has a working initial implementation showcasing its usability and demonstrating functionality.",
        "field": "Central Bank Digital Currencies (CBDCs)",
        "link": "http://arxiv.org/abs/2407.13776v1"
    },
    {
        "id": "2405.10678v1",
        "title": "IT Strategic alignment in the decentralized finance (DeFi): CBDC and digital currencies",
        "authors": [
            "Carlos Alberto Durigan Junior",
            "Fernando Jose Barbin Laurindo"
        ],
        "published": "2024-05-17T10:19:20Z",
        "summary": "Cryptocurrency can be understood as a digital asset transacted among participants in the crypto economy. Every cryptocurrency must have an associated Blockchain. Blockchain is a Distributed Ledger Technology (DLT) which supports cryptocurrencies, this may be considered as the most promising disruptive technology in the industry 4.0 context. Decentralized finance (DeFi) is a Blockchain-based financial infrastructure, the term generally refers to an open, permissionless, and highly interoperable protocol stack built on public smart contract platforms, such as the Ethereum Blockchain. It replicates existing financial services in a more open and transparent way. DeFi does not rely on intermediaries and centralized institutions. Instead, it is based on open protocols and decentralized applications (Dapps). Considering that there are many digital coins, stablecoins and central bank digital currencies (CBDCs), these currencies should interact among each other sometime. For this interaction the Information Technology elements play an important whole as enablers and IT strategic alignment. This paper considers the strategic alignment model proposed by Henderson and Venkatraman (1993) and Luftman (1996). This paper seeks to answer two main questions 1) What are the common IT elements in the DeFi? And 2) How the elements connect to the IT strategic alignment in DeFi? Through a Systematic Literature Review (SLR). Results point out that there are many IT elements already mentioned by literature, however there is a lack in the literature about the connection between IT elements and IT strategic alignment in a Decentralized Finance (DeFi) architectural network. After final considerations, limitations and future research agenda are presented. Keywords: IT Strategic alignment, Decentralized Finance (DeFi), Cryptocurrency, Digital Economy.",
        "field": "Central Bank Digital Currencies (CBDCs)",
        "link": "http://arxiv.org/abs/2405.10678v1"
    },
    {
        "id": "2503.09823v1",
        "title": "Data Traceability for Privacy Alignment",
        "authors": [
            "Kevin Liao",
            "Shreya Thipireddy",
            "Daniel Weitzner"
        ],
        "published": "2025-03-12T20:42:23Z",
        "summary": "This paper offers a new privacy approach for the growing ecosystem of services--ranging from open banking to healthcare--dependent on sensitive personal data sharing between individuals and third-parties. While these services offer significant benefits, individuals want control over their data, transparency regarding how their data is used, and accountability from third-parties for misuse. However, existing legal and technical mechanisms are inadequate for supporting these needs. A comprehensive approach to the modern privacy challenges of accountable third-party data sharing requires a closer alignment of technical system architecture and legal institutional design. In order to achieve this privacy alignment, we extend traditional security threat modeling and analysis to encompass a broader range of privacy notions than has been typically considered. In particular, we introduce the concept of covert-accountability, which addresses adversaries that may act dishonestly but face potential identification and legal consequences. As a concrete instance of this design approach, we present the OTrace protocol, designed to provide traceable, accountable, consumer-control in third-party data sharing ecosystems. OTrace empowers consumers with the knowledge of where their data is, who has it, what it is being used for, and whom it is being shared with. By applying our alignment framework to OTrace, we demonstrate that OTrace's technical affordances can provide more confident, scalable regulatory oversight when combined with complementary legal mechanisms.",
        "field": "Cybersecurity in Banking",
        "link": "http://arxiv.org/abs/2503.09823v1"
    },
    {
        "id": "2503.09586v1",
        "title": "Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot",
        "authors": [
            "Andrew Crossman",
            "Andrew R. Plummer",
            "Chandra Sekharudu",
            "Deepak Warrier",
            "Mohammad Yekrangian"
        ],
        "published": "2025-03-12T17:54:18Z",
        "summary": "We present Auspex - a threat modeling system built using a specialized collection of generative artificial intelligence-based methods that capture threat modeling tradecraft. This new approach, called tradecraft prompting, centers on encoding the on-the-ground knowledge of threat modelers within the prompts that drive a generative AI-based threat modeling system. Auspex employs tradecraft prompts in two processing stages. The first stage centers on ingesting and processing system architecture information using prompts that encode threat modeling tradecraft knowledge pertaining to system decomposition and description. The second stage centers on chaining the resulting system analysis through a collection of prompts that encode tradecraft knowledge on threat identification, classification, and mitigation. The two-stage process yields a threat matrix for a system that specifies threat scenarios, threat types, information security categorizations and potential mitigations. Auspex produces formalized threat model output in minutes, relative to the weeks or months a manual process takes. More broadly, the focus on bespoke tradecraft prompting, as opposed to fine-tuning or agent-based add-ons, makes Auspex a lightweight, flexible, modular, and extensible foundational system capable of addressing the complexity, resource, and standardization limitations of both existing manual and automated threat modeling processes. In this connection, we establish the baseline value of Auspex to threat modelers through an evaluation procedure based on feedback collected from cybersecurity subject matter experts measuring the quality and utility of threat models generated by Auspex on real banking systems. We conclude with a discussion of system performance and plans for enhancements to Auspex.",
        "field": "Cybersecurity in Banking",
        "link": "http://arxiv.org/abs/2503.09586v1"
    },
    {
        "id": "2503.00070v1",
        "title": "Systematic Review of Cybersecurity in Banking: Evolution from Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain, Policies and Practice",
        "authors": [
            "Tue Nhi Tran"
        ],
        "published": "2025-02-27T14:17:06Z",
        "summary": "Throughout the history from pre-industry 4.0 to post-industry 4.0, cybersecurity at banks has undergone significant changes. Pre-industry 4.0 cyber security at banks relied on individual security methods that were highly manual and had low accuracy. When moving to post-industry 4.0, cybersecurity at banks had a major turning point with security methods that combined different technologies such as Artificial Intelligence (AI), Blockchain, IoT, automating necessary processes and significantly increasing the defence layer for banks. However, along with the development of new technologies, the current challenge of cybersecurity at banks lies in scalability, high costs and resources in both money and time for R&D of defence methods along with the threat of high-tech cybercriminals growing and expanding. This report goes from introducing the importance of cybersecurity at banks, analyzing their management, operational and business objectives, evaluating pre-industry 4.0 technologies used for cybersecurity at banks to assessing post-industry 4.0 technologies focusing on Artificial Intelligence and Blockchain, discussing current policies and practices and ending with discussing key advantages and challenges for 4.0 technologies and recommendations for further developing cybersecurity at banks.",
        "field": "Cybersecurity in Banking",
        "link": "http://arxiv.org/abs/2503.00070v1"
    },
    {
        "id": "2503.10644v1",
        "title": "Combined climate stress testing of supply-chain networks and the financial system with nation-wide firm-level emission estimates",
        "authors": [
            "Zlata Tabachová",
            "Christian Diem",
            "Johannes Stangl",
            "András Borsos",
            "Stefan Thurner"
        ],
        "published": "2025-02-25T20:25:47Z",
        "summary": "On the way towards carbon neutrality, climate stress testing provides estimates for the physical and transition risks that climate change poses to the economy and the financial system. Missing firm-level CO2 emissions data severely impedes the assessment of transition risks originating from carbon pricing. Based on the individual emissions of all Hungarian firms (410,523), as estimated from their fossil fuel purchases, we conduct a stress test of both actual and hypothetical carbon pricing policies. Using a simple 1:1 economic ABM and introducing the new carbon-to-profit ratio, we identify firms that become unprofitable and default, and estimate the respective loan write-offs. We find that 45% of all companies are directly exposed to carbon pricing. At a price of 45 EUR/t, direct economic losses of 1.3% of total sales and bank equity losses of 1.2% are expected. Secondary default cascades in supply chain networks could increase these losses by 300% to 4000%, depending on firms' ability to substitute essential inputs. To reduce transition risks, firms should reduce their dependence on essential inputs from supply chains with high CO2 exposure. We discuss the implications of different policy implementations on these transition risks.",
        "field": "Cybersecurity in Banking",
        "link": "http://arxiv.org/abs/2503.10644v1"
    },
    {
        "id": "2502.14766v2",
        "title": "Multi-Layer Deep xVA: Structural Credit Models, Measure Changes and Convergence Analysis",
        "authors": [
            "Kristoffer Andersson",
            "Alessandro Gnoatto"
        ],
        "published": "2025-02-20T17:41:55Z",
        "summary": "We propose a structural default model for portfolio-wide valuation adjustments (xVAs) and represent it as a system of coupled backward stochastic differential equations. The framework is divided into four layers, each capturing a key component: (i) clean values, (ii) initial margin and Collateral Valuation Adjustment (ColVA), (iii) Credit/Debit Valuation Adjustments (CVA/DVA) together with Margin Valuation Adjustment (MVA), and (iv) Funding Valuation Adjustment (FVA). Because these layers depend on one another through collateral and default effects, a naive Monte Carlo approach would require deeply nested simulations, making the problem computationally intractable. To address this challenge, we use an iterative deep BSDE approach, handling each layer sequentially so that earlier outputs serve as inputs to the subsequent layers. Initial margin is computed via deep quantile regression to reflect margin requirements over the Margin Period of Risk. We also adopt a change-of-measure method that highlights rare but significant defaults of the bank or counterparty, ensuring that these events are accurately captured in the training process. We further extend Han and Long's (2020) a posteriori error analysis to BSDEs on bounded domains. Due to the random exit from the domain, we obtain an order of convergence of $\\mathcal{O}(h^{1/4-\\epsilon})$ rather than the usual $\\mathcal{O}(h^{1/2})$. Numerical experiments illustrate that this method drastically reduces computational demands and successfully scales to high-dimensional, non-symmetric portfolios. The results confirm its effectiveness and accuracy, offering a practical alternative to nested Monte Carlo simulations in multi-counterparty xVA analyses.",
        "field": "Cybersecurity in Banking",
        "link": "http://arxiv.org/abs/2502.14766v2"
    },
    {
        "id": "2503.09823v1",
        "title": "Data Traceability for Privacy Alignment",
        "authors": [
            "Kevin Liao",
            "Shreya Thipireddy",
            "Daniel Weitzner"
        ],
        "published": "2025-03-12T20:42:23Z",
        "summary": "This paper offers a new privacy approach for the growing ecosystem of services--ranging from open banking to healthcare--dependent on sensitive personal data sharing between individuals and third-parties. While these services offer significant benefits, individuals want control over their data, transparency regarding how their data is used, and accountability from third-parties for misuse. However, existing legal and technical mechanisms are inadequate for supporting these needs. A comprehensive approach to the modern privacy challenges of accountable third-party data sharing requires a closer alignment of technical system architecture and legal institutional design. In order to achieve this privacy alignment, we extend traditional security threat modeling and analysis to encompass a broader range of privacy notions than has been typically considered. In particular, we introduce the concept of covert-accountability, which addresses adversaries that may act dishonestly but face potential identification and legal consequences. As a concrete instance of this design approach, we present the OTrace protocol, designed to provide traceable, accountable, consumer-control in third-party data sharing ecosystems. OTrace empowers consumers with the knowledge of where their data is, who has it, what it is being used for, and whom it is being shared with. By applying our alignment framework to OTrace, we demonstrate that OTrace's technical affordances can provide more confident, scalable regulatory oversight when combined with complementary legal mechanisms.",
        "field": "Biometrics in Banking",
        "link": "http://arxiv.org/abs/2503.09823v1"
    },
    {
        "id": "2503.09586v1",
        "title": "Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot",
        "authors": [
            "Andrew Crossman",
            "Andrew R. Plummer",
            "Chandra Sekharudu",
            "Deepak Warrier",
            "Mohammad Yekrangian"
        ],
        "published": "2025-03-12T17:54:18Z",
        "summary": "We present Auspex - a threat modeling system built using a specialized collection of generative artificial intelligence-based methods that capture threat modeling tradecraft. This new approach, called tradecraft prompting, centers on encoding the on-the-ground knowledge of threat modelers within the prompts that drive a generative AI-based threat modeling system. Auspex employs tradecraft prompts in two processing stages. The first stage centers on ingesting and processing system architecture information using prompts that encode threat modeling tradecraft knowledge pertaining to system decomposition and description. The second stage centers on chaining the resulting system analysis through a collection of prompts that encode tradecraft knowledge on threat identification, classification, and mitigation. The two-stage process yields a threat matrix for a system that specifies threat scenarios, threat types, information security categorizations and potential mitigations. Auspex produces formalized threat model output in minutes, relative to the weeks or months a manual process takes. More broadly, the focus on bespoke tradecraft prompting, as opposed to fine-tuning or agent-based add-ons, makes Auspex a lightweight, flexible, modular, and extensible foundational system capable of addressing the complexity, resource, and standardization limitations of both existing manual and automated threat modeling processes. In this connection, we establish the baseline value of Auspex to threat modelers through an evaluation procedure based on feedback collected from cybersecurity subject matter experts measuring the quality and utility of threat models generated by Auspex on real banking systems. We conclude with a discussion of system performance and plans for enhancements to Auspex.",
        "field": "Biometrics in Banking",
        "link": "http://arxiv.org/abs/2503.09586v1"
    },
    {
        "id": "2503.00070v1",
        "title": "Systematic Review of Cybersecurity in Banking: Evolution from Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain, Policies and Practice",
        "authors": [
            "Tue Nhi Tran"
        ],
        "published": "2025-02-27T14:17:06Z",
        "summary": "Throughout the history from pre-industry 4.0 to post-industry 4.0, cybersecurity at banks has undergone significant changes. Pre-industry 4.0 cyber security at banks relied on individual security methods that were highly manual and had low accuracy. When moving to post-industry 4.0, cybersecurity at banks had a major turning point with security methods that combined different technologies such as Artificial Intelligence (AI), Blockchain, IoT, automating necessary processes and significantly increasing the defence layer for banks. However, along with the development of new technologies, the current challenge of cybersecurity at banks lies in scalability, high costs and resources in both money and time for R&D of defence methods along with the threat of high-tech cybercriminals growing and expanding. This report goes from introducing the importance of cybersecurity at banks, analyzing their management, operational and business objectives, evaluating pre-industry 4.0 technologies used for cybersecurity at banks to assessing post-industry 4.0 technologies focusing on Artificial Intelligence and Blockchain, discussing current policies and practices and ending with discussing key advantages and challenges for 4.0 technologies and recommendations for further developing cybersecurity at banks.",
        "field": "Biometrics in Banking",
        "link": "http://arxiv.org/abs/2503.00070v1"
    },
    {
        "id": "2503.10644v1",
        "title": "Combined climate stress testing of supply-chain networks and the financial system with nation-wide firm-level emission estimates",
        "authors": [
            "Zlata Tabachová",
            "Christian Diem",
            "Johannes Stangl",
            "András Borsos",
            "Stefan Thurner"
        ],
        "published": "2025-02-25T20:25:47Z",
        "summary": "On the way towards carbon neutrality, climate stress testing provides estimates for the physical and transition risks that climate change poses to the economy and the financial system. Missing firm-level CO2 emissions data severely impedes the assessment of transition risks originating from carbon pricing. Based on the individual emissions of all Hungarian firms (410,523), as estimated from their fossil fuel purchases, we conduct a stress test of both actual and hypothetical carbon pricing policies. Using a simple 1:1 economic ABM and introducing the new carbon-to-profit ratio, we identify firms that become unprofitable and default, and estimate the respective loan write-offs. We find that 45% of all companies are directly exposed to carbon pricing. At a price of 45 EUR/t, direct economic losses of 1.3% of total sales and bank equity losses of 1.2% are expected. Secondary default cascades in supply chain networks could increase these losses by 300% to 4000%, depending on firms' ability to substitute essential inputs. To reduce transition risks, firms should reduce their dependence on essential inputs from supply chains with high CO2 exposure. We discuss the implications of different policy implementations on these transition risks.",
        "field": "Biometrics in Banking",
        "link": "http://arxiv.org/abs/2503.10644v1"
    },
    {
        "id": "2502.14766v2",
        "title": "Multi-Layer Deep xVA: Structural Credit Models, Measure Changes and Convergence Analysis",
        "authors": [
            "Kristoffer Andersson",
            "Alessandro Gnoatto"
        ],
        "published": "2025-02-20T17:41:55Z",
        "summary": "We propose a structural default model for portfolio-wide valuation adjustments (xVAs) and represent it as a system of coupled backward stochastic differential equations. The framework is divided into four layers, each capturing a key component: (i) clean values, (ii) initial margin and Collateral Valuation Adjustment (ColVA), (iii) Credit/Debit Valuation Adjustments (CVA/DVA) together with Margin Valuation Adjustment (MVA), and (iv) Funding Valuation Adjustment (FVA). Because these layers depend on one another through collateral and default effects, a naive Monte Carlo approach would require deeply nested simulations, making the problem computationally intractable. To address this challenge, we use an iterative deep BSDE approach, handling each layer sequentially so that earlier outputs serve as inputs to the subsequent layers. Initial margin is computed via deep quantile regression to reflect margin requirements over the Margin Period of Risk. We also adopt a change-of-measure method that highlights rare but significant defaults of the bank or counterparty, ensuring that these events are accurately captured in the training process. We further extend Han and Long's (2020) a posteriori error analysis to BSDEs on bounded domains. Due to the random exit from the domain, we obtain an order of convergence of $\\mathcal{O}(h^{1/4-\\epsilon})$ rather than the usual $\\mathcal{O}(h^{1/2})$. Numerical experiments illustrate that this method drastically reduces computational demands and successfully scales to high-dimensional, non-symmetric portfolios. The results confirm its effectiveness and accuracy, offering a practical alternative to nested Monte Carlo simulations in multi-counterparty xVA analyses.",
        "field": "Biometrics in Banking",
        "link": "http://arxiv.org/abs/2502.14766v2"
    },
    {
        "id": "2503.09823v1",
        "title": "Data Traceability for Privacy Alignment",
        "authors": [
            "Kevin Liao",
            "Shreya Thipireddy",
            "Daniel Weitzner"
        ],
        "published": "2025-03-12T20:42:23Z",
        "summary": "This paper offers a new privacy approach for the growing ecosystem of services--ranging from open banking to healthcare--dependent on sensitive personal data sharing between individuals and third-parties. While these services offer significant benefits, individuals want control over their data, transparency regarding how their data is used, and accountability from third-parties for misuse. However, existing legal and technical mechanisms are inadequate for supporting these needs. A comprehensive approach to the modern privacy challenges of accountable third-party data sharing requires a closer alignment of technical system architecture and legal institutional design. In order to achieve this privacy alignment, we extend traditional security threat modeling and analysis to encompass a broader range of privacy notions than has been typically considered. In particular, we introduce the concept of covert-accountability, which addresses adversaries that may act dishonestly but face potential identification and legal consequences. As a concrete instance of this design approach, we present the OTrace protocol, designed to provide traceable, accountable, consumer-control in third-party data sharing ecosystems. OTrace empowers consumers with the knowledge of where their data is, who has it, what it is being used for, and whom it is being shared with. By applying our alignment framework to OTrace, we demonstrate that OTrace's technical affordances can provide more confident, scalable regulatory oversight when combined with complementary legal mechanisms.",
        "field": "Edge Computing in Banking",
        "link": "http://arxiv.org/abs/2503.09823v1"
    },
    {
        "id": "2503.09586v1",
        "title": "Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot",
        "authors": [
            "Andrew Crossman",
            "Andrew R. Plummer",
            "Chandra Sekharudu",
            "Deepak Warrier",
            "Mohammad Yekrangian"
        ],
        "published": "2025-03-12T17:54:18Z",
        "summary": "We present Auspex - a threat modeling system built using a specialized collection of generative artificial intelligence-based methods that capture threat modeling tradecraft. This new approach, called tradecraft prompting, centers on encoding the on-the-ground knowledge of threat modelers within the prompts that drive a generative AI-based threat modeling system. Auspex employs tradecraft prompts in two processing stages. The first stage centers on ingesting and processing system architecture information using prompts that encode threat modeling tradecraft knowledge pertaining to system decomposition and description. The second stage centers on chaining the resulting system analysis through a collection of prompts that encode tradecraft knowledge on threat identification, classification, and mitigation. The two-stage process yields a threat matrix for a system that specifies threat scenarios, threat types, information security categorizations and potential mitigations. Auspex produces formalized threat model output in minutes, relative to the weeks or months a manual process takes. More broadly, the focus on bespoke tradecraft prompting, as opposed to fine-tuning or agent-based add-ons, makes Auspex a lightweight, flexible, modular, and extensible foundational system capable of addressing the complexity, resource, and standardization limitations of both existing manual and automated threat modeling processes. In this connection, we establish the baseline value of Auspex to threat modelers through an evaluation procedure based on feedback collected from cybersecurity subject matter experts measuring the quality and utility of threat models generated by Auspex on real banking systems. We conclude with a discussion of system performance and plans for enhancements to Auspex.",
        "field": "Edge Computing in Banking",
        "link": "http://arxiv.org/abs/2503.09586v1"
    },
    {
        "id": "2503.00070v1",
        "title": "Systematic Review of Cybersecurity in Banking: Evolution from Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain, Policies and Practice",
        "authors": [
            "Tue Nhi Tran"
        ],
        "published": "2025-02-27T14:17:06Z",
        "summary": "Throughout the history from pre-industry 4.0 to post-industry 4.0, cybersecurity at banks has undergone significant changes. Pre-industry 4.0 cyber security at banks relied on individual security methods that were highly manual and had low accuracy. When moving to post-industry 4.0, cybersecurity at banks had a major turning point with security methods that combined different technologies such as Artificial Intelligence (AI), Blockchain, IoT, automating necessary processes and significantly increasing the defence layer for banks. However, along with the development of new technologies, the current challenge of cybersecurity at banks lies in scalability, high costs and resources in both money and time for R&D of defence methods along with the threat of high-tech cybercriminals growing and expanding. This report goes from introducing the importance of cybersecurity at banks, analyzing their management, operational and business objectives, evaluating pre-industry 4.0 technologies used for cybersecurity at banks to assessing post-industry 4.0 technologies focusing on Artificial Intelligence and Blockchain, discussing current policies and practices and ending with discussing key advantages and challenges for 4.0 technologies and recommendations for further developing cybersecurity at banks.",
        "field": "Edge Computing in Banking",
        "link": "http://arxiv.org/abs/2503.00070v1"
    },
    {
        "id": "2503.10644v1",
        "title": "Combined climate stress testing of supply-chain networks and the financial system with nation-wide firm-level emission estimates",
        "authors": [
            "Zlata Tabachová",
            "Christian Diem",
            "Johannes Stangl",
            "András Borsos",
            "Stefan Thurner"
        ],
        "published": "2025-02-25T20:25:47Z",
        "summary": "On the way towards carbon neutrality, climate stress testing provides estimates for the physical and transition risks that climate change poses to the economy and the financial system. Missing firm-level CO2 emissions data severely impedes the assessment of transition risks originating from carbon pricing. Based on the individual emissions of all Hungarian firms (410,523), as estimated from their fossil fuel purchases, we conduct a stress test of both actual and hypothetical carbon pricing policies. Using a simple 1:1 economic ABM and introducing the new carbon-to-profit ratio, we identify firms that become unprofitable and default, and estimate the respective loan write-offs. We find that 45% of all companies are directly exposed to carbon pricing. At a price of 45 EUR/t, direct economic losses of 1.3% of total sales and bank equity losses of 1.2% are expected. Secondary default cascades in supply chain networks could increase these losses by 300% to 4000%, depending on firms' ability to substitute essential inputs. To reduce transition risks, firms should reduce their dependence on essential inputs from supply chains with high CO2 exposure. We discuss the implications of different policy implementations on these transition risks.",
        "field": "Edge Computing in Banking",
        "link": "http://arxiv.org/abs/2503.10644v1"
    },
    {
        "id": "2502.14766v2",
        "title": "Multi-Layer Deep xVA: Structural Credit Models, Measure Changes and Convergence Analysis",
        "authors": [
            "Kristoffer Andersson",
            "Alessandro Gnoatto"
        ],
        "published": "2025-02-20T17:41:55Z",
        "summary": "We propose a structural default model for portfolio-wide valuation adjustments (xVAs) and represent it as a system of coupled backward stochastic differential equations. The framework is divided into four layers, each capturing a key component: (i) clean values, (ii) initial margin and Collateral Valuation Adjustment (ColVA), (iii) Credit/Debit Valuation Adjustments (CVA/DVA) together with Margin Valuation Adjustment (MVA), and (iv) Funding Valuation Adjustment (FVA). Because these layers depend on one another through collateral and default effects, a naive Monte Carlo approach would require deeply nested simulations, making the problem computationally intractable. To address this challenge, we use an iterative deep BSDE approach, handling each layer sequentially so that earlier outputs serve as inputs to the subsequent layers. Initial margin is computed via deep quantile regression to reflect margin requirements over the Margin Period of Risk. We also adopt a change-of-measure method that highlights rare but significant defaults of the bank or counterparty, ensuring that these events are accurately captured in the training process. We further extend Han and Long's (2020) a posteriori error analysis to BSDEs on bounded domains. Due to the random exit from the domain, we obtain an order of convergence of $\\mathcal{O}(h^{1/4-\\epsilon})$ rather than the usual $\\mathcal{O}(h^{1/2})$. Numerical experiments illustrate that this method drastically reduces computational demands and successfully scales to high-dimensional, non-symmetric portfolios. The results confirm its effectiveness and accuracy, offering a practical alternative to nested Monte Carlo simulations in multi-counterparty xVA analyses.",
        "field": "Edge Computing in Banking",
        "link": "http://arxiv.org/abs/2502.14766v2"
    },
    {
        "id": "2503.09823v1",
        "title": "Data Traceability for Privacy Alignment",
        "authors": [
            "Kevin Liao",
            "Shreya Thipireddy",
            "Daniel Weitzner"
        ],
        "published": "2025-03-12T20:42:23Z",
        "summary": "This paper offers a new privacy approach for the growing ecosystem of services--ranging from open banking to healthcare--dependent on sensitive personal data sharing between individuals and third-parties. While these services offer significant benefits, individuals want control over their data, transparency regarding how their data is used, and accountability from third-parties for misuse. However, existing legal and technical mechanisms are inadequate for supporting these needs. A comprehensive approach to the modern privacy challenges of accountable third-party data sharing requires a closer alignment of technical system architecture and legal institutional design. In order to achieve this privacy alignment, we extend traditional security threat modeling and analysis to encompass a broader range of privacy notions than has been typically considered. In particular, we introduce the concept of covert-accountability, which addresses adversaries that may act dishonestly but face potential identification and legal consequences. As a concrete instance of this design approach, we present the OTrace protocol, designed to provide traceable, accountable, consumer-control in third-party data sharing ecosystems. OTrace empowers consumers with the knowledge of where their data is, who has it, what it is being used for, and whom it is being shared with. By applying our alignment framework to OTrace, we demonstrate that OTrace's technical affordances can provide more confident, scalable regulatory oversight when combined with complementary legal mechanisms.",
        "field": "Internet of Things (IoT) in Banking",
        "link": "http://arxiv.org/abs/2503.09823v1"
    },
    {
        "id": "2503.09586v1",
        "title": "Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot",
        "authors": [
            "Andrew Crossman",
            "Andrew R. Plummer",
            "Chandra Sekharudu",
            "Deepak Warrier",
            "Mohammad Yekrangian"
        ],
        "published": "2025-03-12T17:54:18Z",
        "summary": "We present Auspex - a threat modeling system built using a specialized collection of generative artificial intelligence-based methods that capture threat modeling tradecraft. This new approach, called tradecraft prompting, centers on encoding the on-the-ground knowledge of threat modelers within the prompts that drive a generative AI-based threat modeling system. Auspex employs tradecraft prompts in two processing stages. The first stage centers on ingesting and processing system architecture information using prompts that encode threat modeling tradecraft knowledge pertaining to system decomposition and description. The second stage centers on chaining the resulting system analysis through a collection of prompts that encode tradecraft knowledge on threat identification, classification, and mitigation. The two-stage process yields a threat matrix for a system that specifies threat scenarios, threat types, information security categorizations and potential mitigations. Auspex produces formalized threat model output in minutes, relative to the weeks or months a manual process takes. More broadly, the focus on bespoke tradecraft prompting, as opposed to fine-tuning or agent-based add-ons, makes Auspex a lightweight, flexible, modular, and extensible foundational system capable of addressing the complexity, resource, and standardization limitations of both existing manual and automated threat modeling processes. In this connection, we establish the baseline value of Auspex to threat modelers through an evaluation procedure based on feedback collected from cybersecurity subject matter experts measuring the quality and utility of threat models generated by Auspex on real banking systems. We conclude with a discussion of system performance and plans for enhancements to Auspex.",
        "field": "Internet of Things (IoT) in Banking",
        "link": "http://arxiv.org/abs/2503.09586v1"
    },
    {
        "id": "2503.00070v1",
        "title": "Systematic Review of Cybersecurity in Banking: Evolution from Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain, Policies and Practice",
        "authors": [
            "Tue Nhi Tran"
        ],
        "published": "2025-02-27T14:17:06Z",
        "summary": "Throughout the history from pre-industry 4.0 to post-industry 4.0, cybersecurity at banks has undergone significant changes. Pre-industry 4.0 cyber security at banks relied on individual security methods that were highly manual and had low accuracy. When moving to post-industry 4.0, cybersecurity at banks had a major turning point with security methods that combined different technologies such as Artificial Intelligence (AI), Blockchain, IoT, automating necessary processes and significantly increasing the defence layer for banks. However, along with the development of new technologies, the current challenge of cybersecurity at banks lies in scalability, high costs and resources in both money and time for R&D of defence methods along with the threat of high-tech cybercriminals growing and expanding. This report goes from introducing the importance of cybersecurity at banks, analyzing their management, operational and business objectives, evaluating pre-industry 4.0 technologies used for cybersecurity at banks to assessing post-industry 4.0 technologies focusing on Artificial Intelligence and Blockchain, discussing current policies and practices and ending with discussing key advantages and challenges for 4.0 technologies and recommendations for further developing cybersecurity at banks.",
        "field": "Internet of Things (IoT) in Banking",
        "link": "http://arxiv.org/abs/2503.00070v1"
    },
    {
        "id": "2503.10644v1",
        "title": "Combined climate stress testing of supply-chain networks and the financial system with nation-wide firm-level emission estimates",
        "authors": [
            "Zlata Tabachová",
            "Christian Diem",
            "Johannes Stangl",
            "András Borsos",
            "Stefan Thurner"
        ],
        "published": "2025-02-25T20:25:47Z",
        "summary": "On the way towards carbon neutrality, climate stress testing provides estimates for the physical and transition risks that climate change poses to the economy and the financial system. Missing firm-level CO2 emissions data severely impedes the assessment of transition risks originating from carbon pricing. Based on the individual emissions of all Hungarian firms (410,523), as estimated from their fossil fuel purchases, we conduct a stress test of both actual and hypothetical carbon pricing policies. Using a simple 1:1 economic ABM and introducing the new carbon-to-profit ratio, we identify firms that become unprofitable and default, and estimate the respective loan write-offs. We find that 45% of all companies are directly exposed to carbon pricing. At a price of 45 EUR/t, direct economic losses of 1.3% of total sales and bank equity losses of 1.2% are expected. Secondary default cascades in supply chain networks could increase these losses by 300% to 4000%, depending on firms' ability to substitute essential inputs. To reduce transition risks, firms should reduce their dependence on essential inputs from supply chains with high CO2 exposure. We discuss the implications of different policy implementations on these transition risks.",
        "field": "Internet of Things (IoT) in Banking",
        "link": "http://arxiv.org/abs/2503.10644v1"
    },
    {
        "id": "2502.14766v2",
        "title": "Multi-Layer Deep xVA: Structural Credit Models, Measure Changes and Convergence Analysis",
        "authors": [
            "Kristoffer Andersson",
            "Alessandro Gnoatto"
        ],
        "published": "2025-02-20T17:41:55Z",
        "summary": "We propose a structural default model for portfolio-wide valuation adjustments (xVAs) and represent it as a system of coupled backward stochastic differential equations. The framework is divided into four layers, each capturing a key component: (i) clean values, (ii) initial margin and Collateral Valuation Adjustment (ColVA), (iii) Credit/Debit Valuation Adjustments (CVA/DVA) together with Margin Valuation Adjustment (MVA), and (iv) Funding Valuation Adjustment (FVA). Because these layers depend on one another through collateral and default effects, a naive Monte Carlo approach would require deeply nested simulations, making the problem computationally intractable. To address this challenge, we use an iterative deep BSDE approach, handling each layer sequentially so that earlier outputs serve as inputs to the subsequent layers. Initial margin is computed via deep quantile regression to reflect margin requirements over the Margin Period of Risk. We also adopt a change-of-measure method that highlights rare but significant defaults of the bank or counterparty, ensuring that these events are accurately captured in the training process. We further extend Han and Long's (2020) a posteriori error analysis to BSDEs on bounded domains. Due to the random exit from the domain, we obtain an order of convergence of $\\mathcal{O}(h^{1/4-\\epsilon})$ rather than the usual $\\mathcal{O}(h^{1/2})$. Numerical experiments illustrate that this method drastically reduces computational demands and successfully scales to high-dimensional, non-symmetric portfolios. The results confirm its effectiveness and accuracy, offering a practical alternative to nested Monte Carlo simulations in multi-counterparty xVA analyses.",
        "field": "Internet of Things (IoT) in Banking",
        "link": "http://arxiv.org/abs/2502.14766v2"
    },
    {
        "id": "2503.09823v1",
        "title": "Data Traceability for Privacy Alignment",
        "authors": [
            "Kevin Liao",
            "Shreya Thipireddy",
            "Daniel Weitzner"
        ],
        "published": "2025-03-12T20:42:23Z",
        "summary": "This paper offers a new privacy approach for the growing ecosystem of services--ranging from open banking to healthcare--dependent on sensitive personal data sharing between individuals and third-parties. While these services offer significant benefits, individuals want control over their data, transparency regarding how their data is used, and accountability from third-parties for misuse. However, existing legal and technical mechanisms are inadequate for supporting these needs. A comprehensive approach to the modern privacy challenges of accountable third-party data sharing requires a closer alignment of technical system architecture and legal institutional design. In order to achieve this privacy alignment, we extend traditional security threat modeling and analysis to encompass a broader range of privacy notions than has been typically considered. In particular, we introduce the concept of covert-accountability, which addresses adversaries that may act dishonestly but face potential identification and legal consequences. As a concrete instance of this design approach, we present the OTrace protocol, designed to provide traceable, accountable, consumer-control in third-party data sharing ecosystems. OTrace empowers consumers with the knowledge of where their data is, who has it, what it is being used for, and whom it is being shared with. By applying our alignment framework to OTrace, we demonstrate that OTrace's technical affordances can provide more confident, scalable regulatory oversight when combined with complementary legal mechanisms.",
        "field": "5G Technology in Banking",
        "link": "http://arxiv.org/abs/2503.09823v1"
    },
    {
        "id": "2503.09586v1",
        "title": "Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot",
        "authors": [
            "Andrew Crossman",
            "Andrew R. Plummer",
            "Chandra Sekharudu",
            "Deepak Warrier",
            "Mohammad Yekrangian"
        ],
        "published": "2025-03-12T17:54:18Z",
        "summary": "We present Auspex - a threat modeling system built using a specialized collection of generative artificial intelligence-based methods that capture threat modeling tradecraft. This new approach, called tradecraft prompting, centers on encoding the on-the-ground knowledge of threat modelers within the prompts that drive a generative AI-based threat modeling system. Auspex employs tradecraft prompts in two processing stages. The first stage centers on ingesting and processing system architecture information using prompts that encode threat modeling tradecraft knowledge pertaining to system decomposition and description. The second stage centers on chaining the resulting system analysis through a collection of prompts that encode tradecraft knowledge on threat identification, classification, and mitigation. The two-stage process yields a threat matrix for a system that specifies threat scenarios, threat types, information security categorizations and potential mitigations. Auspex produces formalized threat model output in minutes, relative to the weeks or months a manual process takes. More broadly, the focus on bespoke tradecraft prompting, as opposed to fine-tuning or agent-based add-ons, makes Auspex a lightweight, flexible, modular, and extensible foundational system capable of addressing the complexity, resource, and standardization limitations of both existing manual and automated threat modeling processes. In this connection, we establish the baseline value of Auspex to threat modelers through an evaluation procedure based on feedback collected from cybersecurity subject matter experts measuring the quality and utility of threat models generated by Auspex on real banking systems. We conclude with a discussion of system performance and plans for enhancements to Auspex.",
        "field": "5G Technology in Banking",
        "link": "http://arxiv.org/abs/2503.09586v1"
    },
    {
        "id": "2503.00070v1",
        "title": "Systematic Review of Cybersecurity in Banking: Evolution from Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain, Policies and Practice",
        "authors": [
            "Tue Nhi Tran"
        ],
        "published": "2025-02-27T14:17:06Z",
        "summary": "Throughout the history from pre-industry 4.0 to post-industry 4.0, cybersecurity at banks has undergone significant changes. Pre-industry 4.0 cyber security at banks relied on individual security methods that were highly manual and had low accuracy. When moving to post-industry 4.0, cybersecurity at banks had a major turning point with security methods that combined different technologies such as Artificial Intelligence (AI), Blockchain, IoT, automating necessary processes and significantly increasing the defence layer for banks. However, along with the development of new technologies, the current challenge of cybersecurity at banks lies in scalability, high costs and resources in both money and time for R&D of defence methods along with the threat of high-tech cybercriminals growing and expanding. This report goes from introducing the importance of cybersecurity at banks, analyzing their management, operational and business objectives, evaluating pre-industry 4.0 technologies used for cybersecurity at banks to assessing post-industry 4.0 technologies focusing on Artificial Intelligence and Blockchain, discussing current policies and practices and ending with discussing key advantages and challenges for 4.0 technologies and recommendations for further developing cybersecurity at banks.",
        "field": "5G Technology in Banking",
        "link": "http://arxiv.org/abs/2503.00070v1"
    },
    {
        "id": "2503.10644v1",
        "title": "Combined climate stress testing of supply-chain networks and the financial system with nation-wide firm-level emission estimates",
        "authors": [
            "Zlata Tabachová",
            "Christian Diem",
            "Johannes Stangl",
            "András Borsos",
            "Stefan Thurner"
        ],
        "published": "2025-02-25T20:25:47Z",
        "summary": "On the way towards carbon neutrality, climate stress testing provides estimates for the physical and transition risks that climate change poses to the economy and the financial system. Missing firm-level CO2 emissions data severely impedes the assessment of transition risks originating from carbon pricing. Based on the individual emissions of all Hungarian firms (410,523), as estimated from their fossil fuel purchases, we conduct a stress test of both actual and hypothetical carbon pricing policies. Using a simple 1:1 economic ABM and introducing the new carbon-to-profit ratio, we identify firms that become unprofitable and default, and estimate the respective loan write-offs. We find that 45% of all companies are directly exposed to carbon pricing. At a price of 45 EUR/t, direct economic losses of 1.3% of total sales and bank equity losses of 1.2% are expected. Secondary default cascades in supply chain networks could increase these losses by 300% to 4000%, depending on firms' ability to substitute essential inputs. To reduce transition risks, firms should reduce their dependence on essential inputs from supply chains with high CO2 exposure. We discuss the implications of different policy implementations on these transition risks.",
        "field": "5G Technology in Banking",
        "link": "http://arxiv.org/abs/2503.10644v1"
    },
    {
        "id": "2502.14766v2",
        "title": "Multi-Layer Deep xVA: Structural Credit Models, Measure Changes and Convergence Analysis",
        "authors": [
            "Kristoffer Andersson",
            "Alessandro Gnoatto"
        ],
        "published": "2025-02-20T17:41:55Z",
        "summary": "We propose a structural default model for portfolio-wide valuation adjustments (xVAs) and represent it as a system of coupled backward stochastic differential equations. The framework is divided into four layers, each capturing a key component: (i) clean values, (ii) initial margin and Collateral Valuation Adjustment (ColVA), (iii) Credit/Debit Valuation Adjustments (CVA/DVA) together with Margin Valuation Adjustment (MVA), and (iv) Funding Valuation Adjustment (FVA). Because these layers depend on one another through collateral and default effects, a naive Monte Carlo approach would require deeply nested simulations, making the problem computationally intractable. To address this challenge, we use an iterative deep BSDE approach, handling each layer sequentially so that earlier outputs serve as inputs to the subsequent layers. Initial margin is computed via deep quantile regression to reflect margin requirements over the Margin Period of Risk. We also adopt a change-of-measure method that highlights rare but significant defaults of the bank or counterparty, ensuring that these events are accurately captured in the training process. We further extend Han and Long's (2020) a posteriori error analysis to BSDEs on bounded domains. Due to the random exit from the domain, we obtain an order of convergence of $\\mathcal{O}(h^{1/4-\\epsilon})$ rather than the usual $\\mathcal{O}(h^{1/2})$. Numerical experiments illustrate that this method drastically reduces computational demands and successfully scales to high-dimensional, non-symmetric portfolios. The results confirm its effectiveness and accuracy, offering a practical alternative to nested Monte Carlo simulations in multi-counterparty xVA analyses.",
        "field": "5G Technology in Banking",
        "link": "http://arxiv.org/abs/2502.14766v2"
    },
    {
        "id": "2503.11619v1",
        "title": "Tit-for-Tat: Safeguarding Large Vision-Language Models Against Jailbreak Attacks via Adversarial Defense",
        "authors": [
            "Shuyang Hao",
            "Yiwei Wang",
            "Bryan Hooi",
            "Ming-Hsuan Yang",
            "Jun Liu",
            "Chengcheng Tang",
            "Zi Huang",
            "Yujun Cai"
        ],
        "published": "2025-03-14T17:39:45Z",
        "summary": "Deploying large vision-language models (LVLMs) introduces a unique vulnerability: susceptibility to malicious attacks via visual inputs. However, existing defense methods suffer from two key limitations: (1) They solely focus on textual defenses, fail to directly address threats in the visual domain where attacks originate, and (2) the additional processing steps often incur significant computational overhead or compromise model performance on benign tasks. Building on these insights, we propose ESIII (Embedding Security Instructions Into Images), a novel methodology for transforming the visual space from a source of vulnerability into an active defense mechanism. Initially, we embed security instructions into defensive images through gradient-based optimization, obtaining security instructions in the visual dimension. Subsequently, we integrate security instructions from visual and textual dimensions with the input query. The collaboration between security instructions from different dimensions ensures comprehensive security protection. Extensive experiments demonstrate that our approach effectively fortifies the robustness of LVLMs against such attacks while preserving their performance on standard benign tasks and incurring an imperceptible increase in time costs.",
        "field": "Neuromorphic Computing",
        "link": "http://arxiv.org/abs/2503.11619v1"
    },
    {
        "id": "2503.11573v1",
        "title": "Synthesizing Access Control Policies using Large Language Models",
        "authors": [
            "Adarsh Vatsa",
            "Pratyush Patel",
            "William Eiers"
        ],
        "published": "2025-03-14T16:40:25Z",
        "summary": "Cloud compute systems allow administrators to write access control policies that govern access to private data. While policies are written in convenient languages, such as AWS Identity and Access Management Policy Language, manually written policies often become complex and error prone. In this paper, we investigate whether and how well Large Language Models (LLMs) can be used to synthesize access control policies. Our investigation focuses on the task of taking an access control request specification and zero-shot prompting LLMs to synthesize a well-formed access control policy which correctly adheres to the request specification. We consider two scenarios, one which the request specification is given as a concrete list of requests to be allowed or denied, and another in which a natural language description is used to specify sets of requests to be allowed or denied. We then argue that for zero-shot prompting, more precise and structured prompts using a syntax based approach are necessary and experimentally show preliminary results validating our approach.",
        "field": "Neuromorphic Computing",
        "link": "http://arxiv.org/abs/2503.11573v1"
    },
    {
        "id": "2503.11404v1",
        "title": "Towards A Correct Usage of Cryptography in Semantic Watermarks for Diffusion Models",
        "authors": [
            "Jonas Thietke",
            "Andreas Müller",
            "Denis Lukovnikov",
            "Asja Fischer",
            "Erwin Quiring"
        ],
        "published": "2025-03-14T13:45:46Z",
        "summary": "Semantic watermarking methods enable the direct integration of watermarks into the generation process of latent diffusion models by only modifying the initial latent noise. One line of approaches building on Gaussian Shading relies on cryptographic primitives to steer the sampling process of the latent noise. However, we identify several issues in the usage of cryptographic techniques in Gaussian Shading, particularly in its proof of lossless performance and key management, causing ambiguity in follow-up works, too. In this work, we therefore revisit the cryptographic primitives for semantic watermarking. We introduce a novel, general proof of lossless performance based on IND\\$-CPA security for semantic watermarks. We then discuss the configuration of the cryptographic primitives in semantic watermarks with respect to security, efficiency, and generation quality.",
        "field": "Neuromorphic Computing",
        "link": "http://arxiv.org/abs/2503.11404v1"
    },
    {
        "id": "2503.11216v1",
        "title": "Cross-Platform Benchmarking of the FHE Libraries: Novel Insights into SEAL and Openfhe",
        "authors": [
            "Faneela",
            "Jawad Ahmad",
            "Baraq Ghaleb",
            "Sana Ullah Jan",
            "William J. Buchanan"
        ],
        "published": "2025-03-14T09:08:30Z",
        "summary": "The rapid growth of cloud computing and data-driven applications has amplified privacy concerns, driven by the increasing demand to process sensitive data securely. Homomorphic encryption (HE) has become a vital solution for addressing these concerns by enabling computations on encrypted data without revealing its contents. This paper provides a comprehensive evaluation of two leading HE libraries, SEAL and OpenFHE, examining their performance, usability, and support for prominent HE schemes such as BGV and CKKS. Our analysis highlights computational efficiency, memory usage, and scalability across Linux and Windows platforms, emphasizing their applicability in real-world scenarios. Results reveal that Linux outperforms Windows in computation efficiency, with OpenFHE emerging as the optimal choice across diverse cryptographic settings. This paper provides valuable insights for researchers and practitioners to advance privacy-preserving applications using FHE.",
        "field": "Neuromorphic Computing",
        "link": "http://arxiv.org/abs/2503.11216v1"
    },
    {
        "id": "2503.11185v1",
        "title": "Align in Depth: Defending Jailbreak Attacks via Progressive Answer Detoxification",
        "authors": [
            "Yingjie Zhang",
            "Tong Liu",
            "Zhe Zhao",
            "Guozhu Meng",
            "Kai Chen"
        ],
        "published": "2025-03-14T08:32:12Z",
        "summary": "Large Language Models (LLMs) are vulnerable to jailbreak attacks, which use crafted prompts to elicit toxic responses. These attacks exploit LLMs' difficulty in dynamically detecting harmful intents during the generation process. Traditional safety alignment methods, often relying on the initial few generation steps, are ineffective due to limited computational budget. This paper proposes DEEPALIGN, a robust defense framework that fine-tunes LLMs to progressively detoxify generated content, significantly improving both the computational budget and effectiveness of mitigating harmful generation. Our approach uses a hybrid loss function operating on hidden states to directly improve LLMs' inherent awareness of toxity during generation. Furthermore, we redefine safe responses by generating semantically relevant answers to harmful queries, thereby increasing robustness against representation-mutation attacks. Evaluations across multiple LLMs demonstrate state-of-the-art defense performance against six different attack types, reducing Attack Success Rates by up to two orders of magnitude compared to previous state-of-the-art defense while preserving utility. This work advances LLM safety by addressing limitations of conventional alignment through dynamic, context-aware mitigation.",
        "field": "Neuromorphic Computing",
        "link": "http://arxiv.org/abs/2503.11185v1"
    },
    {
        "id": "2503.10269v1",
        "title": "Targeted Data Poisoning for Black-Box Audio Datasets Ownership Verification",
        "authors": [
            "Wassim Bouaziz",
            "El-Mahdi El-Mhamdi",
            "Nicolas Usunier"
        ],
        "published": "2025-03-13T11:25:25Z",
        "summary": "Protecting the use of audio datasets is a major concern for data owners, particularly with the recent rise of audio deep learning models. While watermarks can be used to protect the data itself, they do not allow to identify a deep learning model trained on a protected dataset. In this paper, we adapt to audio data the recently introduced data taggants approach. Data taggants is a method to verify if a neural network was trained on a protected image dataset with top-$k$ predictions access to the model only. This method relies on a targeted data poisoning scheme by discreetly altering a small fraction (1%) of the dataset as to induce a harmless behavior on out-of-distribution data called keys. We evaluate our method on the Speechcommands and the ESC50 datasets and state of the art transformer models, and show that we can detect the use of the dataset with high confidence without loss of performance. We also show the robustness of our method against common data augmentation techniques, making it a practical method to protect audio datasets.",
        "field": "Digital Identity Verification",
        "link": "http://arxiv.org/abs/2503.10269v1"
    },
    {
        "id": "2503.10171v1",
        "title": "Verifiable, Efficient and Confidentiality-Preserving Graph Search with Transparency",
        "authors": [
            "Qiuhao Wang",
            "Xu Yang",
            "Yiwei Liu",
            "Saiyu Qi",
            "Hongguang Zhao",
            "Ke Li",
            "Yong Qi"
        ],
        "published": "2025-03-13T08:53:53Z",
        "summary": "Graph databases have garnered extensive attention and research due to their ability to manage relationships between entities efficiently. Today, many graph search services have been outsourced to a third-party server to facilitate storage and computational support. Nevertheless, the outsourcing paradigm may invade the privacy of graphs. PeGraph is the latest scheme achieving encrypted search over social graphs to address the privacy leakage, which maintains two data structures XSet and TSet motivated by the OXT technology to support encrypted conjunctive search. However, PeGraph still exhibits limitations inherent to the underlying OXT. It does not provide transparent search capabilities, suffers from expensive computation and result pattern leakages, and it fails to support search over dynamic encrypted graph database and results verification. In this paper, we propose SecGraph to address the first two limitations, which adopts a novel system architecture that leverages an SGX-enabled cloud server to provide users with secure and transparent search services since the secret key protection and computational overhead have been offloaded to the cloud server. Besides, we design an LDCF-encoded XSet based on the Logarithmic Dynamic Cuckoo Filter to facilitate efficient plaintext computation in trusted memory, effectively mitigating the risks of result pattern leakage and performance degradation due to exceeding the limited trusted memory capacity. Finally, we design a new dynamic version of TSet named Twin-TSet to enable conjunctive search over dynamic encrypted graph database. In order to support verifiable search, we further propose VSecGraph, which utilizes a procedure-oriented verification method to verify all data structures loaded into the trusted memory, thus bypassing the computational overhead associated with the client's local verification.",
        "field": "Digital Identity Verification",
        "link": "http://arxiv.org/abs/2503.10171v1"
    },
    {
        "id": "2503.09513v1",
        "title": "RESTRAIN: Reinforcement Learning-Based Secure Framework for Trigger-Action IoT Environment",
        "authors": [
            "Md Morshed Alam",
            "Lokesh Chandra Das",
            "Sandip Roy",
            "Sachin Shetty",
            "Weichao Wang"
        ],
        "published": "2025-03-12T16:23:14Z",
        "summary": "Internet of Things (IoT) platforms with trigger-action capability allow event conditions to trigger actions in IoT devices autonomously by creating a chain of interactions. Adversaries exploit this chain of interactions to maliciously inject fake event conditions into IoT hubs, triggering unauthorized actions on target IoT devices to implement remote injection attacks. Existing defense mechanisms focus mainly on the verification of event transactions using physical event fingerprints to enforce the security policies to block unsafe event transactions. These approaches are designed to provide offline defense against injection attacks. The state-of-the-art online defense mechanisms offer real-time defense, but extensive reliability on the inference of attack impacts on the IoT network limits the generalization capability of these approaches. In this paper, we propose a platform-independent multi-agent online defense system, namely RESTRAIN, to counter remote injection attacks at runtime. RESTRAIN allows the defense agent to profile attack actions at runtime and leverages reinforcement learning to optimize a defense policy that complies with the security requirements of the IoT network. The experimental results show that the defense agent effectively takes real-time defense actions against complex and dynamic remote injection attacks and maximizes the security gain with minimal computational overhead.",
        "field": "Digital Identity Verification",
        "link": "http://arxiv.org/abs/2503.09513v1"
    },
    {
        "id": "2503.09433v1",
        "title": "CASTLE: Benchmarking Dataset for Static Code Analyzers and LLMs towards CWE Detection",
        "authors": [
            "Richard A. Dubniczky",
            "Krisztofer Zoltán Horvát",
            "Tamás Bisztray",
            "Mohamed Amine Ferrag",
            "Lucas C. Cordeiro",
            "Norbert Tihanyi"
        ],
        "published": "2025-03-12T14:30:05Z",
        "summary": "Identifying vulnerabilities in source code is crucial, especially in critical software components. Existing methods such as static analysis, dynamic analysis, formal verification, and recently Large Language Models are widely used to detect security flaws. This paper introduces CASTLE (CWE Automated Security Testing and Low-Level Evaluation), a benchmarking framework for evaluating the vulnerability detection capabilities of different methods. We assess 13 static analysis tools, 10 LLMs, and 2 formal verification tools using a hand-crafted dataset of 250 micro-benchmark programs covering 25 common CWEs. We propose the CASTLE Score, a novel evaluation metric to ensure fair comparison. Our results reveal key differences: ESBMC (a formal verification tool) minimizes false positives but struggles with vulnerabilities beyond model checking, such as weak cryptography or SQL injection. Static analyzers suffer from high false positives, increasing manual validation efforts for developers. LLMs perform exceptionally well in the CASTLE dataset when identifying vulnerabilities in small code snippets. However, their accuracy declines, and hallucinations increase as the code size grows. These results suggest that LLMs could play a pivotal role in future security solutions, particularly within code completion frameworks, where they can provide real-time guidance to prevent vulnerabilities. The dataset is accessible at https://github.com/CASTLE-Benchmark.",
        "field": "Digital Identity Verification",
        "link": "http://arxiv.org/abs/2503.09433v1"
    },
    {
        "id": "2503.08923v1",
        "title": "Enhancing Large Language Models for Hardware Verification: A Novel SystemVerilog Assertion Dataset",
        "authors": [
            "Anand Menon",
            "Samit S Miftah",
            "Shamik Kundu",
            "Souvik Kundu",
            "Amisha Srivastava",
            "Arnab Raha",
            "Gabriel Theodor Sonnenschein",
            "Suvadeep Banerjee",
            "Deepak Mathaikutty",
            "Kanad Basu"
        ],
        "published": "2025-03-11T22:13:26Z",
        "summary": "Hardware verification is crucial in modern SoC design, consuming around 70% of development time. SystemVerilog assertions ensure correct functionality. However, existing industrial practices rely on manual efforts for assertion generation, which becomes increasingly untenable as hardware systems become complex. Recent research shows that Large Language Models (LLMs) can automate this process. However, proprietary SOTA models like GPT-4o often generate inaccurate assertions and require expensive licenses, while smaller open-source LLMs need fine-tuning to manage HDL code complexities. To address these issues, we introduce **VERT**, an open-source dataset designed to enhance SystemVerilog assertion generation using LLMs. VERT enables researchers in academia and industry to fine-tune open-source models, outperforming larger proprietary ones in both accuracy and efficiency while ensuring data privacy through local fine-tuning and eliminating costly licenses. The dataset is curated by systematically augmenting variables from open-source HDL repositories to generate synthetic code snippets paired with corresponding assertions. Experimental results demonstrate that fine-tuned models like Deepseek Coder 6.7B and Llama 3.1 8B outperform GPT-4o, achieving up to 96.88% improvement over base models and 24.14% over GPT-4o on platforms including OpenTitan, CVA6, OpenPiton and Pulpissimo. VERT is available at https://github.com/AnandMenon12/VERT.",
        "field": "Digital Identity Verification",
        "link": "http://arxiv.org/abs/2503.08923v1"
    },
    {
        "id": "2503.11053v1",
        "title": "Pricing American Parisian Options under General Time-Inhomogeneous Markov Models",
        "authors": [
            "Yuhao Liu",
            "Nian Yang",
            "Gongqiu Zhang"
        ],
        "published": "2025-03-14T03:45:18Z",
        "summary": "This paper develops general approaches for pricing various types of American-style Parisian options (down-in/-out, perpetual/finite-maturity) with general payoff functions based on continuous-time Markov chain (CTMC) approximation under general 1D time-inhomogeneous Markov models. For the down-in types, by conditioning on the Parisian stopping time, we reduce the pricing problem to that of a series of vanilla American options with different maturities and their prices integrated with the distribution function of the Parisian stopping time yield the American Parisian down-in option price. This facilitates an efficient application of CTMC approximation to obtain the approximate option price by calculating the required quantities. For the perpetual down-in cases under time-homogeneous models, significant computational cost can be reduced. The down-out cases are more complicated, for which we use the state augmentation approach to record the excursion duration and then the approximate option price is obtained by solving a series of variational inequalities recursively with the Lemke's pivoting method. We show the convergence of CTMC approximation for all the types of American Parisian options under general time-inhomogeneous Markov models, and the accuracy and efficiency of our algorithms are confirmed with extensive numerical experiments.",
        "field": "Autonomous Finance",
        "link": "http://arxiv.org/abs/2503.11053v1"
    },
    {
        "id": "2503.09988v1",
        "title": "Label Unbalance in High-frequency Trading",
        "authors": [
            "Zijian Zhao",
            "Xuming Chen",
            "Jiayu Wen",
            "Mingwen Liu",
            "Xiaoteng Ma"
        ],
        "published": "2025-03-13T02:55:06Z",
        "summary": "In financial trading, return prediction is one of the foundation for a successful trading system. By the fast development of the deep learning in various areas such as graphical processing, natural language, it has also demonstrate significant edge in handling with financial data. While the success of the deep learning relies on huge amount of labeled sample, labeling each time/event as profitable or unprofitable, under the transaction cost, especially in the high-frequency trading world, suffers from serious label imbalance issue.In this paper, we adopts rigurious end-to-end deep learning framework with comprehensive label imbalance adjustment methods and succeed in predicting in high-frequency return in the Chinese future market. The code for our method is publicly available at https://github.com/RS2002/Label-Unbalance-in-High-Frequency-Trading .",
        "field": "Autonomous Finance",
        "link": "http://arxiv.org/abs/2503.09988v1"
    },
    {
        "id": "2503.09934v1",
        "title": "A Pharmacy Benefit Manager Insurance Business Model",
        "authors": [
            "Lawrence W. Abrams"
        ],
        "published": "2025-03-13T01:13:16Z",
        "summary": "It is time to move on from attempts to make the pharmacy benefit manager (PBM) reseller business model more transparent. Time and time again the Big 3 PBMs have developed opaque alternatives to piece-meal 100% pass-through mandates. Time and time again PBMs have demonstrated expertise in finding loopholes in state government disclosure laws. The purpose of this paper is to provide quantitative estimates of two transparent insurance business models as a solution to the PBM agency issue. The key parameter used is an 8% gross profit margin figure disclosed by the Big 3 PBMs themselves. Based on reported drug trend delivered to plans, we use a $1,200 to $1,500 per member per year (PMPY) as the range for this key performance indicator (KPI). We propose that discussions of PBM insurance business models start with the following figures: (1) a fixed premium model with medical loss ratio ranging from 92% to 85%; (2) a fee-for-service model ranging from $96 to $180 PMPY with risk sharing of deviations from a contracted PMPY delivered drug spend.",
        "field": "Autonomous Finance",
        "link": "http://arxiv.org/abs/2503.09934v1"
    },
    {
        "id": "2503.06707v1",
        "title": "Axes that matter: PCA with a difference",
        "authors": [
            "Brian Huge",
            "Antoine Savine"
        ],
        "published": "2025-03-09T17:47:25Z",
        "summary": "We extend the scope of differential machine learning and introduce a new breed of supervised principal component analysis to reduce dimensionality of Derivatives problems. Applications include the specification and calibration of pricing models, the identification of regression features in least-square Monte-Carlo, and the pre-processing of simulated datasets for (differential) machine learning.",
        "field": "Autonomous Finance",
        "link": "http://arxiv.org/abs/2503.06707v1"
    },
    {
        "id": "2503.06279v1",
        "title": "Mitigating Blockchain extractable value (BEV) threats by Distributed Transaction Sequencing in Blockchains",
        "authors": [
            "Xiongfei Zhao",
            "Hou-Wan Long",
            "Zhengzhe Li",
            "Jiangchuan Liu",
            "Yain-Whar Si"
        ],
        "published": "2025-03-08T16:55:52Z",
        "summary": "The rapid growth of Blockchain and Decentralized Finance (DeFi) has introduced new challenges and vulnerabilities that threaten the integrity and efficiency of the ecosystem. This study identifies critical issues such as Transaction Order Dependence (TOD), Blockchain Extractable Value (BEV), and Transaction Importance Diversity (TID), which collectively undermine the fairness and security of DeFi systems. BEV-related activities, including Sandwich attacks, Liquidations, and Transaction Replay, have emerged as significant threats, collectively generating $540.54 million in losses over 32 months across 11,289 addresses, involving 49,691 cryptocurrencies and 60,830 on-chain markets. These attacks exploit transaction mechanics to manipulate asset prices and extract value at the expense of other participants, with Sandwich attacks being particularly impactful. Additionally, the growing adoption of Blockchain in traditional finance highlights the challenge of TID, where high transaction volumes can strain systems and compromise time-sensitive operations. To address these pressing issues, we propose a novel Distributed Transaction Sequencing Strategy (DTSS), which combines forking mechanisms and the Analytic Hierarchy Process (AHP) to enforce fair and transparent transaction ordering in a decentralized manner. Our approach is further enhanced by an optimization framework and the introduction of the Normalized Allocation Disparity Metric (NADM), which ensures optimal parameter selection for transaction prioritization. Experimental evaluations demonstrate that DTSS effectively mitigates BEV risks, enhances transaction fairness, and significantly improves the security and transparency of DeFi ecosystems. This work is essential for protecting the future of decentralized finance and promoting its integration into global financial systems.",
        "field": "Autonomous Finance",
        "link": "http://arxiv.org/abs/2503.06279v1"
    },
    {
        "id": "2503.09823v1",
        "title": "Data Traceability for Privacy Alignment",
        "authors": [
            "Kevin Liao",
            "Shreya Thipireddy",
            "Daniel Weitzner"
        ],
        "published": "2025-03-12T20:42:23Z",
        "summary": "This paper offers a new privacy approach for the growing ecosystem of services--ranging from open banking to healthcare--dependent on sensitive personal data sharing between individuals and third-parties. While these services offer significant benefits, individuals want control over their data, transparency regarding how their data is used, and accountability from third-parties for misuse. However, existing legal and technical mechanisms are inadequate for supporting these needs. A comprehensive approach to the modern privacy challenges of accountable third-party data sharing requires a closer alignment of technical system architecture and legal institutional design. In order to achieve this privacy alignment, we extend traditional security threat modeling and analysis to encompass a broader range of privacy notions than has been typically considered. In particular, we introduce the concept of covert-accountability, which addresses adversaries that may act dishonestly but face potential identification and legal consequences. As a concrete instance of this design approach, we present the OTrace protocol, designed to provide traceable, accountable, consumer-control in third-party data sharing ecosystems. OTrace empowers consumers with the knowledge of where their data is, who has it, what it is being used for, and whom it is being shared with. By applying our alignment framework to OTrace, we demonstrate that OTrace's technical affordances can provide more confident, scalable regulatory oversight when combined with complementary legal mechanisms.",
        "field": "Synthetic Data in Banking",
        "link": "http://arxiv.org/abs/2503.09823v1"
    },
    {
        "id": "2503.09586v1",
        "title": "Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot",
        "authors": [
            "Andrew Crossman",
            "Andrew R. Plummer",
            "Chandra Sekharudu",
            "Deepak Warrier",
            "Mohammad Yekrangian"
        ],
        "published": "2025-03-12T17:54:18Z",
        "summary": "We present Auspex - a threat modeling system built using a specialized collection of generative artificial intelligence-based methods that capture threat modeling tradecraft. This new approach, called tradecraft prompting, centers on encoding the on-the-ground knowledge of threat modelers within the prompts that drive a generative AI-based threat modeling system. Auspex employs tradecraft prompts in two processing stages. The first stage centers on ingesting and processing system architecture information using prompts that encode threat modeling tradecraft knowledge pertaining to system decomposition and description. The second stage centers on chaining the resulting system analysis through a collection of prompts that encode tradecraft knowledge on threat identification, classification, and mitigation. The two-stage process yields a threat matrix for a system that specifies threat scenarios, threat types, information security categorizations and potential mitigations. Auspex produces formalized threat model output in minutes, relative to the weeks or months a manual process takes. More broadly, the focus on bespoke tradecraft prompting, as opposed to fine-tuning or agent-based add-ons, makes Auspex a lightweight, flexible, modular, and extensible foundational system capable of addressing the complexity, resource, and standardization limitations of both existing manual and automated threat modeling processes. In this connection, we establish the baseline value of Auspex to threat modelers through an evaluation procedure based on feedback collected from cybersecurity subject matter experts measuring the quality and utility of threat models generated by Auspex on real banking systems. We conclude with a discussion of system performance and plans for enhancements to Auspex.",
        "field": "Synthetic Data in Banking",
        "link": "http://arxiv.org/abs/2503.09586v1"
    },
    {
        "id": "2503.00070v1",
        "title": "Systematic Review of Cybersecurity in Banking: Evolution from Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain, Policies and Practice",
        "authors": [
            "Tue Nhi Tran"
        ],
        "published": "2025-02-27T14:17:06Z",
        "summary": "Throughout the history from pre-industry 4.0 to post-industry 4.0, cybersecurity at banks has undergone significant changes. Pre-industry 4.0 cyber security at banks relied on individual security methods that were highly manual and had low accuracy. When moving to post-industry 4.0, cybersecurity at banks had a major turning point with security methods that combined different technologies such as Artificial Intelligence (AI), Blockchain, IoT, automating necessary processes and significantly increasing the defence layer for banks. However, along with the development of new technologies, the current challenge of cybersecurity at banks lies in scalability, high costs and resources in both money and time for R&D of defence methods along with the threat of high-tech cybercriminals growing and expanding. This report goes from introducing the importance of cybersecurity at banks, analyzing their management, operational and business objectives, evaluating pre-industry 4.0 technologies used for cybersecurity at banks to assessing post-industry 4.0 technologies focusing on Artificial Intelligence and Blockchain, discussing current policies and practices and ending with discussing key advantages and challenges for 4.0 technologies and recommendations for further developing cybersecurity at banks.",
        "field": "Synthetic Data in Banking",
        "link": "http://arxiv.org/abs/2503.00070v1"
    },
    {
        "id": "2503.10644v1",
        "title": "Combined climate stress testing of supply-chain networks and the financial system with nation-wide firm-level emission estimates",
        "authors": [
            "Zlata Tabachová",
            "Christian Diem",
            "Johannes Stangl",
            "András Borsos",
            "Stefan Thurner"
        ],
        "published": "2025-02-25T20:25:47Z",
        "summary": "On the way towards carbon neutrality, climate stress testing provides estimates for the physical and transition risks that climate change poses to the economy and the financial system. Missing firm-level CO2 emissions data severely impedes the assessment of transition risks originating from carbon pricing. Based on the individual emissions of all Hungarian firms (410,523), as estimated from their fossil fuel purchases, we conduct a stress test of both actual and hypothetical carbon pricing policies. Using a simple 1:1 economic ABM and introducing the new carbon-to-profit ratio, we identify firms that become unprofitable and default, and estimate the respective loan write-offs. We find that 45% of all companies are directly exposed to carbon pricing. At a price of 45 EUR/t, direct economic losses of 1.3% of total sales and bank equity losses of 1.2% are expected. Secondary default cascades in supply chain networks could increase these losses by 300% to 4000%, depending on firms' ability to substitute essential inputs. To reduce transition risks, firms should reduce their dependence on essential inputs from supply chains with high CO2 exposure. We discuss the implications of different policy implementations on these transition risks.",
        "field": "Synthetic Data in Banking",
        "link": "http://arxiv.org/abs/2503.10644v1"
    },
    {
        "id": "2502.14766v2",
        "title": "Multi-Layer Deep xVA: Structural Credit Models, Measure Changes and Convergence Analysis",
        "authors": [
            "Kristoffer Andersson",
            "Alessandro Gnoatto"
        ],
        "published": "2025-02-20T17:41:55Z",
        "summary": "We propose a structural default model for portfolio-wide valuation adjustments (xVAs) and represent it as a system of coupled backward stochastic differential equations. The framework is divided into four layers, each capturing a key component: (i) clean values, (ii) initial margin and Collateral Valuation Adjustment (ColVA), (iii) Credit/Debit Valuation Adjustments (CVA/DVA) together with Margin Valuation Adjustment (MVA), and (iv) Funding Valuation Adjustment (FVA). Because these layers depend on one another through collateral and default effects, a naive Monte Carlo approach would require deeply nested simulations, making the problem computationally intractable. To address this challenge, we use an iterative deep BSDE approach, handling each layer sequentially so that earlier outputs serve as inputs to the subsequent layers. Initial margin is computed via deep quantile regression to reflect margin requirements over the Margin Period of Risk. We also adopt a change-of-measure method that highlights rare but significant defaults of the bank or counterparty, ensuring that these events are accurately captured in the training process. We further extend Han and Long's (2020) a posteriori error analysis to BSDEs on bounded domains. Due to the random exit from the domain, we obtain an order of convergence of $\\mathcal{O}(h^{1/4-\\epsilon})$ rather than the usual $\\mathcal{O}(h^{1/2})$. Numerical experiments illustrate that this method drastically reduces computational demands and successfully scales to high-dimensional, non-symmetric portfolios. The results confirm its effectiveness and accuracy, offering a practical alternative to nested Monte Carlo simulations in multi-counterparty xVA analyses.",
        "field": "Synthetic Data in Banking",
        "link": "http://arxiv.org/abs/2502.14766v2"
    },
    {
        "id": "2503.10207v1",
        "title": "Efficient Implementation of CRYSTALS-KYBER Key Encapsulation Mechanism on ESP32",
        "authors": [
            "Fabian Segatz",
            "Muhammad Ihsan Al Hafiz"
        ],
        "published": "2025-03-13T09:45:31Z",
        "summary": "Kyber, an IND-CCA2-secure lattice-based post-quantum key-encapsulation mechanism, is the winner of the first post-quantum cryptography standardization process of the US National Institute of Standards and Technology. In this work, we provide an efficient implementation of Kyber on ESP32, a very popular microcontroller for Internet of Things applications. We hand-partition the Kyber algorithm to enable utilization of the ESP32 dual-core architecture, which allows us to speed up its execution by 1.21x (keygen), 1.22x (encaps) and 1.20x (decaps). We also explore the possibility of gaining further improvement by utilizing the ESP32 SHA and AES coprocessor and achieve a culminated speed-up of 1.72x (keygen), 1.84x (encaps) and 1.69x (decaps).",
        "field": "Green Finance Technology",
        "link": "http://arxiv.org/abs/2503.10207v1"
    },
    {
        "id": "2503.10171v1",
        "title": "Verifiable, Efficient and Confidentiality-Preserving Graph Search with Transparency",
        "authors": [
            "Qiuhao Wang",
            "Xu Yang",
            "Yiwei Liu",
            "Saiyu Qi",
            "Hongguang Zhao",
            "Ke Li",
            "Yong Qi"
        ],
        "published": "2025-03-13T08:53:53Z",
        "summary": "Graph databases have garnered extensive attention and research due to their ability to manage relationships between entities efficiently. Today, many graph search services have been outsourced to a third-party server to facilitate storage and computational support. Nevertheless, the outsourcing paradigm may invade the privacy of graphs. PeGraph is the latest scheme achieving encrypted search over social graphs to address the privacy leakage, which maintains two data structures XSet and TSet motivated by the OXT technology to support encrypted conjunctive search. However, PeGraph still exhibits limitations inherent to the underlying OXT. It does not provide transparent search capabilities, suffers from expensive computation and result pattern leakages, and it fails to support search over dynamic encrypted graph database and results verification. In this paper, we propose SecGraph to address the first two limitations, which adopts a novel system architecture that leverages an SGX-enabled cloud server to provide users with secure and transparent search services since the secret key protection and computational overhead have been offloaded to the cloud server. Besides, we design an LDCF-encoded XSet based on the Logarithmic Dynamic Cuckoo Filter to facilitate efficient plaintext computation in trusted memory, effectively mitigating the risks of result pattern leakage and performance degradation due to exceeding the limited trusted memory capacity. Finally, we design a new dynamic version of TSet named Twin-TSet to enable conjunctive search over dynamic encrypted graph database. In order to support verifiable search, we further propose VSecGraph, which utilizes a procedure-oriented verification method to verify all data structures loaded into the trusted memory, thus bypassing the computational overhead associated with the client's local verification.",
        "field": "Green Finance Technology",
        "link": "http://arxiv.org/abs/2503.10171v1"
    },
    {
        "id": "2503.09666v1",
        "title": "Blockchain-Enabled Management Framework for Federated Coalition Networks",
        "authors": [
            "Jorge Álvaro González",
            "Ana María Saiz García",
            "Victor Monzon Baeza"
        ],
        "published": "2025-03-12T16:59:23Z",
        "summary": "In a globalized and interconnected world, interoperability has become a key concept for advancing tactical scenarios. Federated Coalition Networks (FCN) enable cooperation between entities from multiple nations while allowing each to maintain control over their systems. However, this interoperability necessitates the sharing of increasing amounts of information between different tactical assets, raising the need for higher security measures. Emerging technologies like blockchain drive a revolution in secure communications, paving the way for new tactical scenarios. In this work, we propose a blockchain-based framework to enhance the resilience and security of the management of these networks. We offer a guide to FCN design to help a broad audience understand the military networks in international missions by a use case and key functions applied to a proposed architecture. We evaluate its effectiveness and performance in information encryption to validate this framework.",
        "field": "Green Finance Technology",
        "link": "http://arxiv.org/abs/2503.09666v1"
    },
    {
        "id": "2503.09375v1",
        "title": "Quantum Computing and Cybersecurity Education: A Novel Curriculum for Enhancing Graduate STEM Learning",
        "authors": [
            "Suryansh Upadhyay",
            "Koustubh Phalak",
            "Jungeun Lee",
            "Kathleen Mitchell Hill",
            "Swaroop Ghosh"
        ],
        "published": "2025-03-12T13:26:54Z",
        "summary": "Quantum computing is an emerging paradigm with the potential to transform numerous application areas by addressing problems considered intractable in the classical domain. However, its integration into cyberspace introduces significant security and privacy challenges. The exponential rise in cyber attacks, further complicated by quantum capabilities, poses serious risks to financial systems and national security. The scope of quantum threats extends beyond traditional software, operating system, and network vulnerabilities, necessitating a shift in cybersecurity education. Traditional cybersecurity education, often reliant on didactic methods, lacks hands on, student centered learning experiences necessary to prepare students for these evolving challenges. There is an urgent need for curricula that address both classical and quantum security threats through experiential learning. In this work, we present the design and evaluation of EE 597: Introduction to Hardware Security, a graduate level course integrating hands-on quantum security learning with classical security concepts through simulations and cloud-based quantum hardware. Unlike conventional courses focused on quantum threats to cryptographic systems, EE 597 explores security challenges specific to quantum computing itself. We employ a mixed-methods evaluation using pre and post surveys to assess student learning outcomes and engagement. Results indicate significant improvements in students' understanding of quantum and hardware security, with strong positive feedback on course structure and remote instruction (mean scores: 3.33 to 3.83 on a 4 point scale).",
        "field": "Green Finance Technology",
        "link": "http://arxiv.org/abs/2503.09375v1"
    },
    {
        "id": "2503.09327v1",
        "title": "Heuristic-Based Address Clustering in Cardano Blockchain",
        "authors": [
            "Mostafa Chegenizadeh",
            "Sina Rafati Niya",
            "Claudio J. Tessone"
        ],
        "published": "2025-03-12T12:22:26Z",
        "summary": "Blockchain technology has recently gained widespread popularity as a practical method of storing immutable data while preserving the privacy of users by anonymizing their real identities. This anonymization approach, however, significantly complicates the analysis of blockchain data. To address this problem, heuristic-based clustering algorithms as an effective way of linking all addresses controlled by the same entity have been presented in the literature. In this paper, considering the particular features of the Extended Unspent Transaction Outputs accounting model introduced by the Cardano blockchain, two new clustering heuristics are proposed for clustering the Cardano payment addresses. Applying these heuristics and employing the UnionFind algorithm, we efficiently cluster all the addresses that have appeared on the Cardano blockchain from September 2017 to January 2023, where each cluster represents a distinct entity. The results show that each medium-sized entity in the Cardano network owns and controls 9.67 payment addresses on average. The results also confirm that a power law distribution is fitted to the distribution of entity sizes recognized using our proposed heuristics.",
        "field": "Green Finance Technology",
        "link": "http://arxiv.org/abs/2503.09327v1"
    },
    {
        "id": "2503.09823v1",
        "title": "Data Traceability for Privacy Alignment",
        "authors": [
            "Kevin Liao",
            "Shreya Thipireddy",
            "Daniel Weitzner"
        ],
        "published": "2025-03-12T20:42:23Z",
        "summary": "This paper offers a new privacy approach for the growing ecosystem of services--ranging from open banking to healthcare--dependent on sensitive personal data sharing between individuals and third-parties. While these services offer significant benefits, individuals want control over their data, transparency regarding how their data is used, and accountability from third-parties for misuse. However, existing legal and technical mechanisms are inadequate for supporting these needs. A comprehensive approach to the modern privacy challenges of accountable third-party data sharing requires a closer alignment of technical system architecture and legal institutional design. In order to achieve this privacy alignment, we extend traditional security threat modeling and analysis to encompass a broader range of privacy notions than has been typically considered. In particular, we introduce the concept of covert-accountability, which addresses adversaries that may act dishonestly but face potential identification and legal consequences. As a concrete instance of this design approach, we present the OTrace protocol, designed to provide traceable, accountable, consumer-control in third-party data sharing ecosystems. OTrace empowers consumers with the knowledge of where their data is, who has it, what it is being used for, and whom it is being shared with. By applying our alignment framework to OTrace, we demonstrate that OTrace's technical affordances can provide more confident, scalable regulatory oversight when combined with complementary legal mechanisms.",
        "field": "Digital Twin Technology in Banking",
        "link": "http://arxiv.org/abs/2503.09823v1"
    },
    {
        "id": "2503.09586v1",
        "title": "Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot",
        "authors": [
            "Andrew Crossman",
            "Andrew R. Plummer",
            "Chandra Sekharudu",
            "Deepak Warrier",
            "Mohammad Yekrangian"
        ],
        "published": "2025-03-12T17:54:18Z",
        "summary": "We present Auspex - a threat modeling system built using a specialized collection of generative artificial intelligence-based methods that capture threat modeling tradecraft. This new approach, called tradecraft prompting, centers on encoding the on-the-ground knowledge of threat modelers within the prompts that drive a generative AI-based threat modeling system. Auspex employs tradecraft prompts in two processing stages. The first stage centers on ingesting and processing system architecture information using prompts that encode threat modeling tradecraft knowledge pertaining to system decomposition and description. The second stage centers on chaining the resulting system analysis through a collection of prompts that encode tradecraft knowledge on threat identification, classification, and mitigation. The two-stage process yields a threat matrix for a system that specifies threat scenarios, threat types, information security categorizations and potential mitigations. Auspex produces formalized threat model output in minutes, relative to the weeks or months a manual process takes. More broadly, the focus on bespoke tradecraft prompting, as opposed to fine-tuning or agent-based add-ons, makes Auspex a lightweight, flexible, modular, and extensible foundational system capable of addressing the complexity, resource, and standardization limitations of both existing manual and automated threat modeling processes. In this connection, we establish the baseline value of Auspex to threat modelers through an evaluation procedure based on feedback collected from cybersecurity subject matter experts measuring the quality and utility of threat models generated by Auspex on real banking systems. We conclude with a discussion of system performance and plans for enhancements to Auspex.",
        "field": "Digital Twin Technology in Banking",
        "link": "http://arxiv.org/abs/2503.09586v1"
    },
    {
        "id": "2503.00070v1",
        "title": "Systematic Review of Cybersecurity in Banking: Evolution from Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain, Policies and Practice",
        "authors": [
            "Tue Nhi Tran"
        ],
        "published": "2025-02-27T14:17:06Z",
        "summary": "Throughout the history from pre-industry 4.0 to post-industry 4.0, cybersecurity at banks has undergone significant changes. Pre-industry 4.0 cyber security at banks relied on individual security methods that were highly manual and had low accuracy. When moving to post-industry 4.0, cybersecurity at banks had a major turning point with security methods that combined different technologies such as Artificial Intelligence (AI), Blockchain, IoT, automating necessary processes and significantly increasing the defence layer for banks. However, along with the development of new technologies, the current challenge of cybersecurity at banks lies in scalability, high costs and resources in both money and time for R&D of defence methods along with the threat of high-tech cybercriminals growing and expanding. This report goes from introducing the importance of cybersecurity at banks, analyzing their management, operational and business objectives, evaluating pre-industry 4.0 technologies used for cybersecurity at banks to assessing post-industry 4.0 technologies focusing on Artificial Intelligence and Blockchain, discussing current policies and practices and ending with discussing key advantages and challenges for 4.0 technologies and recommendations for further developing cybersecurity at banks.",
        "field": "Digital Twin Technology in Banking",
        "link": "http://arxiv.org/abs/2503.00070v1"
    },
    {
        "id": "2503.10644v1",
        "title": "Combined climate stress testing of supply-chain networks and the financial system with nation-wide firm-level emission estimates",
        "authors": [
            "Zlata Tabachová",
            "Christian Diem",
            "Johannes Stangl",
            "András Borsos",
            "Stefan Thurner"
        ],
        "published": "2025-02-25T20:25:47Z",
        "summary": "On the way towards carbon neutrality, climate stress testing provides estimates for the physical and transition risks that climate change poses to the economy and the financial system. Missing firm-level CO2 emissions data severely impedes the assessment of transition risks originating from carbon pricing. Based on the individual emissions of all Hungarian firms (410,523), as estimated from their fossil fuel purchases, we conduct a stress test of both actual and hypothetical carbon pricing policies. Using a simple 1:1 economic ABM and introducing the new carbon-to-profit ratio, we identify firms that become unprofitable and default, and estimate the respective loan write-offs. We find that 45% of all companies are directly exposed to carbon pricing. At a price of 45 EUR/t, direct economic losses of 1.3% of total sales and bank equity losses of 1.2% are expected. Secondary default cascades in supply chain networks could increase these losses by 300% to 4000%, depending on firms' ability to substitute essential inputs. To reduce transition risks, firms should reduce their dependence on essential inputs from supply chains with high CO2 exposure. We discuss the implications of different policy implementations on these transition risks.",
        "field": "Digital Twin Technology in Banking",
        "link": "http://arxiv.org/abs/2503.10644v1"
    },
    {
        "id": "2502.14766v2",
        "title": "Multi-Layer Deep xVA: Structural Credit Models, Measure Changes and Convergence Analysis",
        "authors": [
            "Kristoffer Andersson",
            "Alessandro Gnoatto"
        ],
        "published": "2025-02-20T17:41:55Z",
        "summary": "We propose a structural default model for portfolio-wide valuation adjustments (xVAs) and represent it as a system of coupled backward stochastic differential equations. The framework is divided into four layers, each capturing a key component: (i) clean values, (ii) initial margin and Collateral Valuation Adjustment (ColVA), (iii) Credit/Debit Valuation Adjustments (CVA/DVA) together with Margin Valuation Adjustment (MVA), and (iv) Funding Valuation Adjustment (FVA). Because these layers depend on one another through collateral and default effects, a naive Monte Carlo approach would require deeply nested simulations, making the problem computationally intractable. To address this challenge, we use an iterative deep BSDE approach, handling each layer sequentially so that earlier outputs serve as inputs to the subsequent layers. Initial margin is computed via deep quantile regression to reflect margin requirements over the Margin Period of Risk. We also adopt a change-of-measure method that highlights rare but significant defaults of the bank or counterparty, ensuring that these events are accurately captured in the training process. We further extend Han and Long's (2020) a posteriori error analysis to BSDEs on bounded domains. Due to the random exit from the domain, we obtain an order of convergence of $\\mathcal{O}(h^{1/4-\\epsilon})$ rather than the usual $\\mathcal{O}(h^{1/2})$. Numerical experiments illustrate that this method drastically reduces computational demands and successfully scales to high-dimensional, non-symmetric portfolios. The results confirm its effectiveness and accuracy, offering a practical alternative to nested Monte Carlo simulations in multi-counterparty xVA analyses.",
        "field": "Digital Twin Technology in Banking",
        "link": "http://arxiv.org/abs/2502.14766v2"
    },
    {
        "id": "2503.09823v1",
        "title": "Data Traceability for Privacy Alignment",
        "authors": [
            "Kevin Liao",
            "Shreya Thipireddy",
            "Daniel Weitzner"
        ],
        "published": "2025-03-12T20:42:23Z",
        "summary": "This paper offers a new privacy approach for the growing ecosystem of services--ranging from open banking to healthcare--dependent on sensitive personal data sharing between individuals and third-parties. While these services offer significant benefits, individuals want control over their data, transparency regarding how their data is used, and accountability from third-parties for misuse. However, existing legal and technical mechanisms are inadequate for supporting these needs. A comprehensive approach to the modern privacy challenges of accountable third-party data sharing requires a closer alignment of technical system architecture and legal institutional design. In order to achieve this privacy alignment, we extend traditional security threat modeling and analysis to encompass a broader range of privacy notions than has been typically considered. In particular, we introduce the concept of covert-accountability, which addresses adversaries that may act dishonestly but face potential identification and legal consequences. As a concrete instance of this design approach, we present the OTrace protocol, designed to provide traceable, accountable, consumer-control in third-party data sharing ecosystems. OTrace empowers consumers with the knowledge of where their data is, who has it, what it is being used for, and whom it is being shared with. By applying our alignment framework to OTrace, we demonstrate that OTrace's technical affordances can provide more confident, scalable regulatory oversight when combined with complementary legal mechanisms.",
        "field": "Quantum Machine Learning in Banking",
        "link": "http://arxiv.org/abs/2503.09823v1"
    },
    {
        "id": "2503.09586v1",
        "title": "Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot",
        "authors": [
            "Andrew Crossman",
            "Andrew R. Plummer",
            "Chandra Sekharudu",
            "Deepak Warrier",
            "Mohammad Yekrangian"
        ],
        "published": "2025-03-12T17:54:18Z",
        "summary": "We present Auspex - a threat modeling system built using a specialized collection of generative artificial intelligence-based methods that capture threat modeling tradecraft. This new approach, called tradecraft prompting, centers on encoding the on-the-ground knowledge of threat modelers within the prompts that drive a generative AI-based threat modeling system. Auspex employs tradecraft prompts in two processing stages. The first stage centers on ingesting and processing system architecture information using prompts that encode threat modeling tradecraft knowledge pertaining to system decomposition and description. The second stage centers on chaining the resulting system analysis through a collection of prompts that encode tradecraft knowledge on threat identification, classification, and mitigation. The two-stage process yields a threat matrix for a system that specifies threat scenarios, threat types, information security categorizations and potential mitigations. Auspex produces formalized threat model output in minutes, relative to the weeks or months a manual process takes. More broadly, the focus on bespoke tradecraft prompting, as opposed to fine-tuning or agent-based add-ons, makes Auspex a lightweight, flexible, modular, and extensible foundational system capable of addressing the complexity, resource, and standardization limitations of both existing manual and automated threat modeling processes. In this connection, we establish the baseline value of Auspex to threat modelers through an evaluation procedure based on feedback collected from cybersecurity subject matter experts measuring the quality and utility of threat models generated by Auspex on real banking systems. We conclude with a discussion of system performance and plans for enhancements to Auspex.",
        "field": "Quantum Machine Learning in Banking",
        "link": "http://arxiv.org/abs/2503.09586v1"
    },
    {
        "id": "2503.00070v1",
        "title": "Systematic Review of Cybersecurity in Banking: Evolution from Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain, Policies and Practice",
        "authors": [
            "Tue Nhi Tran"
        ],
        "published": "2025-02-27T14:17:06Z",
        "summary": "Throughout the history from pre-industry 4.0 to post-industry 4.0, cybersecurity at banks has undergone significant changes. Pre-industry 4.0 cyber security at banks relied on individual security methods that were highly manual and had low accuracy. When moving to post-industry 4.0, cybersecurity at banks had a major turning point with security methods that combined different technologies such as Artificial Intelligence (AI), Blockchain, IoT, automating necessary processes and significantly increasing the defence layer for banks. However, along with the development of new technologies, the current challenge of cybersecurity at banks lies in scalability, high costs and resources in both money and time for R&D of defence methods along with the threat of high-tech cybercriminals growing and expanding. This report goes from introducing the importance of cybersecurity at banks, analyzing their management, operational and business objectives, evaluating pre-industry 4.0 technologies used for cybersecurity at banks to assessing post-industry 4.0 technologies focusing on Artificial Intelligence and Blockchain, discussing current policies and practices and ending with discussing key advantages and challenges for 4.0 technologies and recommendations for further developing cybersecurity at banks.",
        "field": "Quantum Machine Learning in Banking",
        "link": "http://arxiv.org/abs/2503.00070v1"
    },
    {
        "id": "2503.10644v1",
        "title": "Combined climate stress testing of supply-chain networks and the financial system with nation-wide firm-level emission estimates",
        "authors": [
            "Zlata Tabachová",
            "Christian Diem",
            "Johannes Stangl",
            "András Borsos",
            "Stefan Thurner"
        ],
        "published": "2025-02-25T20:25:47Z",
        "summary": "On the way towards carbon neutrality, climate stress testing provides estimates for the physical and transition risks that climate change poses to the economy and the financial system. Missing firm-level CO2 emissions data severely impedes the assessment of transition risks originating from carbon pricing. Based on the individual emissions of all Hungarian firms (410,523), as estimated from their fossil fuel purchases, we conduct a stress test of both actual and hypothetical carbon pricing policies. Using a simple 1:1 economic ABM and introducing the new carbon-to-profit ratio, we identify firms that become unprofitable and default, and estimate the respective loan write-offs. We find that 45% of all companies are directly exposed to carbon pricing. At a price of 45 EUR/t, direct economic losses of 1.3% of total sales and bank equity losses of 1.2% are expected. Secondary default cascades in supply chain networks could increase these losses by 300% to 4000%, depending on firms' ability to substitute essential inputs. To reduce transition risks, firms should reduce their dependence on essential inputs from supply chains with high CO2 exposure. We discuss the implications of different policy implementations on these transition risks.",
        "field": "Quantum Machine Learning in Banking",
        "link": "http://arxiv.org/abs/2503.10644v1"
    },
    {
        "id": "2502.14766v2",
        "title": "Multi-Layer Deep xVA: Structural Credit Models, Measure Changes and Convergence Analysis",
        "authors": [
            "Kristoffer Andersson",
            "Alessandro Gnoatto"
        ],
        "published": "2025-02-20T17:41:55Z",
        "summary": "We propose a structural default model for portfolio-wide valuation adjustments (xVAs) and represent it as a system of coupled backward stochastic differential equations. The framework is divided into four layers, each capturing a key component: (i) clean values, (ii) initial margin and Collateral Valuation Adjustment (ColVA), (iii) Credit/Debit Valuation Adjustments (CVA/DVA) together with Margin Valuation Adjustment (MVA), and (iv) Funding Valuation Adjustment (FVA). Because these layers depend on one another through collateral and default effects, a naive Monte Carlo approach would require deeply nested simulations, making the problem computationally intractable. To address this challenge, we use an iterative deep BSDE approach, handling each layer sequentially so that earlier outputs serve as inputs to the subsequent layers. Initial margin is computed via deep quantile regression to reflect margin requirements over the Margin Period of Risk. We also adopt a change-of-measure method that highlights rare but significant defaults of the bank or counterparty, ensuring that these events are accurately captured in the training process. We further extend Han and Long's (2020) a posteriori error analysis to BSDEs on bounded domains. Due to the random exit from the domain, we obtain an order of convergence of $\\mathcal{O}(h^{1/4-\\epsilon})$ rather than the usual $\\mathcal{O}(h^{1/2})$. Numerical experiments illustrate that this method drastically reduces computational demands and successfully scales to high-dimensional, non-symmetric portfolios. The results confirm its effectiveness and accuracy, offering a practical alternative to nested Monte Carlo simulations in multi-counterparty xVA analyses.",
        "field": "Quantum Machine Learning in Banking",
        "link": "http://arxiv.org/abs/2502.14766v2"
    },
    {
        "id": "2503.09823v1",
        "title": "Data Traceability for Privacy Alignment",
        "authors": [
            "Kevin Liao",
            "Shreya Thipireddy",
            "Daniel Weitzner"
        ],
        "published": "2025-03-12T20:42:23Z",
        "summary": "This paper offers a new privacy approach for the growing ecosystem of services--ranging from open banking to healthcare--dependent on sensitive personal data sharing between individuals and third-parties. While these services offer significant benefits, individuals want control over their data, transparency regarding how their data is used, and accountability from third-parties for misuse. However, existing legal and technical mechanisms are inadequate for supporting these needs. A comprehensive approach to the modern privacy challenges of accountable third-party data sharing requires a closer alignment of technical system architecture and legal institutional design. In order to achieve this privacy alignment, we extend traditional security threat modeling and analysis to encompass a broader range of privacy notions than has been typically considered. In particular, we introduce the concept of covert-accountability, which addresses adversaries that may act dishonestly but face potential identification and legal consequences. As a concrete instance of this design approach, we present the OTrace protocol, designed to provide traceable, accountable, consumer-control in third-party data sharing ecosystems. OTrace empowers consumers with the knowledge of where their data is, who has it, what it is being used for, and whom it is being shared with. By applying our alignment framework to OTrace, we demonstrate that OTrace's technical affordances can provide more confident, scalable regulatory oversight when combined with complementary legal mechanisms.",
        "field": "Augmented Reality (AR) in Banking",
        "link": "http://arxiv.org/abs/2503.09823v1"
    },
    {
        "id": "2503.09586v1",
        "title": "Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot",
        "authors": [
            "Andrew Crossman",
            "Andrew R. Plummer",
            "Chandra Sekharudu",
            "Deepak Warrier",
            "Mohammad Yekrangian"
        ],
        "published": "2025-03-12T17:54:18Z",
        "summary": "We present Auspex - a threat modeling system built using a specialized collection of generative artificial intelligence-based methods that capture threat modeling tradecraft. This new approach, called tradecraft prompting, centers on encoding the on-the-ground knowledge of threat modelers within the prompts that drive a generative AI-based threat modeling system. Auspex employs tradecraft prompts in two processing stages. The first stage centers on ingesting and processing system architecture information using prompts that encode threat modeling tradecraft knowledge pertaining to system decomposition and description. The second stage centers on chaining the resulting system analysis through a collection of prompts that encode tradecraft knowledge on threat identification, classification, and mitigation. The two-stage process yields a threat matrix for a system that specifies threat scenarios, threat types, information security categorizations and potential mitigations. Auspex produces formalized threat model output in minutes, relative to the weeks or months a manual process takes. More broadly, the focus on bespoke tradecraft prompting, as opposed to fine-tuning or agent-based add-ons, makes Auspex a lightweight, flexible, modular, and extensible foundational system capable of addressing the complexity, resource, and standardization limitations of both existing manual and automated threat modeling processes. In this connection, we establish the baseline value of Auspex to threat modelers through an evaluation procedure based on feedback collected from cybersecurity subject matter experts measuring the quality and utility of threat models generated by Auspex on real banking systems. We conclude with a discussion of system performance and plans for enhancements to Auspex.",
        "field": "Augmented Reality (AR) in Banking",
        "link": "http://arxiv.org/abs/2503.09586v1"
    },
    {
        "id": "2503.00070v1",
        "title": "Systematic Review of Cybersecurity in Banking: Evolution from Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain, Policies and Practice",
        "authors": [
            "Tue Nhi Tran"
        ],
        "published": "2025-02-27T14:17:06Z",
        "summary": "Throughout the history from pre-industry 4.0 to post-industry 4.0, cybersecurity at banks has undergone significant changes. Pre-industry 4.0 cyber security at banks relied on individual security methods that were highly manual and had low accuracy. When moving to post-industry 4.0, cybersecurity at banks had a major turning point with security methods that combined different technologies such as Artificial Intelligence (AI), Blockchain, IoT, automating necessary processes and significantly increasing the defence layer for banks. However, along with the development of new technologies, the current challenge of cybersecurity at banks lies in scalability, high costs and resources in both money and time for R&D of defence methods along with the threat of high-tech cybercriminals growing and expanding. This report goes from introducing the importance of cybersecurity at banks, analyzing their management, operational and business objectives, evaluating pre-industry 4.0 technologies used for cybersecurity at banks to assessing post-industry 4.0 technologies focusing on Artificial Intelligence and Blockchain, discussing current policies and practices and ending with discussing key advantages and challenges for 4.0 technologies and recommendations for further developing cybersecurity at banks.",
        "field": "Augmented Reality (AR) in Banking",
        "link": "http://arxiv.org/abs/2503.00070v1"
    },
    {
        "id": "2503.10644v1",
        "title": "Combined climate stress testing of supply-chain networks and the financial system with nation-wide firm-level emission estimates",
        "authors": [
            "Zlata Tabachová",
            "Christian Diem",
            "Johannes Stangl",
            "András Borsos",
            "Stefan Thurner"
        ],
        "published": "2025-02-25T20:25:47Z",
        "summary": "On the way towards carbon neutrality, climate stress testing provides estimates for the physical and transition risks that climate change poses to the economy and the financial system. Missing firm-level CO2 emissions data severely impedes the assessment of transition risks originating from carbon pricing. Based on the individual emissions of all Hungarian firms (410,523), as estimated from their fossil fuel purchases, we conduct a stress test of both actual and hypothetical carbon pricing policies. Using a simple 1:1 economic ABM and introducing the new carbon-to-profit ratio, we identify firms that become unprofitable and default, and estimate the respective loan write-offs. We find that 45% of all companies are directly exposed to carbon pricing. At a price of 45 EUR/t, direct economic losses of 1.3% of total sales and bank equity losses of 1.2% are expected. Secondary default cascades in supply chain networks could increase these losses by 300% to 4000%, depending on firms' ability to substitute essential inputs. To reduce transition risks, firms should reduce their dependence on essential inputs from supply chains with high CO2 exposure. We discuss the implications of different policy implementations on these transition risks.",
        "field": "Augmented Reality (AR) in Banking",
        "link": "http://arxiv.org/abs/2503.10644v1"
    },
    {
        "id": "2502.14766v2",
        "title": "Multi-Layer Deep xVA: Structural Credit Models, Measure Changes and Convergence Analysis",
        "authors": [
            "Kristoffer Andersson",
            "Alessandro Gnoatto"
        ],
        "published": "2025-02-20T17:41:55Z",
        "summary": "We propose a structural default model for portfolio-wide valuation adjustments (xVAs) and represent it as a system of coupled backward stochastic differential equations. The framework is divided into four layers, each capturing a key component: (i) clean values, (ii) initial margin and Collateral Valuation Adjustment (ColVA), (iii) Credit/Debit Valuation Adjustments (CVA/DVA) together with Margin Valuation Adjustment (MVA), and (iv) Funding Valuation Adjustment (FVA). Because these layers depend on one another through collateral and default effects, a naive Monte Carlo approach would require deeply nested simulations, making the problem computationally intractable. To address this challenge, we use an iterative deep BSDE approach, handling each layer sequentially so that earlier outputs serve as inputs to the subsequent layers. Initial margin is computed via deep quantile regression to reflect margin requirements over the Margin Period of Risk. We also adopt a change-of-measure method that highlights rare but significant defaults of the bank or counterparty, ensuring that these events are accurately captured in the training process. We further extend Han and Long's (2020) a posteriori error analysis to BSDEs on bounded domains. Due to the random exit from the domain, we obtain an order of convergence of $\\mathcal{O}(h^{1/4-\\epsilon})$ rather than the usual $\\mathcal{O}(h^{1/2})$. Numerical experiments illustrate that this method drastically reduces computational demands and successfully scales to high-dimensional, non-symmetric portfolios. The results confirm its effectiveness and accuracy, offering a practical alternative to nested Monte Carlo simulations in multi-counterparty xVA analyses.",
        "field": "Augmented Reality (AR) in Banking",
        "link": "http://arxiv.org/abs/2502.14766v2"
    },
    {
        "id": "2503.09823v1",
        "title": "Data Traceability for Privacy Alignment",
        "authors": [
            "Kevin Liao",
            "Shreya Thipireddy",
            "Daniel Weitzner"
        ],
        "published": "2025-03-12T20:42:23Z",
        "summary": "This paper offers a new privacy approach for the growing ecosystem of services--ranging from open banking to healthcare--dependent on sensitive personal data sharing between individuals and third-parties. While these services offer significant benefits, individuals want control over their data, transparency regarding how their data is used, and accountability from third-parties for misuse. However, existing legal and technical mechanisms are inadequate for supporting these needs. A comprehensive approach to the modern privacy challenges of accountable third-party data sharing requires a closer alignment of technical system architecture and legal institutional design. In order to achieve this privacy alignment, we extend traditional security threat modeling and analysis to encompass a broader range of privacy notions than has been typically considered. In particular, we introduce the concept of covert-accountability, which addresses adversaries that may act dishonestly but face potential identification and legal consequences. As a concrete instance of this design approach, we present the OTrace protocol, designed to provide traceable, accountable, consumer-control in third-party data sharing ecosystems. OTrace empowers consumers with the knowledge of where their data is, who has it, what it is being used for, and whom it is being shared with. By applying our alignment framework to OTrace, we demonstrate that OTrace's technical affordances can provide more confident, scalable regulatory oversight when combined with complementary legal mechanisms.",
        "field": "Federated Learning in Banking",
        "link": "http://arxiv.org/abs/2503.09823v1"
    },
    {
        "id": "2503.09586v1",
        "title": "Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot",
        "authors": [
            "Andrew Crossman",
            "Andrew R. Plummer",
            "Chandra Sekharudu",
            "Deepak Warrier",
            "Mohammad Yekrangian"
        ],
        "published": "2025-03-12T17:54:18Z",
        "summary": "We present Auspex - a threat modeling system built using a specialized collection of generative artificial intelligence-based methods that capture threat modeling tradecraft. This new approach, called tradecraft prompting, centers on encoding the on-the-ground knowledge of threat modelers within the prompts that drive a generative AI-based threat modeling system. Auspex employs tradecraft prompts in two processing stages. The first stage centers on ingesting and processing system architecture information using prompts that encode threat modeling tradecraft knowledge pertaining to system decomposition and description. The second stage centers on chaining the resulting system analysis through a collection of prompts that encode tradecraft knowledge on threat identification, classification, and mitigation. The two-stage process yields a threat matrix for a system that specifies threat scenarios, threat types, information security categorizations and potential mitigations. Auspex produces formalized threat model output in minutes, relative to the weeks or months a manual process takes. More broadly, the focus on bespoke tradecraft prompting, as opposed to fine-tuning or agent-based add-ons, makes Auspex a lightweight, flexible, modular, and extensible foundational system capable of addressing the complexity, resource, and standardization limitations of both existing manual and automated threat modeling processes. In this connection, we establish the baseline value of Auspex to threat modelers through an evaluation procedure based on feedback collected from cybersecurity subject matter experts measuring the quality and utility of threat models generated by Auspex on real banking systems. We conclude with a discussion of system performance and plans for enhancements to Auspex.",
        "field": "Federated Learning in Banking",
        "link": "http://arxiv.org/abs/2503.09586v1"
    },
    {
        "id": "2503.00070v1",
        "title": "Systematic Review of Cybersecurity in Banking: Evolution from Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain, Policies and Practice",
        "authors": [
            "Tue Nhi Tran"
        ],
        "published": "2025-02-27T14:17:06Z",
        "summary": "Throughout the history from pre-industry 4.0 to post-industry 4.0, cybersecurity at banks has undergone significant changes. Pre-industry 4.0 cyber security at banks relied on individual security methods that were highly manual and had low accuracy. When moving to post-industry 4.0, cybersecurity at banks had a major turning point with security methods that combined different technologies such as Artificial Intelligence (AI), Blockchain, IoT, automating necessary processes and significantly increasing the defence layer for banks. However, along with the development of new technologies, the current challenge of cybersecurity at banks lies in scalability, high costs and resources in both money and time for R&D of defence methods along with the threat of high-tech cybercriminals growing and expanding. This report goes from introducing the importance of cybersecurity at banks, analyzing their management, operational and business objectives, evaluating pre-industry 4.0 technologies used for cybersecurity at banks to assessing post-industry 4.0 technologies focusing on Artificial Intelligence and Blockchain, discussing current policies and practices and ending with discussing key advantages and challenges for 4.0 technologies and recommendations for further developing cybersecurity at banks.",
        "field": "Federated Learning in Banking",
        "link": "http://arxiv.org/abs/2503.00070v1"
    },
    {
        "id": "2503.10644v1",
        "title": "Combined climate stress testing of supply-chain networks and the financial system with nation-wide firm-level emission estimates",
        "authors": [
            "Zlata Tabachová",
            "Christian Diem",
            "Johannes Stangl",
            "András Borsos",
            "Stefan Thurner"
        ],
        "published": "2025-02-25T20:25:47Z",
        "summary": "On the way towards carbon neutrality, climate stress testing provides estimates for the physical and transition risks that climate change poses to the economy and the financial system. Missing firm-level CO2 emissions data severely impedes the assessment of transition risks originating from carbon pricing. Based on the individual emissions of all Hungarian firms (410,523), as estimated from their fossil fuel purchases, we conduct a stress test of both actual and hypothetical carbon pricing policies. Using a simple 1:1 economic ABM and introducing the new carbon-to-profit ratio, we identify firms that become unprofitable and default, and estimate the respective loan write-offs. We find that 45% of all companies are directly exposed to carbon pricing. At a price of 45 EUR/t, direct economic losses of 1.3% of total sales and bank equity losses of 1.2% are expected. Secondary default cascades in supply chain networks could increase these losses by 300% to 4000%, depending on firms' ability to substitute essential inputs. To reduce transition risks, firms should reduce their dependence on essential inputs from supply chains with high CO2 exposure. We discuss the implications of different policy implementations on these transition risks.",
        "field": "Federated Learning in Banking",
        "link": "http://arxiv.org/abs/2503.10644v1"
    },
    {
        "id": "2502.14766v2",
        "title": "Multi-Layer Deep xVA: Structural Credit Models, Measure Changes and Convergence Analysis",
        "authors": [
            "Kristoffer Andersson",
            "Alessandro Gnoatto"
        ],
        "published": "2025-02-20T17:41:55Z",
        "summary": "We propose a structural default model for portfolio-wide valuation adjustments (xVAs) and represent it as a system of coupled backward stochastic differential equations. The framework is divided into four layers, each capturing a key component: (i) clean values, (ii) initial margin and Collateral Valuation Adjustment (ColVA), (iii) Credit/Debit Valuation Adjustments (CVA/DVA) together with Margin Valuation Adjustment (MVA), and (iv) Funding Valuation Adjustment (FVA). Because these layers depend on one another through collateral and default effects, a naive Monte Carlo approach would require deeply nested simulations, making the problem computationally intractable. To address this challenge, we use an iterative deep BSDE approach, handling each layer sequentially so that earlier outputs serve as inputs to the subsequent layers. Initial margin is computed via deep quantile regression to reflect margin requirements over the Margin Period of Risk. We also adopt a change-of-measure method that highlights rare but significant defaults of the bank or counterparty, ensuring that these events are accurately captured in the training process. We further extend Han and Long's (2020) a posteriori error analysis to BSDEs on bounded domains. Due to the random exit from the domain, we obtain an order of convergence of $\\mathcal{O}(h^{1/4-\\epsilon})$ rather than the usual $\\mathcal{O}(h^{1/2})$. Numerical experiments illustrate that this method drastically reduces computational demands and successfully scales to high-dimensional, non-symmetric portfolios. The results confirm its effectiveness and accuracy, offering a practical alternative to nested Monte Carlo simulations in multi-counterparty xVA analyses.",
        "field": "Federated Learning in Banking",
        "link": "http://arxiv.org/abs/2502.14766v2"
    },
    {
        "id": "1603.00991v2",
        "title": "Financial Services, Economic Growth and Well-Being: A Four-Pronged Study",
        "authors": [
            "Ravi Kashyap"
        ],
        "published": "2016-03-03T06:35:43Z",
        "summary": "A four-pronged approach to dealing with Social Science Phenomenon is outlined. This methodology is applied to Financial Services, Economic Growth and Well-Being. The four prongs are like the four directions for an army general looking for victory. Just like the four directions, we need to be aware that there is a degree of interconnectedness in the below four prongs. -Uncertainty Principle of the Social Sciences -Responsibilities of Fiscal Janitors -Need for Smaller Organizations -Redirecting Growth that Generates Garbage The importance of gaining a more profound comprehension of welfare and delineating its components into those that result from an increase in goods and services, and hence can be attributed to economic growth, and into those that are not related to economic growth but lead to a better quality of life, is highlighted. The reasoning being that economic growth alone is an inadequate indicator of well-being. Hand in hand with a better understanding of the characteristics of welfare, comes the need to consider the metrics we currently have that gauge economic growth and supplement those with measures that capture well-being more holistically.",
        "field": "Digital Therapeutics in Financial Well-being",
        "link": "http://arxiv.org/abs/1603.00991v2"
    },
    {
        "id": "2503.09823v1",
        "title": "Data Traceability for Privacy Alignment",
        "authors": [
            "Kevin Liao",
            "Shreya Thipireddy",
            "Daniel Weitzner"
        ],
        "published": "2025-03-12T20:42:23Z",
        "summary": "This paper offers a new privacy approach for the growing ecosystem of services--ranging from open banking to healthcare--dependent on sensitive personal data sharing between individuals and third-parties. While these services offer significant benefits, individuals want control over their data, transparency regarding how their data is used, and accountability from third-parties for misuse. However, existing legal and technical mechanisms are inadequate for supporting these needs. A comprehensive approach to the modern privacy challenges of accountable third-party data sharing requires a closer alignment of technical system architecture and legal institutional design. In order to achieve this privacy alignment, we extend traditional security threat modeling and analysis to encompass a broader range of privacy notions than has been typically considered. In particular, we introduce the concept of covert-accountability, which addresses adversaries that may act dishonestly but face potential identification and legal consequences. As a concrete instance of this design approach, we present the OTrace protocol, designed to provide traceable, accountable, consumer-control in third-party data sharing ecosystems. OTrace empowers consumers with the knowledge of where their data is, who has it, what it is being used for, and whom it is being shared with. By applying our alignment framework to OTrace, we demonstrate that OTrace's technical affordances can provide more confident, scalable regulatory oversight when combined with complementary legal mechanisms.",
        "field": "Blockchain Interoperability in Banking",
        "link": "http://arxiv.org/abs/2503.09823v1"
    },
    {
        "id": "2503.09586v1",
        "title": "Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot",
        "authors": [
            "Andrew Crossman",
            "Andrew R. Plummer",
            "Chandra Sekharudu",
            "Deepak Warrier",
            "Mohammad Yekrangian"
        ],
        "published": "2025-03-12T17:54:18Z",
        "summary": "We present Auspex - a threat modeling system built using a specialized collection of generative artificial intelligence-based methods that capture threat modeling tradecraft. This new approach, called tradecraft prompting, centers on encoding the on-the-ground knowledge of threat modelers within the prompts that drive a generative AI-based threat modeling system. Auspex employs tradecraft prompts in two processing stages. The first stage centers on ingesting and processing system architecture information using prompts that encode threat modeling tradecraft knowledge pertaining to system decomposition and description. The second stage centers on chaining the resulting system analysis through a collection of prompts that encode tradecraft knowledge on threat identification, classification, and mitigation. The two-stage process yields a threat matrix for a system that specifies threat scenarios, threat types, information security categorizations and potential mitigations. Auspex produces formalized threat model output in minutes, relative to the weeks or months a manual process takes. More broadly, the focus on bespoke tradecraft prompting, as opposed to fine-tuning or agent-based add-ons, makes Auspex a lightweight, flexible, modular, and extensible foundational system capable of addressing the complexity, resource, and standardization limitations of both existing manual and automated threat modeling processes. In this connection, we establish the baseline value of Auspex to threat modelers through an evaluation procedure based on feedback collected from cybersecurity subject matter experts measuring the quality and utility of threat models generated by Auspex on real banking systems. We conclude with a discussion of system performance and plans for enhancements to Auspex.",
        "field": "Blockchain Interoperability in Banking",
        "link": "http://arxiv.org/abs/2503.09586v1"
    },
    {
        "id": "2503.00070v1",
        "title": "Systematic Review of Cybersecurity in Banking: Evolution from Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain, Policies and Practice",
        "authors": [
            "Tue Nhi Tran"
        ],
        "published": "2025-02-27T14:17:06Z",
        "summary": "Throughout the history from pre-industry 4.0 to post-industry 4.0, cybersecurity at banks has undergone significant changes. Pre-industry 4.0 cyber security at banks relied on individual security methods that were highly manual and had low accuracy. When moving to post-industry 4.0, cybersecurity at banks had a major turning point with security methods that combined different technologies such as Artificial Intelligence (AI), Blockchain, IoT, automating necessary processes and significantly increasing the defence layer for banks. However, along with the development of new technologies, the current challenge of cybersecurity at banks lies in scalability, high costs and resources in both money and time for R&D of defence methods along with the threat of high-tech cybercriminals growing and expanding. This report goes from introducing the importance of cybersecurity at banks, analyzing their management, operational and business objectives, evaluating pre-industry 4.0 technologies used for cybersecurity at banks to assessing post-industry 4.0 technologies focusing on Artificial Intelligence and Blockchain, discussing current policies and practices and ending with discussing key advantages and challenges for 4.0 technologies and recommendations for further developing cybersecurity at banks.",
        "field": "Blockchain Interoperability in Banking",
        "link": "http://arxiv.org/abs/2503.00070v1"
    },
    {
        "id": "2503.10644v1",
        "title": "Combined climate stress testing of supply-chain networks and the financial system with nation-wide firm-level emission estimates",
        "authors": [
            "Zlata Tabachová",
            "Christian Diem",
            "Johannes Stangl",
            "András Borsos",
            "Stefan Thurner"
        ],
        "published": "2025-02-25T20:25:47Z",
        "summary": "On the way towards carbon neutrality, climate stress testing provides estimates for the physical and transition risks that climate change poses to the economy and the financial system. Missing firm-level CO2 emissions data severely impedes the assessment of transition risks originating from carbon pricing. Based on the individual emissions of all Hungarian firms (410,523), as estimated from their fossil fuel purchases, we conduct a stress test of both actual and hypothetical carbon pricing policies. Using a simple 1:1 economic ABM and introducing the new carbon-to-profit ratio, we identify firms that become unprofitable and default, and estimate the respective loan write-offs. We find that 45% of all companies are directly exposed to carbon pricing. At a price of 45 EUR/t, direct economic losses of 1.3% of total sales and bank equity losses of 1.2% are expected. Secondary default cascades in supply chain networks could increase these losses by 300% to 4000%, depending on firms' ability to substitute essential inputs. To reduce transition risks, firms should reduce their dependence on essential inputs from supply chains with high CO2 exposure. We discuss the implications of different policy implementations on these transition risks.",
        "field": "Blockchain Interoperability in Banking",
        "link": "http://arxiv.org/abs/2503.10644v1"
    },
    {
        "id": "2502.14766v2",
        "title": "Multi-Layer Deep xVA: Structural Credit Models, Measure Changes and Convergence Analysis",
        "authors": [
            "Kristoffer Andersson",
            "Alessandro Gnoatto"
        ],
        "published": "2025-02-20T17:41:55Z",
        "summary": "We propose a structural default model for portfolio-wide valuation adjustments (xVAs) and represent it as a system of coupled backward stochastic differential equations. The framework is divided into four layers, each capturing a key component: (i) clean values, (ii) initial margin and Collateral Valuation Adjustment (ColVA), (iii) Credit/Debit Valuation Adjustments (CVA/DVA) together with Margin Valuation Adjustment (MVA), and (iv) Funding Valuation Adjustment (FVA). Because these layers depend on one another through collateral and default effects, a naive Monte Carlo approach would require deeply nested simulations, making the problem computationally intractable. To address this challenge, we use an iterative deep BSDE approach, handling each layer sequentially so that earlier outputs serve as inputs to the subsequent layers. Initial margin is computed via deep quantile regression to reflect margin requirements over the Margin Period of Risk. We also adopt a change-of-measure method that highlights rare but significant defaults of the bank or counterparty, ensuring that these events are accurately captured in the training process. We further extend Han and Long's (2020) a posteriori error analysis to BSDEs on bounded domains. Due to the random exit from the domain, we obtain an order of convergence of $\\mathcal{O}(h^{1/4-\\epsilon})$ rather than the usual $\\mathcal{O}(h^{1/2})$. Numerical experiments illustrate that this method drastically reduces computational demands and successfully scales to high-dimensional, non-symmetric portfolios. The results confirm its effectiveness and accuracy, offering a practical alternative to nested Monte Carlo simulations in multi-counterparty xVA analyses.",
        "field": "Blockchain Interoperability in Banking",
        "link": "http://arxiv.org/abs/2502.14766v2"
    },
    {
        "id": "2503.09823v1",
        "title": "Data Traceability for Privacy Alignment",
        "authors": [
            "Kevin Liao",
            "Shreya Thipireddy",
            "Daniel Weitzner"
        ],
        "published": "2025-03-12T20:42:23Z",
        "summary": "This paper offers a new privacy approach for the growing ecosystem of services--ranging from open banking to healthcare--dependent on sensitive personal data sharing between individuals and third-parties. While these services offer significant benefits, individuals want control over their data, transparency regarding how their data is used, and accountability from third-parties for misuse. However, existing legal and technical mechanisms are inadequate for supporting these needs. A comprehensive approach to the modern privacy challenges of accountable third-party data sharing requires a closer alignment of technical system architecture and legal institutional design. In order to achieve this privacy alignment, we extend traditional security threat modeling and analysis to encompass a broader range of privacy notions than has been typically considered. In particular, we introduce the concept of covert-accountability, which addresses adversaries that may act dishonestly but face potential identification and legal consequences. As a concrete instance of this design approach, we present the OTrace protocol, designed to provide traceable, accountable, consumer-control in third-party data sharing ecosystems. OTrace empowers consumers with the knowledge of where their data is, who has it, what it is being used for, and whom it is being shared with. By applying our alignment framework to OTrace, we demonstrate that OTrace's technical affordances can provide more confident, scalable regulatory oversight when combined with complementary legal mechanisms.",
        "field": "Metaverse Banking",
        "link": "http://arxiv.org/abs/2503.09823v1"
    },
    {
        "id": "2503.09586v1",
        "title": "Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot",
        "authors": [
            "Andrew Crossman",
            "Andrew R. Plummer",
            "Chandra Sekharudu",
            "Deepak Warrier",
            "Mohammad Yekrangian"
        ],
        "published": "2025-03-12T17:54:18Z",
        "summary": "We present Auspex - a threat modeling system built using a specialized collection of generative artificial intelligence-based methods that capture threat modeling tradecraft. This new approach, called tradecraft prompting, centers on encoding the on-the-ground knowledge of threat modelers within the prompts that drive a generative AI-based threat modeling system. Auspex employs tradecraft prompts in two processing stages. The first stage centers on ingesting and processing system architecture information using prompts that encode threat modeling tradecraft knowledge pertaining to system decomposition and description. The second stage centers on chaining the resulting system analysis through a collection of prompts that encode tradecraft knowledge on threat identification, classification, and mitigation. The two-stage process yields a threat matrix for a system that specifies threat scenarios, threat types, information security categorizations and potential mitigations. Auspex produces formalized threat model output in minutes, relative to the weeks or months a manual process takes. More broadly, the focus on bespoke tradecraft prompting, as opposed to fine-tuning or agent-based add-ons, makes Auspex a lightweight, flexible, modular, and extensible foundational system capable of addressing the complexity, resource, and standardization limitations of both existing manual and automated threat modeling processes. In this connection, we establish the baseline value of Auspex to threat modelers through an evaluation procedure based on feedback collected from cybersecurity subject matter experts measuring the quality and utility of threat models generated by Auspex on real banking systems. We conclude with a discussion of system performance and plans for enhancements to Auspex.",
        "field": "Metaverse Banking",
        "link": "http://arxiv.org/abs/2503.09586v1"
    },
    {
        "id": "2503.00070v1",
        "title": "Systematic Review of Cybersecurity in Banking: Evolution from Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain, Policies and Practice",
        "authors": [
            "Tue Nhi Tran"
        ],
        "published": "2025-02-27T14:17:06Z",
        "summary": "Throughout the history from pre-industry 4.0 to post-industry 4.0, cybersecurity at banks has undergone significant changes. Pre-industry 4.0 cyber security at banks relied on individual security methods that were highly manual and had low accuracy. When moving to post-industry 4.0, cybersecurity at banks had a major turning point with security methods that combined different technologies such as Artificial Intelligence (AI), Blockchain, IoT, automating necessary processes and significantly increasing the defence layer for banks. However, along with the development of new technologies, the current challenge of cybersecurity at banks lies in scalability, high costs and resources in both money and time for R&D of defence methods along with the threat of high-tech cybercriminals growing and expanding. This report goes from introducing the importance of cybersecurity at banks, analyzing their management, operational and business objectives, evaluating pre-industry 4.0 technologies used for cybersecurity at banks to assessing post-industry 4.0 technologies focusing on Artificial Intelligence and Blockchain, discussing current policies and practices and ending with discussing key advantages and challenges for 4.0 technologies and recommendations for further developing cybersecurity at banks.",
        "field": "Metaverse Banking",
        "link": "http://arxiv.org/abs/2503.00070v1"
    },
    {
        "id": "2503.10644v1",
        "title": "Combined climate stress testing of supply-chain networks and the financial system with nation-wide firm-level emission estimates",
        "authors": [
            "Zlata Tabachová",
            "Christian Diem",
            "Johannes Stangl",
            "András Borsos",
            "Stefan Thurner"
        ],
        "published": "2025-02-25T20:25:47Z",
        "summary": "On the way towards carbon neutrality, climate stress testing provides estimates for the physical and transition risks that climate change poses to the economy and the financial system. Missing firm-level CO2 emissions data severely impedes the assessment of transition risks originating from carbon pricing. Based on the individual emissions of all Hungarian firms (410,523), as estimated from their fossil fuel purchases, we conduct a stress test of both actual and hypothetical carbon pricing policies. Using a simple 1:1 economic ABM and introducing the new carbon-to-profit ratio, we identify firms that become unprofitable and default, and estimate the respective loan write-offs. We find that 45% of all companies are directly exposed to carbon pricing. At a price of 45 EUR/t, direct economic losses of 1.3% of total sales and bank equity losses of 1.2% are expected. Secondary default cascades in supply chain networks could increase these losses by 300% to 4000%, depending on firms' ability to substitute essential inputs. To reduce transition risks, firms should reduce their dependence on essential inputs from supply chains with high CO2 exposure. We discuss the implications of different policy implementations on these transition risks.",
        "field": "Metaverse Banking",
        "link": "http://arxiv.org/abs/2503.10644v1"
    },
    {
        "id": "2502.14766v2",
        "title": "Multi-Layer Deep xVA: Structural Credit Models, Measure Changes and Convergence Analysis",
        "authors": [
            "Kristoffer Andersson",
            "Alessandro Gnoatto"
        ],
        "published": "2025-02-20T17:41:55Z",
        "summary": "We propose a structural default model for portfolio-wide valuation adjustments (xVAs) and represent it as a system of coupled backward stochastic differential equations. The framework is divided into four layers, each capturing a key component: (i) clean values, (ii) initial margin and Collateral Valuation Adjustment (ColVA), (iii) Credit/Debit Valuation Adjustments (CVA/DVA) together with Margin Valuation Adjustment (MVA), and (iv) Funding Valuation Adjustment (FVA). Because these layers depend on one another through collateral and default effects, a naive Monte Carlo approach would require deeply nested simulations, making the problem computationally intractable. To address this challenge, we use an iterative deep BSDE approach, handling each layer sequentially so that earlier outputs serve as inputs to the subsequent layers. Initial margin is computed via deep quantile regression to reflect margin requirements over the Margin Period of Risk. We also adopt a change-of-measure method that highlights rare but significant defaults of the bank or counterparty, ensuring that these events are accurately captured in the training process. We further extend Han and Long's (2020) a posteriori error analysis to BSDEs on bounded domains. Due to the random exit from the domain, we obtain an order of convergence of $\\mathcal{O}(h^{1/4-\\epsilon})$ rather than the usual $\\mathcal{O}(h^{1/2})$. Numerical experiments illustrate that this method drastically reduces computational demands and successfully scales to high-dimensional, non-symmetric portfolios. The results confirm its effectiveness and accuracy, offering a practical alternative to nested Monte Carlo simulations in multi-counterparty xVA analyses.",
        "field": "Metaverse Banking",
        "link": "http://arxiv.org/abs/2502.14766v2"
    },
    {
        "id": "2503.09823v1",
        "title": "Data Traceability for Privacy Alignment",
        "authors": [
            "Kevin Liao",
            "Shreya Thipireddy",
            "Daniel Weitzner"
        ],
        "published": "2025-03-12T20:42:23Z",
        "summary": "This paper offers a new privacy approach for the growing ecosystem of services--ranging from open banking to healthcare--dependent on sensitive personal data sharing between individuals and third-parties. While these services offer significant benefits, individuals want control over their data, transparency regarding how their data is used, and accountability from third-parties for misuse. However, existing legal and technical mechanisms are inadequate for supporting these needs. A comprehensive approach to the modern privacy challenges of accountable third-party data sharing requires a closer alignment of technical system architecture and legal institutional design. In order to achieve this privacy alignment, we extend traditional security threat modeling and analysis to encompass a broader range of privacy notions than has been typically considered. In particular, we introduce the concept of covert-accountability, which addresses adversaries that may act dishonestly but face potential identification and legal consequences. As a concrete instance of this design approach, we present the OTrace protocol, designed to provide traceable, accountable, consumer-control in third-party data sharing ecosystems. OTrace empowers consumers with the knowledge of where their data is, who has it, what it is being used for, and whom it is being shared with. By applying our alignment framework to OTrace, we demonstrate that OTrace's technical affordances can provide more confident, scalable regulatory oversight when combined with complementary legal mechanisms.",
        "field": "Quantum Cryptography in Banking",
        "link": "http://arxiv.org/abs/2503.09823v1"
    },
    {
        "id": "2503.09586v1",
        "title": "Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot",
        "authors": [
            "Andrew Crossman",
            "Andrew R. Plummer",
            "Chandra Sekharudu",
            "Deepak Warrier",
            "Mohammad Yekrangian"
        ],
        "published": "2025-03-12T17:54:18Z",
        "summary": "We present Auspex - a threat modeling system built using a specialized collection of generative artificial intelligence-based methods that capture threat modeling tradecraft. This new approach, called tradecraft prompting, centers on encoding the on-the-ground knowledge of threat modelers within the prompts that drive a generative AI-based threat modeling system. Auspex employs tradecraft prompts in two processing stages. The first stage centers on ingesting and processing system architecture information using prompts that encode threat modeling tradecraft knowledge pertaining to system decomposition and description. The second stage centers on chaining the resulting system analysis through a collection of prompts that encode tradecraft knowledge on threat identification, classification, and mitigation. The two-stage process yields a threat matrix for a system that specifies threat scenarios, threat types, information security categorizations and potential mitigations. Auspex produces formalized threat model output in minutes, relative to the weeks or months a manual process takes. More broadly, the focus on bespoke tradecraft prompting, as opposed to fine-tuning or agent-based add-ons, makes Auspex a lightweight, flexible, modular, and extensible foundational system capable of addressing the complexity, resource, and standardization limitations of both existing manual and automated threat modeling processes. In this connection, we establish the baseline value of Auspex to threat modelers through an evaluation procedure based on feedback collected from cybersecurity subject matter experts measuring the quality and utility of threat models generated by Auspex on real banking systems. We conclude with a discussion of system performance and plans for enhancements to Auspex.",
        "field": "Quantum Cryptography in Banking",
        "link": "http://arxiv.org/abs/2503.09586v1"
    },
    {
        "id": "2503.00070v1",
        "title": "Systematic Review of Cybersecurity in Banking: Evolution from Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain, Policies and Practice",
        "authors": [
            "Tue Nhi Tran"
        ],
        "published": "2025-02-27T14:17:06Z",
        "summary": "Throughout the history from pre-industry 4.0 to post-industry 4.0, cybersecurity at banks has undergone significant changes. Pre-industry 4.0 cyber security at banks relied on individual security methods that were highly manual and had low accuracy. When moving to post-industry 4.0, cybersecurity at banks had a major turning point with security methods that combined different technologies such as Artificial Intelligence (AI), Blockchain, IoT, automating necessary processes and significantly increasing the defence layer for banks. However, along with the development of new technologies, the current challenge of cybersecurity at banks lies in scalability, high costs and resources in both money and time for R&D of defence methods along with the threat of high-tech cybercriminals growing and expanding. This report goes from introducing the importance of cybersecurity at banks, analyzing their management, operational and business objectives, evaluating pre-industry 4.0 technologies used for cybersecurity at banks to assessing post-industry 4.0 technologies focusing on Artificial Intelligence and Blockchain, discussing current policies and practices and ending with discussing key advantages and challenges for 4.0 technologies and recommendations for further developing cybersecurity at banks.",
        "field": "Quantum Cryptography in Banking",
        "link": "http://arxiv.org/abs/2503.00070v1"
    },
    {
        "id": "2503.10644v1",
        "title": "Combined climate stress testing of supply-chain networks and the financial system with nation-wide firm-level emission estimates",
        "authors": [
            "Zlata Tabachová",
            "Christian Diem",
            "Johannes Stangl",
            "András Borsos",
            "Stefan Thurner"
        ],
        "published": "2025-02-25T20:25:47Z",
        "summary": "On the way towards carbon neutrality, climate stress testing provides estimates for the physical and transition risks that climate change poses to the economy and the financial system. Missing firm-level CO2 emissions data severely impedes the assessment of transition risks originating from carbon pricing. Based on the individual emissions of all Hungarian firms (410,523), as estimated from their fossil fuel purchases, we conduct a stress test of both actual and hypothetical carbon pricing policies. Using a simple 1:1 economic ABM and introducing the new carbon-to-profit ratio, we identify firms that become unprofitable and default, and estimate the respective loan write-offs. We find that 45% of all companies are directly exposed to carbon pricing. At a price of 45 EUR/t, direct economic losses of 1.3% of total sales and bank equity losses of 1.2% are expected. Secondary default cascades in supply chain networks could increase these losses by 300% to 4000%, depending on firms' ability to substitute essential inputs. To reduce transition risks, firms should reduce their dependence on essential inputs from supply chains with high CO2 exposure. We discuss the implications of different policy implementations on these transition risks.",
        "field": "Quantum Cryptography in Banking",
        "link": "http://arxiv.org/abs/2503.10644v1"
    },
    {
        "id": "2502.14766v2",
        "title": "Multi-Layer Deep xVA: Structural Credit Models, Measure Changes and Convergence Analysis",
        "authors": [
            "Kristoffer Andersson",
            "Alessandro Gnoatto"
        ],
        "published": "2025-02-20T17:41:55Z",
        "summary": "We propose a structural default model for portfolio-wide valuation adjustments (xVAs) and represent it as a system of coupled backward stochastic differential equations. The framework is divided into four layers, each capturing a key component: (i) clean values, (ii) initial margin and Collateral Valuation Adjustment (ColVA), (iii) Credit/Debit Valuation Adjustments (CVA/DVA) together with Margin Valuation Adjustment (MVA), and (iv) Funding Valuation Adjustment (FVA). Because these layers depend on one another through collateral and default effects, a naive Monte Carlo approach would require deeply nested simulations, making the problem computationally intractable. To address this challenge, we use an iterative deep BSDE approach, handling each layer sequentially so that earlier outputs serve as inputs to the subsequent layers. Initial margin is computed via deep quantile regression to reflect margin requirements over the Margin Period of Risk. We also adopt a change-of-measure method that highlights rare but significant defaults of the bank or counterparty, ensuring that these events are accurately captured in the training process. We further extend Han and Long's (2020) a posteriori error analysis to BSDEs on bounded domains. Due to the random exit from the domain, we obtain an order of convergence of $\\mathcal{O}(h^{1/4-\\epsilon})$ rather than the usual $\\mathcal{O}(h^{1/2})$. Numerical experiments illustrate that this method drastically reduces computational demands and successfully scales to high-dimensional, non-symmetric portfolios. The results confirm its effectiveness and accuracy, offering a practical alternative to nested Monte Carlo simulations in multi-counterparty xVA analyses.",
        "field": "Quantum Cryptography in Banking",
        "link": "http://arxiv.org/abs/2502.14766v2"
    },
    {
        "id": "2503.11216v1",
        "title": "Cross-Platform Benchmarking of the FHE Libraries: Novel Insights into SEAL and Openfhe",
        "authors": [
            "Faneela",
            "Jawad Ahmad",
            "Baraq Ghaleb",
            "Sana Ullah Jan",
            "William J. Buchanan"
        ],
        "published": "2025-03-14T09:08:30Z",
        "summary": "The rapid growth of cloud computing and data-driven applications has amplified privacy concerns, driven by the increasing demand to process sensitive data securely. Homomorphic encryption (HE) has become a vital solution for addressing these concerns by enabling computations on encrypted data without revealing its contents. This paper provides a comprehensive evaluation of two leading HE libraries, SEAL and OpenFHE, examining their performance, usability, and support for prominent HE schemes such as BGV and CKKS. Our analysis highlights computational efficiency, memory usage, and scalability across Linux and Windows platforms, emphasizing their applicability in real-world scenarios. Results reveal that Linux outperforms Windows in computation efficiency, with OpenFHE emerging as the optimal choice across diverse cryptographic settings. This paper provides valuable insights for researchers and practitioners to advance privacy-preserving applications using FHE.",
        "field": "Digital Asset Custody Solutions",
        "link": "http://arxiv.org/abs/2503.11216v1"
    },
    {
        "id": "2503.10058v1",
        "title": "Deep Learning Approaches for Anti-Money Laundering on Mobile Transactions: Review, Framework, and Directions",
        "authors": [
            "Jiani Fan",
            "Lwin Khin Shar",
            "Ruichen Zhang",
            "Ziyao Liu",
            "Wenzhuo Yang",
            "Dusit Niyato",
            "Bomin Mao",
            "Kwok-Yan Lam"
        ],
        "published": "2025-03-13T05:19:44Z",
        "summary": "Money laundering is a financial crime that obscures the origin of illicit funds, necessitating the development and enforcement of anti-money laundering (AML) policies by governments and organizations. The proliferation of mobile payment platforms and smart IoT devices has significantly complicated AML investigations. As payment networks become more interconnected, there is an increasing need for efficient real-time detection to process large volumes of transaction data on heterogeneous payment systems by different operators such as digital currencies, cryptocurrencies and account-based payments. Most of these mobile payment networks are supported by connected devices, many of which are considered loT devices in the FinTech space that constantly generate data. Furthermore, the growing complexity and unpredictability of transaction patterns across these networks contribute to a higher incidence of false positives. While machine learning solutions have the potential to enhance detection efficiency, their application in AML faces unique challenges, such as addressing privacy concerns tied to sensitive financial data and managing the real-world constraint of limited data availability due to data regulations. Existing surveys in the AML literature broadly review machine learning approaches for money laundering detection, but they often lack an in-depth exploration of advanced deep learning techniques - an emerging field with significant potential. To address this gap, this paper conducts a comprehensive review of deep learning solutions and the challenges associated with their use in AML. Additionally, we propose a novel framework that applies the least-privilege principle by integrating machine learning techniques, codifying AML red flags, and employing account profiling to provide context for predictions and enable effective fraud detection under limited data availability....",
        "field": "Digital Asset Custody Solutions",
        "link": "http://arxiv.org/abs/2503.10058v1"
    },
    {
        "id": "2503.09934v1",
        "title": "A Pharmacy Benefit Manager Insurance Business Model",
        "authors": [
            "Lawrence W. Abrams"
        ],
        "published": "2025-03-13T01:13:16Z",
        "summary": "It is time to move on from attempts to make the pharmacy benefit manager (PBM) reseller business model more transparent. Time and time again the Big 3 PBMs have developed opaque alternatives to piece-meal 100% pass-through mandates. Time and time again PBMs have demonstrated expertise in finding loopholes in state government disclosure laws. The purpose of this paper is to provide quantitative estimates of two transparent insurance business models as a solution to the PBM agency issue. The key parameter used is an 8% gross profit margin figure disclosed by the Big 3 PBMs themselves. Based on reported drug trend delivered to plans, we use a $1,200 to $1,500 per member per year (PMPY) as the range for this key performance indicator (KPI). We propose that discussions of PBM insurance business models start with the following figures: (1) a fixed premium model with medical loss ratio ranging from 92% to 85%; (2) a fee-for-service model ranging from $96 to $180 PMPY with risk sharing of deviations from a contracted PMPY delivered drug spend.",
        "field": "Digital Asset Custody Solutions",
        "link": "http://arxiv.org/abs/2503.09934v1"
    },
    {
        "id": "2503.09433v1",
        "title": "CASTLE: Benchmarking Dataset for Static Code Analyzers and LLMs towards CWE Detection",
        "authors": [
            "Richard A. Dubniczky",
            "Krisztofer Zoltán Horvát",
            "Tamás Bisztray",
            "Mohamed Amine Ferrag",
            "Lucas C. Cordeiro",
            "Norbert Tihanyi"
        ],
        "published": "2025-03-12T14:30:05Z",
        "summary": "Identifying vulnerabilities in source code is crucial, especially in critical software components. Existing methods such as static analysis, dynamic analysis, formal verification, and recently Large Language Models are widely used to detect security flaws. This paper introduces CASTLE (CWE Automated Security Testing and Low-Level Evaluation), a benchmarking framework for evaluating the vulnerability detection capabilities of different methods. We assess 13 static analysis tools, 10 LLMs, and 2 formal verification tools using a hand-crafted dataset of 250 micro-benchmark programs covering 25 common CWEs. We propose the CASTLE Score, a novel evaluation metric to ensure fair comparison. Our results reveal key differences: ESBMC (a formal verification tool) minimizes false positives but struggles with vulnerabilities beyond model checking, such as weak cryptography or SQL injection. Static analyzers suffer from high false positives, increasing manual validation efforts for developers. LLMs perform exceptionally well in the CASTLE dataset when identifying vulnerabilities in small code snippets. However, their accuracy declines, and hallucinations increase as the code size grows. These results suggest that LLMs could play a pivotal role in future security solutions, particularly within code completion frameworks, where they can provide real-time guidance to prevent vulnerabilities. The dataset is accessible at https://github.com/CASTLE-Benchmark.",
        "field": "Digital Asset Custody Solutions",
        "link": "http://arxiv.org/abs/2503.09433v1"
    },
    {
        "id": "2503.09317v1",
        "title": "RaceTEE: A Practical Privacy-Preserving Off-Chain Smart Contract Execution Architecture",
        "authors": [
            "Keyu Zhang",
            "Andrew Martin"
        ],
        "published": "2025-03-12T12:10:02Z",
        "summary": "Decentralized on-chain smart contracts enable trustless collaboration, yet their inherent data transparency and execution overhead hinder widespread adoption. Existing cryptographic approaches incur high computational costs and lack generality. Meanwhile, prior TEE-based solutions suffer from practical limitations, such as the inability to support inter-contract interactions, reliance on unbreakable TEEs, and compromised usability. We introduce RaceTEE, a practical and privacy-preserving off-chain execution architecture for smart contracts that leverages Trusted Execution Environments (TEEs). RaceTEE decouples transaction ordering (on-chain) from execution (off-chain), with computations performed competitively in TEEs, ensuring confidentiality and minimizing overhead. It further enhances practicality through three key improvements: supporting secure inter-contract interactions, providing a key rotation scheme that enforces forward and backward secrecy even in the event of TEE breaches, and enabling full compatibility with existing blockchains without altering the user interaction model. To validate its feasibility, we prototype RaceTEE using Intel SGX and Ethereum, demonstrating its applicability across various use cases and evaluating its performance.",
        "field": "Digital Asset Custody Solutions",
        "link": "http://arxiv.org/abs/2503.09317v1"
    },
    {
        "id": "2503.09823v1",
        "title": "Data Traceability for Privacy Alignment",
        "authors": [
            "Kevin Liao",
            "Shreya Thipireddy",
            "Daniel Weitzner"
        ],
        "published": "2025-03-12T20:42:23Z",
        "summary": "This paper offers a new privacy approach for the growing ecosystem of services--ranging from open banking to healthcare--dependent on sensitive personal data sharing between individuals and third-parties. While these services offer significant benefits, individuals want control over their data, transparency regarding how their data is used, and accountability from third-parties for misuse. However, existing legal and technical mechanisms are inadequate for supporting these needs. A comprehensive approach to the modern privacy challenges of accountable third-party data sharing requires a closer alignment of technical system architecture and legal institutional design. In order to achieve this privacy alignment, we extend traditional security threat modeling and analysis to encompass a broader range of privacy notions than has been typically considered. In particular, we introduce the concept of covert-accountability, which addresses adversaries that may act dishonestly but face potential identification and legal consequences. As a concrete instance of this design approach, we present the OTrace protocol, designed to provide traceable, accountable, consumer-control in third-party data sharing ecosystems. OTrace empowers consumers with the knowledge of where their data is, who has it, what it is being used for, and whom it is being shared with. By applying our alignment framework to OTrace, we demonstrate that OTrace's technical affordances can provide more confident, scalable regulatory oversight when combined with complementary legal mechanisms.",
        "field": "RegTech in Banking",
        "link": "http://arxiv.org/abs/2503.09823v1"
    },
    {
        "id": "2503.09586v1",
        "title": "Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot",
        "authors": [
            "Andrew Crossman",
            "Andrew R. Plummer",
            "Chandra Sekharudu",
            "Deepak Warrier",
            "Mohammad Yekrangian"
        ],
        "published": "2025-03-12T17:54:18Z",
        "summary": "We present Auspex - a threat modeling system built using a specialized collection of generative artificial intelligence-based methods that capture threat modeling tradecraft. This new approach, called tradecraft prompting, centers on encoding the on-the-ground knowledge of threat modelers within the prompts that drive a generative AI-based threat modeling system. Auspex employs tradecraft prompts in two processing stages. The first stage centers on ingesting and processing system architecture information using prompts that encode threat modeling tradecraft knowledge pertaining to system decomposition and description. The second stage centers on chaining the resulting system analysis through a collection of prompts that encode tradecraft knowledge on threat identification, classification, and mitigation. The two-stage process yields a threat matrix for a system that specifies threat scenarios, threat types, information security categorizations and potential mitigations. Auspex produces formalized threat model output in minutes, relative to the weeks or months a manual process takes. More broadly, the focus on bespoke tradecraft prompting, as opposed to fine-tuning or agent-based add-ons, makes Auspex a lightweight, flexible, modular, and extensible foundational system capable of addressing the complexity, resource, and standardization limitations of both existing manual and automated threat modeling processes. In this connection, we establish the baseline value of Auspex to threat modelers through an evaluation procedure based on feedback collected from cybersecurity subject matter experts measuring the quality and utility of threat models generated by Auspex on real banking systems. We conclude with a discussion of system performance and plans for enhancements to Auspex.",
        "field": "RegTech in Banking",
        "link": "http://arxiv.org/abs/2503.09586v1"
    },
    {
        "id": "2503.00070v1",
        "title": "Systematic Review of Cybersecurity in Banking: Evolution from Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain, Policies and Practice",
        "authors": [
            "Tue Nhi Tran"
        ],
        "published": "2025-02-27T14:17:06Z",
        "summary": "Throughout the history from pre-industry 4.0 to post-industry 4.0, cybersecurity at banks has undergone significant changes. Pre-industry 4.0 cyber security at banks relied on individual security methods that were highly manual and had low accuracy. When moving to post-industry 4.0, cybersecurity at banks had a major turning point with security methods that combined different technologies such as Artificial Intelligence (AI), Blockchain, IoT, automating necessary processes and significantly increasing the defence layer for banks. However, along with the development of new technologies, the current challenge of cybersecurity at banks lies in scalability, high costs and resources in both money and time for R&D of defence methods along with the threat of high-tech cybercriminals growing and expanding. This report goes from introducing the importance of cybersecurity at banks, analyzing their management, operational and business objectives, evaluating pre-industry 4.0 technologies used for cybersecurity at banks to assessing post-industry 4.0 technologies focusing on Artificial Intelligence and Blockchain, discussing current policies and practices and ending with discussing key advantages and challenges for 4.0 technologies and recommendations for further developing cybersecurity at banks.",
        "field": "RegTech in Banking",
        "link": "http://arxiv.org/abs/2503.00070v1"
    },
    {
        "id": "2503.10644v1",
        "title": "Combined climate stress testing of supply-chain networks and the financial system with nation-wide firm-level emission estimates",
        "authors": [
            "Zlata Tabachová",
            "Christian Diem",
            "Johannes Stangl",
            "András Borsos",
            "Stefan Thurner"
        ],
        "published": "2025-02-25T20:25:47Z",
        "summary": "On the way towards carbon neutrality, climate stress testing provides estimates for the physical and transition risks that climate change poses to the economy and the financial system. Missing firm-level CO2 emissions data severely impedes the assessment of transition risks originating from carbon pricing. Based on the individual emissions of all Hungarian firms (410,523), as estimated from their fossil fuel purchases, we conduct a stress test of both actual and hypothetical carbon pricing policies. Using a simple 1:1 economic ABM and introducing the new carbon-to-profit ratio, we identify firms that become unprofitable and default, and estimate the respective loan write-offs. We find that 45% of all companies are directly exposed to carbon pricing. At a price of 45 EUR/t, direct economic losses of 1.3% of total sales and bank equity losses of 1.2% are expected. Secondary default cascades in supply chain networks could increase these losses by 300% to 4000%, depending on firms' ability to substitute essential inputs. To reduce transition risks, firms should reduce their dependence on essential inputs from supply chains with high CO2 exposure. We discuss the implications of different policy implementations on these transition risks.",
        "field": "RegTech in Banking",
        "link": "http://arxiv.org/abs/2503.10644v1"
    },
    {
        "id": "2502.14766v2",
        "title": "Multi-Layer Deep xVA: Structural Credit Models, Measure Changes and Convergence Analysis",
        "authors": [
            "Kristoffer Andersson",
            "Alessandro Gnoatto"
        ],
        "published": "2025-02-20T17:41:55Z",
        "summary": "We propose a structural default model for portfolio-wide valuation adjustments (xVAs) and represent it as a system of coupled backward stochastic differential equations. The framework is divided into four layers, each capturing a key component: (i) clean values, (ii) initial margin and Collateral Valuation Adjustment (ColVA), (iii) Credit/Debit Valuation Adjustments (CVA/DVA) together with Margin Valuation Adjustment (MVA), and (iv) Funding Valuation Adjustment (FVA). Because these layers depend on one another through collateral and default effects, a naive Monte Carlo approach would require deeply nested simulations, making the problem computationally intractable. To address this challenge, we use an iterative deep BSDE approach, handling each layer sequentially so that earlier outputs serve as inputs to the subsequent layers. Initial margin is computed via deep quantile regression to reflect margin requirements over the Margin Period of Risk. We also adopt a change-of-measure method that highlights rare but significant defaults of the bank or counterparty, ensuring that these events are accurately captured in the training process. We further extend Han and Long's (2020) a posteriori error analysis to BSDEs on bounded domains. Due to the random exit from the domain, we obtain an order of convergence of $\\mathcal{O}(h^{1/4-\\epsilon})$ rather than the usual $\\mathcal{O}(h^{1/2})$. Numerical experiments illustrate that this method drastically reduces computational demands and successfully scales to high-dimensional, non-symmetric portfolios. The results confirm its effectiveness and accuracy, offering a practical alternative to nested Monte Carlo simulations in multi-counterparty xVA analyses.",
        "field": "RegTech in Banking",
        "link": "http://arxiv.org/abs/2502.14766v2"
    },
    {
        "id": "2503.09823v1",
        "title": "Data Traceability for Privacy Alignment",
        "authors": [
            "Kevin Liao",
            "Shreya Thipireddy",
            "Daniel Weitzner"
        ],
        "published": "2025-03-12T20:42:23Z",
        "summary": "This paper offers a new privacy approach for the growing ecosystem of services--ranging from open banking to healthcare--dependent on sensitive personal data sharing between individuals and third-parties. While these services offer significant benefits, individuals want control over their data, transparency regarding how their data is used, and accountability from third-parties for misuse. However, existing legal and technical mechanisms are inadequate for supporting these needs. A comprehensive approach to the modern privacy challenges of accountable third-party data sharing requires a closer alignment of technical system architecture and legal institutional design. In order to achieve this privacy alignment, we extend traditional security threat modeling and analysis to encompass a broader range of privacy notions than has been typically considered. In particular, we introduce the concept of covert-accountability, which addresses adversaries that may act dishonestly but face potential identification and legal consequences. As a concrete instance of this design approach, we present the OTrace protocol, designed to provide traceable, accountable, consumer-control in third-party data sharing ecosystems. OTrace empowers consumers with the knowledge of where their data is, who has it, what it is being used for, and whom it is being shared with. By applying our alignment framework to OTrace, we demonstrate that OTrace's technical affordances can provide more confident, scalable regulatory oversight when combined with complementary legal mechanisms.",
        "field": "Voice Banking",
        "link": "http://arxiv.org/abs/2503.09823v1"
    },
    {
        "id": "2503.09586v1",
        "title": "Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot",
        "authors": [
            "Andrew Crossman",
            "Andrew R. Plummer",
            "Chandra Sekharudu",
            "Deepak Warrier",
            "Mohammad Yekrangian"
        ],
        "published": "2025-03-12T17:54:18Z",
        "summary": "We present Auspex - a threat modeling system built using a specialized collection of generative artificial intelligence-based methods that capture threat modeling tradecraft. This new approach, called tradecraft prompting, centers on encoding the on-the-ground knowledge of threat modelers within the prompts that drive a generative AI-based threat modeling system. Auspex employs tradecraft prompts in two processing stages. The first stage centers on ingesting and processing system architecture information using prompts that encode threat modeling tradecraft knowledge pertaining to system decomposition and description. The second stage centers on chaining the resulting system analysis through a collection of prompts that encode tradecraft knowledge on threat identification, classification, and mitigation. The two-stage process yields a threat matrix for a system that specifies threat scenarios, threat types, information security categorizations and potential mitigations. Auspex produces formalized threat model output in minutes, relative to the weeks or months a manual process takes. More broadly, the focus on bespoke tradecraft prompting, as opposed to fine-tuning or agent-based add-ons, makes Auspex a lightweight, flexible, modular, and extensible foundational system capable of addressing the complexity, resource, and standardization limitations of both existing manual and automated threat modeling processes. In this connection, we establish the baseline value of Auspex to threat modelers through an evaluation procedure based on feedback collected from cybersecurity subject matter experts measuring the quality and utility of threat models generated by Auspex on real banking systems. We conclude with a discussion of system performance and plans for enhancements to Auspex.",
        "field": "Voice Banking",
        "link": "http://arxiv.org/abs/2503.09586v1"
    },
    {
        "id": "2503.00070v1",
        "title": "Systematic Review of Cybersecurity in Banking: Evolution from Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain, Policies and Practice",
        "authors": [
            "Tue Nhi Tran"
        ],
        "published": "2025-02-27T14:17:06Z",
        "summary": "Throughout the history from pre-industry 4.0 to post-industry 4.0, cybersecurity at banks has undergone significant changes. Pre-industry 4.0 cyber security at banks relied on individual security methods that were highly manual and had low accuracy. When moving to post-industry 4.0, cybersecurity at banks had a major turning point with security methods that combined different technologies such as Artificial Intelligence (AI), Blockchain, IoT, automating necessary processes and significantly increasing the defence layer for banks. However, along with the development of new technologies, the current challenge of cybersecurity at banks lies in scalability, high costs and resources in both money and time for R&D of defence methods along with the threat of high-tech cybercriminals growing and expanding. This report goes from introducing the importance of cybersecurity at banks, analyzing their management, operational and business objectives, evaluating pre-industry 4.0 technologies used for cybersecurity at banks to assessing post-industry 4.0 technologies focusing on Artificial Intelligence and Blockchain, discussing current policies and practices and ending with discussing key advantages and challenges for 4.0 technologies and recommendations for further developing cybersecurity at banks.",
        "field": "Voice Banking",
        "link": "http://arxiv.org/abs/2503.00070v1"
    },
    {
        "id": "2503.10644v1",
        "title": "Combined climate stress testing of supply-chain networks and the financial system with nation-wide firm-level emission estimates",
        "authors": [
            "Zlata Tabachová",
            "Christian Diem",
            "Johannes Stangl",
            "András Borsos",
            "Stefan Thurner"
        ],
        "published": "2025-02-25T20:25:47Z",
        "summary": "On the way towards carbon neutrality, climate stress testing provides estimates for the physical and transition risks that climate change poses to the economy and the financial system. Missing firm-level CO2 emissions data severely impedes the assessment of transition risks originating from carbon pricing. Based on the individual emissions of all Hungarian firms (410,523), as estimated from their fossil fuel purchases, we conduct a stress test of both actual and hypothetical carbon pricing policies. Using a simple 1:1 economic ABM and introducing the new carbon-to-profit ratio, we identify firms that become unprofitable and default, and estimate the respective loan write-offs. We find that 45% of all companies are directly exposed to carbon pricing. At a price of 45 EUR/t, direct economic losses of 1.3% of total sales and bank equity losses of 1.2% are expected. Secondary default cascades in supply chain networks could increase these losses by 300% to 4000%, depending on firms' ability to substitute essential inputs. To reduce transition risks, firms should reduce their dependence on essential inputs from supply chains with high CO2 exposure. We discuss the implications of different policy implementations on these transition risks.",
        "field": "Voice Banking",
        "link": "http://arxiv.org/abs/2503.10644v1"
    },
    {
        "id": "2502.14766v2",
        "title": "Multi-Layer Deep xVA: Structural Credit Models, Measure Changes and Convergence Analysis",
        "authors": [
            "Kristoffer Andersson",
            "Alessandro Gnoatto"
        ],
        "published": "2025-02-20T17:41:55Z",
        "summary": "We propose a structural default model for portfolio-wide valuation adjustments (xVAs) and represent it as a system of coupled backward stochastic differential equations. The framework is divided into four layers, each capturing a key component: (i) clean values, (ii) initial margin and Collateral Valuation Adjustment (ColVA), (iii) Credit/Debit Valuation Adjustments (CVA/DVA) together with Margin Valuation Adjustment (MVA), and (iv) Funding Valuation Adjustment (FVA). Because these layers depend on one another through collateral and default effects, a naive Monte Carlo approach would require deeply nested simulations, making the problem computationally intractable. To address this challenge, we use an iterative deep BSDE approach, handling each layer sequentially so that earlier outputs serve as inputs to the subsequent layers. Initial margin is computed via deep quantile regression to reflect margin requirements over the Margin Period of Risk. We also adopt a change-of-measure method that highlights rare but significant defaults of the bank or counterparty, ensuring that these events are accurately captured in the training process. We further extend Han and Long's (2020) a posteriori error analysis to BSDEs on bounded domains. Due to the random exit from the domain, we obtain an order of convergence of $\\mathcal{O}(h^{1/4-\\epsilon})$ rather than the usual $\\mathcal{O}(h^{1/2})$. Numerical experiments illustrate that this method drastically reduces computational demands and successfully scales to high-dimensional, non-symmetric portfolios. The results confirm its effectiveness and accuracy, offering a practical alternative to nested Monte Carlo simulations in multi-counterparty xVA analyses.",
        "field": "Voice Banking",
        "link": "http://arxiv.org/abs/2502.14766v2"
    }
]