[
    {
        "id": "2503.11619v1",
        "title": "Tit-for-Tat: Safeguarding Large Vision-Language Models Against Jailbreak Attacks via Adversarial Defense",
        "authors": [
            "Shuyang Hao",
            "Yiwei Wang",
            "Bryan Hooi",
            "Ming-Hsuan Yang",
            "Jun Liu",
            "Chengcheng Tang",
            "Zi Huang",
            "Yujun Cai"
        ],
        "published": "2025-03-14T17:39:45Z",
        "summary": "Deploying large vision-language models (LVLMs) introduces a unique vulnerability: susceptibility to malicious attacks via visual inputs. However, existing defense methods suffer from two key limitations: (1) They solely focus on textual defenses, fail to directly address threats in the visual domain where attacks originate, and (2) the additional processing steps often incur significant computational overhead or compromise model performance on benign tasks. Building on these insights, we propose ESIII (Embedding Security Instructions Into Images), a novel methodology for transforming the visual space from a source of vulnerability into an active defense mechanism. Initially, we embed security instructions into defensive images through gradient-based optimization, obtaining security instructions in the visual dimension. Subsequently, we integrate security instructions from visual and textual dimensions with the input query. The collaboration between security instructions from different dimensions ensures comprehensive security protection. Extensive experiments demonstrate that our approach effectively fortifies the robustness of LVLMs against such attacks while preserving their performance on standard benign tasks and incurring an imperceptible increase in time costs.",
        "field": "Quantum Computing",
        "link": "http://arxiv.org/abs/2503.11619v1"
    },
    {
        "id": "2503.11573v1",
        "title": "Synthesizing Access Control Policies using Large Language Models",
        "authors": [
            "Adarsh Vatsa",
            "Pratyush Patel",
            "William Eiers"
        ],
        "published": "2025-03-14T16:40:25Z",
        "summary": "Cloud compute systems allow administrators to write access control policies that govern access to private data. While policies are written in convenient languages, such as AWS Identity and Access Management Policy Language, manually written policies often become complex and error prone. In this paper, we investigate whether and how well Large Language Models (LLMs) can be used to synthesize access control policies. Our investigation focuses on the task of taking an access control request specification and zero-shot prompting LLMs to synthesize a well-formed access control policy which correctly adheres to the request specification. We consider two scenarios, one which the request specification is given as a concrete list of requests to be allowed or denied, and another in which a natural language description is used to specify sets of requests to be allowed or denied. We then argue that for zero-shot prompting, more precise and structured prompts using a syntax based approach are necessary and experimentally show preliminary results validating our approach.",
        "field": "Quantum Computing",
        "link": "http://arxiv.org/abs/2503.11573v1"
    },
    {
        "id": "2503.11404v1",
        "title": "Towards A Correct Usage of Cryptography in Semantic Watermarks for Diffusion Models",
        "authors": [
            "Jonas Thietke",
            "Andreas MÃ¼ller",
            "Denis Lukovnikov",
            "Asja Fischer",
            "Erwin Quiring"
        ],
        "published": "2025-03-14T13:45:46Z",
        "summary": "Semantic watermarking methods enable the direct integration of watermarks into the generation process of latent diffusion models by only modifying the initial latent noise. One line of approaches building on Gaussian Shading relies on cryptographic primitives to steer the sampling process of the latent noise. However, we identify several issues in the usage of cryptographic techniques in Gaussian Shading, particularly in its proof of lossless performance and key management, causing ambiguity in follow-up works, too. In this work, we therefore revisit the cryptographic primitives for semantic watermarking. We introduce a novel, general proof of lossless performance based on IND\\$-CPA security for semantic watermarks. We then discuss the configuration of the cryptographic primitives in semantic watermarks with respect to security, efficiency, and generation quality.",
        "field": "Quantum Computing",
        "link": "http://arxiv.org/abs/2503.11404v1"
    },
    {
        "id": "2503.11216v1",
        "title": "Cross-Platform Benchmarking of the FHE Libraries: Novel Insights into SEAL and Openfhe",
        "authors": [
            "Faneela",
            "Jawad Ahmad",
            "Baraq Ghaleb",
            "Sana Ullah Jan",
            "William J. Buchanan"
        ],
        "published": "2025-03-14T09:08:30Z",
        "summary": "The rapid growth of cloud computing and data-driven applications has amplified privacy concerns, driven by the increasing demand to process sensitive data securely. Homomorphic encryption (HE) has become a vital solution for addressing these concerns by enabling computations on encrypted data without revealing its contents. This paper provides a comprehensive evaluation of two leading HE libraries, SEAL and OpenFHE, examining their performance, usability, and support for prominent HE schemes such as BGV and CKKS. Our analysis highlights computational efficiency, memory usage, and scalability across Linux and Windows platforms, emphasizing their applicability in real-world scenarios. Results reveal that Linux outperforms Windows in computation efficiency, with OpenFHE emerging as the optimal choice across diverse cryptographic settings. This paper provides valuable insights for researchers and practitioners to advance privacy-preserving applications using FHE.",
        "field": "Quantum Computing",
        "link": "http://arxiv.org/abs/2503.11216v1"
    },
    {
        "id": "2503.11185v1",
        "title": "Align in Depth: Defending Jailbreak Attacks via Progressive Answer Detoxification",
        "authors": [
            "Yingjie Zhang",
            "Tong Liu",
            "Zhe Zhao",
            "Guozhu Meng",
            "Kai Chen"
        ],
        "published": "2025-03-14T08:32:12Z",
        "summary": "Large Language Models (LLMs) are vulnerable to jailbreak attacks, which use crafted prompts to elicit toxic responses. These attacks exploit LLMs' difficulty in dynamically detecting harmful intents during the generation process. Traditional safety alignment methods, often relying on the initial few generation steps, are ineffective due to limited computational budget. This paper proposes DEEPALIGN, a robust defense framework that fine-tunes LLMs to progressively detoxify generated content, significantly improving both the computational budget and effectiveness of mitigating harmful generation. Our approach uses a hybrid loss function operating on hidden states to directly improve LLMs' inherent awareness of toxity during generation. Furthermore, we redefine safe responses by generating semantically relevant answers to harmful queries, thereby increasing robustness against representation-mutation attacks. Evaluations across multiple LLMs demonstrate state-of-the-art defense performance against six different attack types, reducing Attack Success Rates by up to two orders of magnitude compared to previous state-of-the-art defense while preserving utility. This work advances LLM safety by addressing limitations of conventional alignment through dynamic, context-aware mitigation.",
        "field": "Quantum Computing",
        "link": "http://arxiv.org/abs/2503.11185v1"
    },
    {
        "id": "2503.06279v1",
        "title": "Mitigating Blockchain extractable value (BEV) threats by Distributed Transaction Sequencing in Blockchains",
        "authors": [
            "Xiongfei Zhao",
            "Hou-Wan Long",
            "Zhengzhe Li",
            "Jiangchuan Liu",
            "Yain-Whar Si"
        ],
        "published": "2025-03-08T16:55:52Z",
        "summary": "The rapid growth of Blockchain and Decentralized Finance (DeFi) has introduced new challenges and vulnerabilities that threaten the integrity and efficiency of the ecosystem. This study identifies critical issues such as Transaction Order Dependence (TOD), Blockchain Extractable Value (BEV), and Transaction Importance Diversity (TID), which collectively undermine the fairness and security of DeFi systems. BEV-related activities, including Sandwich attacks, Liquidations, and Transaction Replay, have emerged as significant threats, collectively generating $540.54 million in losses over 32 months across 11,289 addresses, involving 49,691 cryptocurrencies and 60,830 on-chain markets. These attacks exploit transaction mechanics to manipulate asset prices and extract value at the expense of other participants, with Sandwich attacks being particularly impactful. Additionally, the growing adoption of Blockchain in traditional finance highlights the challenge of TID, where high transaction volumes can strain systems and compromise time-sensitive operations. To address these pressing issues, we propose a novel Distributed Transaction Sequencing Strategy (DTSS), which combines forking mechanisms and the Analytic Hierarchy Process (AHP) to enforce fair and transparent transaction ordering in a decentralized manner. Our approach is further enhanced by an optimization framework and the introduction of the Normalized Allocation Disparity Metric (NADM), which ensures optimal parameter selection for transaction prioritization. Experimental evaluations demonstrate that DTSS effectively mitigates BEV risks, enhances transaction fairness, and significantly improves the security and transparency of DeFi ecosystems. This work is essential for protecting the future of decentralized finance and promoting its integration into global financial systems.",
        "field": "Decentralized Finance (DeFi)",
        "link": "http://arxiv.org/abs/2503.06279v1"
    },
    {
        "id": "2503.04850v2",
        "title": "Slow is Fast! Dissecting Ethereum's Slow Liquidity Drain Scams",
        "authors": [
            "Minh Trung Tran",
            "Nasrin Sohrabi",
            "Zahir Tari",
            "Qin Wang",
            "Xiaoyu Xia"
        ],
        "published": "2025-03-06T02:24:35Z",
        "summary": "We identify the slow liquidity drain (SLID) scam, an insidious and highly profitable threat to decentralized finance (DeFi), posing a large-scale, persistent, and growing risk to the ecosystem. Unlike traditional scams such as rug pulls or honeypots (USENIX Sec'19, USENIX Sec'23), SLID gradually siphons funds from liquidity pools over extended periods, making detection significantly more challenging. In this paper, we conducted the first large-scale empirical analysis of 319,166 liquidity pools across six major decentralized exchanges (DEXs) since 2018. We identified 3,117 SLID affected liquidity pools, resulting in cumulative losses of more than US$103 million. We propose a rule-based heuristic and an enhanced machine learning model for early detection. Our machine learning model achieves a detection speed 4.77 times faster than the heuristic while maintaining 95% accuracy. Our study establishes a foundation for protecting DeFi investors at an early stage and promoting transparency in the DeFi ecosystem.",
        "field": "Decentralized Finance (DeFi)",
        "link": "http://arxiv.org/abs/2503.04850v2"
    },
    {
        "id": "2503.01944v1",
        "title": "Protecting DeFi Platforms against Non-Price Flash Loan Attacks",
        "authors": [
            "Abdulrahman Alhaidari",
            "Balaji Palanisamy",
            "Prashant Krishnamurthy"
        ],
        "published": "2025-03-03T18:18:05Z",
        "summary": "Smart contracts in Decentralized Finance (DeFi) platforms are attractive targets for attacks as their vulnerabilities can lead to massive amounts of financial losses. Flash loan attacks, in particular, pose a major threat to DeFi protocols that hold a Total Value Locked (TVL) exceeding \\$106 billion. These attacks use the atomicity property of blockchains to drain funds from smart contracts in a single transaction. While existing research primarily focuses on price manipulation attacks, such as oracle manipulation, mitigating non-price flash loan attacks that often exploit smart contracts' zero-day vulnerabilities remains largely unaddressed. These attacks are challenging to detect because of their unique patterns, time sensitivity, and complexity. In this paper, we present FlashGuard, a runtime detection and mitigation method for non-price flash loan attacks. Our approach targets smart contract function signatures to identify attacks in real-time and counterattack by disrupting the attack transaction atomicity by leveraging the short window when transactions are visible in the mempool but not yet confirmed. When FlashGuard detects an attack, it dispatches a stealthy dusting counterattack transaction to miners to change the victim contract's state which disrupts the attack's atomicity and forces the attack transaction to revert. We evaluate our approach using 20 historical attacks and several unseen attacks. FlashGuard achieves an average real-time detection latency of 150.31ms, a detection accuracy of over 99.93\\%, and an average disruption time of 410.92ms. FlashGuard could have potentially rescued over \\$405.71 million in losses if it were deployed prior to these attack instances. FlashGuard demonstrates significant potential as a DeFi security solution to mitigate and handle rising threats of non-price flash loan attacks.",
        "field": "Decentralized Finance (DeFi)",
        "link": "http://arxiv.org/abs/2503.01944v1"
    },
    {
        "id": "2502.11521v1",
        "title": "DeFiScope: Detecting Various DeFi Price Manipulations with LLM Reasoning",
        "authors": [
            "Juantao Zhong",
            "Daoyuan Wu",
            "Ye Liu",
            "Maoyi Xie",
            "Yang Liu",
            "Yi Li",
            "Ning Liu"
        ],
        "published": "2025-02-17T07:45:03Z",
        "summary": "DeFi (Decentralized Finance) is one of the most important applications of today's cryptocurrencies and smart contracts. It manages hundreds of billions in Total Value Locked (TVL) on-chain, yet it remains susceptible to common DeFi price manipulation attacks. Despite state-of-the-art (SOTA) systems like DeFiRanger and DeFort, we found that they are less effective to non-standard price models in custom DeFi protocols, which account for 44.2% of the 95 DeFi price manipulation attacks reported over the past three years. In this paper, we introduce the first LLM-based approach, DeFiScope, for detecting DeFi price manipulation attacks in both standard and custom price models. Our insight is that large language models (LLMs) have certain intelligence to abstract price calculation from code and infer the trend of token price changes based on the extracted price models. To further strengthen LLMs in this aspect, we leverage Foundry to synthesize on-chain data and use it to fine-tune a DeFi price-specific LLM. Together with the high-level DeFi operations recovered from low-level transaction data, DeFiScope detects various DeFi price manipulations according to systematically mined patterns. Experimental results show that DeFiScope achieves a high precision of 96% and a recall rate of 80%, significantly outperforming SOTA approaches. Moreover, we evaluate DeFiScope's cost-effectiveness and demonstrate its practicality by helping our industry partner confirm 147 real-world price manipulation attacks, including discovering 81 previously unknown historical incidents.",
        "field": "Decentralized Finance (DeFi)",
        "link": "http://arxiv.org/abs/2502.11521v1"
    },
    {
        "id": "2502.13167v1",
        "title": "SmartLLM: Smart Contract Auditing using Custom Generative AI",
        "authors": [
            "Jun Kevin",
            "Pujianto Yugopuspito"
        ],
        "published": "2025-02-17T06:22:05Z",
        "summary": "Smart contracts are essential to decentralized finance (DeFi) and blockchain ecosystems but are increasingly vulnerable to exploits due to coding errors and complex attack vectors. Traditional static analysis tools and existing vulnerability detection methods often fail to address these challenges comprehensively, leading to high false-positive rates and an inability to detect dynamic vulnerabilities. This paper introduces SmartLLM, a novel approach leveraging fine-tuned LLaMA 3.1 models with Retrieval-Augmented Generation (RAG) to enhance the accuracy and efficiency of smart contract auditing. By integrating domain-specific knowledge from ERC standards and employing advanced techniques such as QLoRA for efficient fine-tuning, SmartLLM achieves superior performance compared to static analysis tools like Mythril and Slither, as well as zero-shot large language model (LLM) prompting methods such as GPT-3.5 and GPT-4. Experimental results demonstrate a perfect recall of 100% and an accuracy score of 70%, highlighting the model's robustness in identifying vulnerabilities, including reentrancy and access control issues. This research advances smart contract security by offering a scalable and effective auditing solution, supporting the secure adoption of decentralized applications.",
        "field": "Decentralized Finance (DeFi)",
        "link": "http://arxiv.org/abs/2502.13167v1"
    },
    {
        "id": "2503.09823v1",
        "title": "Data Traceability for Privacy Alignment",
        "authors": [
            "Kevin Liao",
            "Shreya Thipireddy",
            "Daniel Weitzner"
        ],
        "published": "2025-03-12T20:42:23Z",
        "summary": "This paper offers a new privacy approach for the growing ecosystem of services--ranging from open banking to healthcare--dependent on sensitive personal data sharing between individuals and third-parties. While these services offer significant benefits, individuals want control over their data, transparency regarding how their data is used, and accountability from third-parties for misuse. However, existing legal and technical mechanisms are inadequate for supporting these needs. A comprehensive approach to the modern privacy challenges of accountable third-party data sharing requires a closer alignment of technical system architecture and legal institutional design. In order to achieve this privacy alignment, we extend traditional security threat modeling and analysis to encompass a broader range of privacy notions than has been typically considered. In particular, we introduce the concept of covert-accountability, which addresses adversaries that may act dishonestly but face potential identification and legal consequences. As a concrete instance of this design approach, we present the OTrace protocol, designed to provide traceable, accountable, consumer-control in third-party data sharing ecosystems. OTrace empowers consumers with the knowledge of where their data is, who has it, what it is being used for, and whom it is being shared with. By applying our alignment framework to OTrace, we demonstrate that OTrace's technical affordances can provide more confident, scalable regulatory oversight when combined with complementary legal mechanisms.",
        "field": "Artificial Intelligence (AI) in Banking",
        "link": "http://arxiv.org/abs/2503.09823v1"
    },
    {
        "id": "2503.09586v1",
        "title": "Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot",
        "authors": [
            "Andrew Crossman",
            "Andrew R. Plummer",
            "Chandra Sekharudu",
            "Deepak Warrier",
            "Mohammad Yekrangian"
        ],
        "published": "2025-03-12T17:54:18Z",
        "summary": "We present Auspex - a threat modeling system built using a specialized collection of generative artificial intelligence-based methods that capture threat modeling tradecraft. This new approach, called tradecraft prompting, centers on encoding the on-the-ground knowledge of threat modelers within the prompts that drive a generative AI-based threat modeling system. Auspex employs tradecraft prompts in two processing stages. The first stage centers on ingesting and processing system architecture information using prompts that encode threat modeling tradecraft knowledge pertaining to system decomposition and description. The second stage centers on chaining the resulting system analysis through a collection of prompts that encode tradecraft knowledge on threat identification, classification, and mitigation. The two-stage process yields a threat matrix for a system that specifies threat scenarios, threat types, information security categorizations and potential mitigations. Auspex produces formalized threat model output in minutes, relative to the weeks or months a manual process takes. More broadly, the focus on bespoke tradecraft prompting, as opposed to fine-tuning or agent-based add-ons, makes Auspex a lightweight, flexible, modular, and extensible foundational system capable of addressing the complexity, resource, and standardization limitations of both existing manual and automated threat modeling processes. In this connection, we establish the baseline value of Auspex to threat modelers through an evaluation procedure based on feedback collected from cybersecurity subject matter experts measuring the quality and utility of threat models generated by Auspex on real banking systems. We conclude with a discussion of system performance and plans for enhancements to Auspex.",
        "field": "Artificial Intelligence (AI) in Banking",
        "link": "http://arxiv.org/abs/2503.09586v1"
    },
    {
        "id": "2503.00070v1",
        "title": "Systematic Review of Cybersecurity in Banking: Evolution from Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain, Policies and Practice",
        "authors": [
            "Tue Nhi Tran"
        ],
        "published": "2025-02-27T14:17:06Z",
        "summary": "Throughout the history from pre-industry 4.0 to post-industry 4.0, cybersecurity at banks has undergone significant changes. Pre-industry 4.0 cyber security at banks relied on individual security methods that were highly manual and had low accuracy. When moving to post-industry 4.0, cybersecurity at banks had a major turning point with security methods that combined different technologies such as Artificial Intelligence (AI), Blockchain, IoT, automating necessary processes and significantly increasing the defence layer for banks. However, along with the development of new technologies, the current challenge of cybersecurity at banks lies in scalability, high costs and resources in both money and time for R&D of defence methods along with the threat of high-tech cybercriminals growing and expanding. This report goes from introducing the importance of cybersecurity at banks, analyzing their management, operational and business objectives, evaluating pre-industry 4.0 technologies used for cybersecurity at banks to assessing post-industry 4.0 technologies focusing on Artificial Intelligence and Blockchain, discussing current policies and practices and ending with discussing key advantages and challenges for 4.0 technologies and recommendations for further developing cybersecurity at banks.",
        "field": "Artificial Intelligence (AI) in Banking",
        "link": "http://arxiv.org/abs/2503.00070v1"
    },
    {
        "id": "2503.10644v1",
        "title": "Combined climate stress testing of supply-chain networks and the financial system with nation-wide firm-level emission estimates",
        "authors": [
            "Zlata TabachovÃ¡",
            "Christian Diem",
            "Johannes Stangl",
            "AndrÃ¡s Borsos",
            "Stefan Thurner"
        ],
        "published": "2025-02-25T20:25:47Z",
        "summary": "On the way towards carbon neutrality, climate stress testing provides estimates for the physical and transition risks that climate change poses to the economy and the financial system. Missing firm-level CO2 emissions data severely impedes the assessment of transition risks originating from carbon pricing. Based on the individual emissions of all Hungarian firms (410,523), as estimated from their fossil fuel purchases, we conduct a stress test of both actual and hypothetical carbon pricing policies. Using a simple 1:1 economic ABM and introducing the new carbon-to-profit ratio, we identify firms that become unprofitable and default, and estimate the respective loan write-offs. We find that 45% of all companies are directly exposed to carbon pricing. At a price of 45 EUR/t, direct economic losses of 1.3% of total sales and bank equity losses of 1.2% are expected. Secondary default cascades in supply chain networks could increase these losses by 300% to 4000%, depending on firms' ability to substitute essential inputs. To reduce transition risks, firms should reduce their dependence on essential inputs from supply chains with high CO2 exposure. We discuss the implications of different policy implementations on these transition risks.",
        "field": "Artificial Intelligence (AI) in Banking",
        "link": "http://arxiv.org/abs/2503.10644v1"
    },
    {
        "id": "2502.14766v2",
        "title": "Multi-Layer Deep xVA: Structural Credit Models, Measure Changes and Convergence Analysis",
        "authors": [
            "Kristoffer Andersson",
            "Alessandro Gnoatto"
        ],
        "published": "2025-02-20T17:41:55Z",
        "summary": "We propose a structural default model for portfolio-wide valuation adjustments (xVAs) and represent it as a system of coupled backward stochastic differential equations. The framework is divided into four layers, each capturing a key component: (i) clean values, (ii) initial margin and Collateral Valuation Adjustment (ColVA), (iii) Credit/Debit Valuation Adjustments (CVA/DVA) together with Margin Valuation Adjustment (MVA), and (iv) Funding Valuation Adjustment (FVA). Because these layers depend on one another through collateral and default effects, a naive Monte Carlo approach would require deeply nested simulations, making the problem computationally intractable. To address this challenge, we use an iterative deep BSDE approach, handling each layer sequentially so that earlier outputs serve as inputs to the subsequent layers. Initial margin is computed via deep quantile regression to reflect margin requirements over the Margin Period of Risk. We also adopt a change-of-measure method that highlights rare but significant defaults of the bank or counterparty, ensuring that these events are accurately captured in the training process. We further extend Han and Long's (2020) a posteriori error analysis to BSDEs on bounded domains. Due to the random exit from the domain, we obtain an order of convergence of $\\mathcal{O}(h^{1/4-\\epsilon})$ rather than the usual $\\mathcal{O}(h^{1/2})$. Numerical experiments illustrate that this method drastically reduces computational demands and successfully scales to high-dimensional, non-symmetric portfolios. The results confirm its effectiveness and accuracy, offering a practical alternative to nested Monte Carlo simulations in multi-counterparty xVA analyses.",
        "field": "Artificial Intelligence (AI) in Banking",
        "link": "http://arxiv.org/abs/2502.14766v2"
    },
    {
        "id": "2412.04051v1",
        "title": "How to design a Public Key Infrastructure for a Central Bank Digital Currency",
        "authors": [
            "Makan Rafiee",
            "Lars Hupel"
        ],
        "published": "2024-12-05T10:41:38Z",
        "summary": "Central Bank Digital Currency (CBDC) is a new form of money, issued by a country's or region's central bank, that can be used for a variety of payment scenarios. Depending on its concrete implementation, there are many participants in a production CBDC ecosystem, including the central bank, commercial banks, merchants, individuals, and wallet providers. There is a need for robust and scalable Public Key Infrastructure (PKI) for CBDC to ensure the continued trust of all entities in the system. This paper discusses the criteria that should flow into the design of a PKI and proposes a certificate hierarchy, together with a rollover concept ensuring continuous operation of the system. We further consider several peculiarities, such as the circulation of offline-capable hardware wallets.",
        "field": "Central Bank Digital Currencies (CBDCs)",
        "link": "http://arxiv.org/abs/2412.04051v1"
    },
    {
        "id": "2411.06362v1",
        "title": "Will Central Bank Digital Currencies (CBDC) and Blockchain Cryptocurrencies Coexist in the Post Quantum Era?",
        "authors": [
            "Abraham Itzhak Weinberg",
            "Pythagoras Petratos",
            "Alessio Faccia"
        ],
        "published": "2024-11-10T05:05:55Z",
        "summary": "This paper explores the coexistence possibilities of Central Bank Digital Currencies (CBDCs) and blockchain-based cryptocurrencies within a post-quantum computing landscape. It examines the implications of emerging quantum algorithms and cryptographic techniques such as Multi-Party Computation (MPC) and Oblivious Transfer (OT). While exploring how CBDCs and cryptocurrencies might integrate defenses like post-quantum cryptography, it highlights the substantial hurdles in transitioning legacy systems and fostering widespread adoption of new standards. The paper includes comprehensive evaluations of CBDCs in a quantum context. It also features comparisons to alternative cryptocurrency models. Additionally, the paper provides insightful analyses of pertinent quantum methodologies. Examinations of interfaces between these methods and blockchain architectures are also included. The paper carries out considered appraisals of quantum threats and their relevance for cryptocurrency schemes. Furthermore, it features discussions of the influence of anticipated advances in quantum computing on algorithms and their applications. The paper renders the judicious conclusion that long-term coexistence is viable provided challenges are constructively addressed through ongoing collaborative efforts to validate solutions and guide evolving policies.",
        "field": "Central Bank Digital Currencies (CBDCs)",
        "link": "http://arxiv.org/abs/2411.06362v1"
    },
    {
        "id": "2408.06956v1",
        "title": "PayOff: A Regulated Central Bank Digital Currency with Private Offline Payments",
        "authors": [
            "Carolin Beer",
            "Sheila Zingg",
            "Kari Kostiainen",
            "Karl WÃ¼st",
            "Vedran Capkun",
            "Srdjan Capkun"
        ],
        "published": "2024-08-13T15:15:06Z",
        "summary": "The European Central Bank is preparing for the potential issuance of a central bank digital currency (CBDC), called the digital euro. A recent regulatory proposal by the European Commission defines several requirements for the digital euro, such as support for both online and offline payments. Offline payments are expected to enable cash-like privacy, local payment settlement, and the enforcement of holding limits. While other central banks have expressed similar desired functionality, achieving such offline payments poses a novel technical challenge. We observe that none of the existing research solutions, including offline E-cash schemes, are fully compliant. Proposed solutions based on secure elements offer no guarantees in case of compromise and can therefore lead to significant payment fraud. The main contribution of this paper is PayOff, a novel CBDC design motivated by the digital euro regulation, which focuses on offline payments. We analyze the security implications of local payment settlement and identify new security objectives. PayOff protects user privacy, supports complex regulations such as holding limits, and implements safeguards to increase robustness against secure element failure. Our analysis shows that PayOff provides strong privacy and identifies residual leakages that may arise in real-world deployments. Our evaluation shows that offline payments can be fast and that the central bank can handle high payment loads with moderate computing resources. However, the main limitation of PayOff is that offline payment messages and storage requirements grow in the number of payments that the sender makes or receives without going online in between.",
        "field": "Central Bank Digital Currencies (CBDCs)",
        "link": "http://arxiv.org/abs/2408.06956v1"
    },
    {
        "id": "2407.13776v1",
        "title": "Offline Digital Euro: a Minimum Viable CBDC using Groth-Sahai proofs",
        "authors": [
            "Leon Kempen",
            "Johan Pouwelse"
        ],
        "published": "2024-07-01T09:55:14Z",
        "summary": "Current digital payment solutions are fragile and offer less privacy than traditional cash. Their critical dependency on an online service used to perform and validate transactions makes them void if this service is unreachable. Moreover, no transaction can be executed during server malfunctions or power outages. Due to climate change, the likelihood of extreme weather increases. As extreme weather is a major cause of power outages, the frequency of power outages is expected to increase. The lack of privacy is an inherent result of their account-based design or the use of a public ledger. The critical dependency and lack of privacy can be resolved with a Central Bank Digital Currency that can be used offline. This thesis proposes a design and a first implementation for an offline-first digital euro. The protocol offers complete privacy during transactions using zero-knowledge proofs. Furthermore, transactions can be executed offline without third parties and retroactive double-spending detection is facilitated. To protect the users' privacy, but also guard against money laundering, we have added the following privacy-guarding mechanism. The bank and trusted third parties for law enforcement must collaborate to decrypt transactions, revealing the digital pseudonym used in the transaction. Importantly, the transaction can be decrypted without decrypting prior transactions attached to the digital euro. The protocol has a working initial implementation showcasing its usability and demonstrating functionality.",
        "field": "Central Bank Digital Currencies (CBDCs)",
        "link": "http://arxiv.org/abs/2407.13776v1"
    },
    {
        "id": "2405.10678v1",
        "title": "IT Strategic alignment in the decentralized finance (DeFi): CBDC and digital currencies",
        "authors": [
            "Carlos Alberto Durigan Junior",
            "Fernando Jose Barbin Laurindo"
        ],
        "published": "2024-05-17T10:19:20Z",
        "summary": "Cryptocurrency can be understood as a digital asset transacted among participants in the crypto economy. Every cryptocurrency must have an associated Blockchain. Blockchain is a Distributed Ledger Technology (DLT) which supports cryptocurrencies, this may be considered as the most promising disruptive technology in the industry 4.0 context. Decentralized finance (DeFi) is a Blockchain-based financial infrastructure, the term generally refers to an open, permissionless, and highly interoperable protocol stack built on public smart contract platforms, such as the Ethereum Blockchain. It replicates existing financial services in a more open and transparent way. DeFi does not rely on intermediaries and centralized institutions. Instead, it is based on open protocols and decentralized applications (Dapps). Considering that there are many digital coins, stablecoins and central bank digital currencies (CBDCs), these currencies should interact among each other sometime. For this interaction the Information Technology elements play an important whole as enablers and IT strategic alignment. This paper considers the strategic alignment model proposed by Henderson and Venkatraman (1993) and Luftman (1996). This paper seeks to answer two main questions 1) What are the common IT elements in the DeFi? And 2) How the elements connect to the IT strategic alignment in DeFi? Through a Systematic Literature Review (SLR). Results point out that there are many IT elements already mentioned by literature, however there is a lack in the literature about the connection between IT elements and IT strategic alignment in a Decentralized Finance (DeFi) architectural network. After final considerations, limitations and future research agenda are presented. Keywords: IT Strategic alignment, Decentralized Finance (DeFi), Cryptocurrency, Digital Economy.",
        "field": "Central Bank Digital Currencies (CBDCs)",
        "link": "http://arxiv.org/abs/2405.10678v1"
    },
    {
        "id": "2503.09823v1",
        "title": "Data Traceability for Privacy Alignment",
        "authors": [
            "Kevin Liao",
            "Shreya Thipireddy",
            "Daniel Weitzner"
        ],
        "published": "2025-03-12T20:42:23Z",
        "summary": "This paper offers a new privacy approach for the growing ecosystem of services--ranging from open banking to healthcare--dependent on sensitive personal data sharing between individuals and third-parties. While these services offer significant benefits, individuals want control over their data, transparency regarding how their data is used, and accountability from third-parties for misuse. However, existing legal and technical mechanisms are inadequate for supporting these needs. A comprehensive approach to the modern privacy challenges of accountable third-party data sharing requires a closer alignment of technical system architecture and legal institutional design. In order to achieve this privacy alignment, we extend traditional security threat modeling and analysis to encompass a broader range of privacy notions than has been typically considered. In particular, we introduce the concept of covert-accountability, which addresses adversaries that may act dishonestly but face potential identification and legal consequences. As a concrete instance of this design approach, we present the OTrace protocol, designed to provide traceable, accountable, consumer-control in third-party data sharing ecosystems. OTrace empowers consumers with the knowledge of where their data is, who has it, what it is being used for, and whom it is being shared with. By applying our alignment framework to OTrace, we demonstrate that OTrace's technical affordances can provide more confident, scalable regulatory oversight when combined with complementary legal mechanisms.",
        "field": "Cybersecurity in Banking",
        "link": "http://arxiv.org/abs/2503.09823v1"
    },
    {
        "id": "2503.09586v1",
        "title": "Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot",
        "authors": [
            "Andrew Crossman",
            "Andrew R. Plummer",
            "Chandra Sekharudu",
            "Deepak Warrier",
            "Mohammad Yekrangian"
        ],
        "published": "2025-03-12T17:54:18Z",
        "summary": "We present Auspex - a threat modeling system built using a specialized collection of generative artificial intelligence-based methods that capture threat modeling tradecraft. This new approach, called tradecraft prompting, centers on encoding the on-the-ground knowledge of threat modelers within the prompts that drive a generative AI-based threat modeling system. Auspex employs tradecraft prompts in two processing stages. The first stage centers on ingesting and processing system architecture information using prompts that encode threat modeling tradecraft knowledge pertaining to system decomposition and description. The second stage centers on chaining the resulting system analysis through a collection of prompts that encode tradecraft knowledge on threat identification, classification, and mitigation. The two-stage process yields a threat matrix for a system that specifies threat scenarios, threat types, information security categorizations and potential mitigations. Auspex produces formalized threat model output in minutes, relative to the weeks or months a manual process takes. More broadly, the focus on bespoke tradecraft prompting, as opposed to fine-tuning or agent-based add-ons, makes Auspex a lightweight, flexible, modular, and extensible foundational system capable of addressing the complexity, resource, and standardization limitations of both existing manual and automated threat modeling processes. In this connection, we establish the baseline value of Auspex to threat modelers through an evaluation procedure based on feedback collected from cybersecurity subject matter experts measuring the quality and utility of threat models generated by Auspex on real banking systems. We conclude with a discussion of system performance and plans for enhancements to Auspex.",
        "field": "Cybersecurity in Banking",
        "link": "http://arxiv.org/abs/2503.09586v1"
    },
    {
        "id": "2503.00070v1",
        "title": "Systematic Review of Cybersecurity in Banking: Evolution from Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain, Policies and Practice",
        "authors": [
            "Tue Nhi Tran"
        ],
        "published": "2025-02-27T14:17:06Z",
        "summary": "Throughout the history from pre-industry 4.0 to post-industry 4.0, cybersecurity at banks has undergone significant changes. Pre-industry 4.0 cyber security at banks relied on individual security methods that were highly manual and had low accuracy. When moving to post-industry 4.0, cybersecurity at banks had a major turning point with security methods that combined different technologies such as Artificial Intelligence (AI), Blockchain, IoT, automating necessary processes and significantly increasing the defence layer for banks. However, along with the development of new technologies, the current challenge of cybersecurity at banks lies in scalability, high costs and resources in both money and time for R&D of defence methods along with the threat of high-tech cybercriminals growing and expanding. This report goes from introducing the importance of cybersecurity at banks, analyzing their management, operational and business objectives, evaluating pre-industry 4.0 technologies used for cybersecurity at banks to assessing post-industry 4.0 technologies focusing on Artificial Intelligence and Blockchain, discussing current policies and practices and ending with discussing key advantages and challenges for 4.0 technologies and recommendations for further developing cybersecurity at banks.",
        "field": "Cybersecurity in Banking",
        "link": "http://arxiv.org/abs/2503.00070v1"
    },
    {
        "id": "2503.10644v1",
        "title": "Combined climate stress testing of supply-chain networks and the financial system with nation-wide firm-level emission estimates",
        "authors": [
            "Zlata TabachovÃ¡",
            "Christian Diem",
            "Johannes Stangl",
            "AndrÃ¡s Borsos",
            "Stefan Thurner"
        ],
        "published": "2025-02-25T20:25:47Z",
        "summary": "On the way towards carbon neutrality, climate stress testing provides estimates for the physical and transition risks that climate change poses to the economy and the financial system. Missing firm-level CO2 emissions data severely impedes the assessment of transition risks originating from carbon pricing. Based on the individual emissions of all Hungarian firms (410,523), as estimated from their fossil fuel purchases, we conduct a stress test of both actual and hypothetical carbon pricing policies. Using a simple 1:1 economic ABM and introducing the new carbon-to-profit ratio, we identify firms that become unprofitable and default, and estimate the respective loan write-offs. We find that 45% of all companies are directly exposed to carbon pricing. At a price of 45 EUR/t, direct economic losses of 1.3% of total sales and bank equity losses of 1.2% are expected. Secondary default cascades in supply chain networks could increase these losses by 300% to 4000%, depending on firms' ability to substitute essential inputs. To reduce transition risks, firms should reduce their dependence on essential inputs from supply chains with high CO2 exposure. We discuss the implications of different policy implementations on these transition risks.",
        "field": "Cybersecurity in Banking",
        "link": "http://arxiv.org/abs/2503.10644v1"
    },
    {
        "id": "2502.14766v2",
        "title": "Multi-Layer Deep xVA: Structural Credit Models, Measure Changes and Convergence Analysis",
        "authors": [
            "Kristoffer Andersson",
            "Alessandro Gnoatto"
        ],
        "published": "2025-02-20T17:41:55Z",
        "summary": "We propose a structural default model for portfolio-wide valuation adjustments (xVAs) and represent it as a system of coupled backward stochastic differential equations. The framework is divided into four layers, each capturing a key component: (i) clean values, (ii) initial margin and Collateral Valuation Adjustment (ColVA), (iii) Credit/Debit Valuation Adjustments (CVA/DVA) together with Margin Valuation Adjustment (MVA), and (iv) Funding Valuation Adjustment (FVA). Because these layers depend on one another through collateral and default effects, a naive Monte Carlo approach would require deeply nested simulations, making the problem computationally intractable. To address this challenge, we use an iterative deep BSDE approach, handling each layer sequentially so that earlier outputs serve as inputs to the subsequent layers. Initial margin is computed via deep quantile regression to reflect margin requirements over the Margin Period of Risk. We also adopt a change-of-measure method that highlights rare but significant defaults of the bank or counterparty, ensuring that these events are accurately captured in the training process. We further extend Han and Long's (2020) a posteriori error analysis to BSDEs on bounded domains. Due to the random exit from the domain, we obtain an order of convergence of $\\mathcal{O}(h^{1/4-\\epsilon})$ rather than the usual $\\mathcal{O}(h^{1/2})$. Numerical experiments illustrate that this method drastically reduces computational demands and successfully scales to high-dimensional, non-symmetric portfolios. The results confirm its effectiveness and accuracy, offering a practical alternative to nested Monte Carlo simulations in multi-counterparty xVA analyses.",
        "field": "Cybersecurity in Banking",
        "link": "http://arxiv.org/abs/2502.14766v2"
    },
    {
        "id": "2503.09823v1",
        "title": "Data Traceability for Privacy Alignment",
        "authors": [
            "Kevin Liao",
            "Shreya Thipireddy",
            "Daniel Weitzner"
        ],
        "published": "2025-03-12T20:42:23Z",
        "summary": "This paper offers a new privacy approach for the growing ecosystem of services--ranging from open banking to healthcare--dependent on sensitive personal data sharing between individuals and third-parties. While these services offer significant benefits, individuals want control over their data, transparency regarding how their data is used, and accountability from third-parties for misuse. However, existing legal and technical mechanisms are inadequate for supporting these needs. A comprehensive approach to the modern privacy challenges of accountable third-party data sharing requires a closer alignment of technical system architecture and legal institutional design. In order to achieve this privacy alignment, we extend traditional security threat modeling and analysis to encompass a broader range of privacy notions than has been typically considered. In particular, we introduce the concept of covert-accountability, which addresses adversaries that may act dishonestly but face potential identification and legal consequences. As a concrete instance of this design approach, we present the OTrace protocol, designed to provide traceable, accountable, consumer-control in third-party data sharing ecosystems. OTrace empowers consumers with the knowledge of where their data is, who has it, what it is being used for, and whom it is being shared with. By applying our alignment framework to OTrace, we demonstrate that OTrace's technical affordances can provide more confident, scalable regulatory oversight when combined with complementary legal mechanisms.",
        "field": "Biometrics in Banking",
        "link": "http://arxiv.org/abs/2503.09823v1"
    },
    {
        "id": "2503.09586v1",
        "title": "Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot",
        "authors": [
            "Andrew Crossman",
            "Andrew R. Plummer",
            "Chandra Sekharudu",
            "Deepak Warrier",
            "Mohammad Yekrangian"
        ],
        "published": "2025-03-12T17:54:18Z",
        "summary": "We present Auspex - a threat modeling system built using a specialized collection of generative artificial intelligence-based methods that capture threat modeling tradecraft. This new approach, called tradecraft prompting, centers on encoding the on-the-ground knowledge of threat modelers within the prompts that drive a generative AI-based threat modeling system. Auspex employs tradecraft prompts in two processing stages. The first stage centers on ingesting and processing system architecture information using prompts that encode threat modeling tradecraft knowledge pertaining to system decomposition and description. The second stage centers on chaining the resulting system analysis through a collection of prompts that encode tradecraft knowledge on threat identification, classification, and mitigation. The two-stage process yields a threat matrix for a system that specifies threat scenarios, threat types, information security categorizations and potential mitigations. Auspex produces formalized threat model output in minutes, relative to the weeks or months a manual process takes. More broadly, the focus on bespoke tradecraft prompting, as opposed to fine-tuning or agent-based add-ons, makes Auspex a lightweight, flexible, modular, and extensible foundational system capable of addressing the complexity, resource, and standardization limitations of both existing manual and automated threat modeling processes. In this connection, we establish the baseline value of Auspex to threat modelers through an evaluation procedure based on feedback collected from cybersecurity subject matter experts measuring the quality and utility of threat models generated by Auspex on real banking systems. We conclude with a discussion of system performance and plans for enhancements to Auspex.",
        "field": "Biometrics in Banking",
        "link": "http://arxiv.org/abs/2503.09586v1"
    },
    {
        "id": "2503.00070v1",
        "title": "Systematic Review of Cybersecurity in Banking: Evolution from Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain, Policies and Practice",
        "authors": [
            "Tue Nhi Tran"
        ],
        "published": "2025-02-27T14:17:06Z",
        "summary": "Throughout the history from pre-industry 4.0 to post-industry 4.0, cybersecurity at banks has undergone significant changes. Pre-industry 4.0 cyber security at banks relied on individual security methods that were highly manual and had low accuracy. When moving to post-industry 4.0, cybersecurity at banks had a major turning point with security methods that combined different technologies such as Artificial Intelligence (AI), Blockchain, IoT, automating necessary processes and significantly increasing the defence layer for banks. However, along with the development of new technologies, the current challenge of cybersecurity at banks lies in scalability, high costs and resources in both money and time for R&D of defence methods along with the threat of high-tech cybercriminals growing and expanding. This report goes from introducing the importance of cybersecurity at banks, analyzing their management, operational and business objectives, evaluating pre-industry 4.0 technologies used for cybersecurity at banks to assessing post-industry 4.0 technologies focusing on Artificial Intelligence and Blockchain, discussing current policies and practices and ending with discussing key advantages and challenges for 4.0 technologies and recommendations for further developing cybersecurity at banks.",
        "field": "Biometrics in Banking",
        "link": "http://arxiv.org/abs/2503.00070v1"
    },
    {
        "id": "2503.10644v1",
        "title": "Combined climate stress testing of supply-chain networks and the financial system with nation-wide firm-level emission estimates",
        "authors": [
            "Zlata TabachovÃ¡",
            "Christian Diem",
            "Johannes Stangl",
            "AndrÃ¡s Borsos",
            "Stefan Thurner"
        ],
        "published": "2025-02-25T20:25:47Z",
        "summary": "On the way towards carbon neutrality, climate stress testing provides estimates for the physical and transition risks that climate change poses to the economy and the financial system. Missing firm-level CO2 emissions data severely impedes the assessment of transition risks originating from carbon pricing. Based on the individual emissions of all Hungarian firms (410,523), as estimated from their fossil fuel purchases, we conduct a stress test of both actual and hypothetical carbon pricing policies. Using a simple 1:1 economic ABM and introducing the new carbon-to-profit ratio, we identify firms that become unprofitable and default, and estimate the respective loan write-offs. We find that 45% of all companies are directly exposed to carbon pricing. At a price of 45 EUR/t, direct economic losses of 1.3% of total sales and bank equity losses of 1.2% are expected. Secondary default cascades in supply chain networks could increase these losses by 300% to 4000%, depending on firms' ability to substitute essential inputs. To reduce transition risks, firms should reduce their dependence on essential inputs from supply chains with high CO2 exposure. We discuss the implications of different policy implementations on these transition risks.",
        "field": "Biometrics in Banking",
        "link": "http://arxiv.org/abs/2503.10644v1"
    },
    {
        "id": "2502.14766v2",
        "title": "Multi-Layer Deep xVA: Structural Credit Models, Measure Changes and Convergence Analysis",
        "authors": [
            "Kristoffer Andersson",
            "Alessandro Gnoatto"
        ],
        "published": "2025-02-20T17:41:55Z",
        "summary": "We propose a structural default model for portfolio-wide valuation adjustments (xVAs) and represent it as a system of coupled backward stochastic differential equations. The framework is divided into four layers, each capturing a key component: (i) clean values, (ii) initial margin and Collateral Valuation Adjustment (ColVA), (iii) Credit/Debit Valuation Adjustments (CVA/DVA) together with Margin Valuation Adjustment (MVA), and (iv) Funding Valuation Adjustment (FVA). Because these layers depend on one another through collateral and default effects, a naive Monte Carlo approach would require deeply nested simulations, making the problem computationally intractable. To address this challenge, we use an iterative deep BSDE approach, handling each layer sequentially so that earlier outputs serve as inputs to the subsequent layers. Initial margin is computed via deep quantile regression to reflect margin requirements over the Margin Period of Risk. We also adopt a change-of-measure method that highlights rare but significant defaults of the bank or counterparty, ensuring that these events are accurately captured in the training process. We further extend Han and Long's (2020) a posteriori error analysis to BSDEs on bounded domains. Due to the random exit from the domain, we obtain an order of convergence of $\\mathcal{O}(h^{1/4-\\epsilon})$ rather than the usual $\\mathcal{O}(h^{1/2})$. Numerical experiments illustrate that this method drastically reduces computational demands and successfully scales to high-dimensional, non-symmetric portfolios. The results confirm its effectiveness and accuracy, offering a practical alternative to nested Monte Carlo simulations in multi-counterparty xVA analyses.",
        "field": "Biometrics in Banking",
        "link": "http://arxiv.org/abs/2502.14766v2"
    },
    {
        "id": "2503.09823v1",
        "title": "Data Traceability for Privacy Alignment",
        "authors": [
            "Kevin Liao",
            "Shreya Thipireddy",
            "Daniel Weitzner"
        ],
        "published": "2025-03-12T20:42:23Z",
        "summary": "This paper offers a new privacy approach for the growing ecosystem of services--ranging from open banking to healthcare--dependent on sensitive personal data sharing between individuals and third-parties. While these services offer significant benefits, individuals want control over their data, transparency regarding how their data is used, and accountability from third-parties for misuse. However, existing legal and technical mechanisms are inadequate for supporting these needs. A comprehensive approach to the modern privacy challenges of accountable third-party data sharing requires a closer alignment of technical system architecture and legal institutional design. In order to achieve this privacy alignment, we extend traditional security threat modeling and analysis to encompass a broader range of privacy notions than has been typically considered. In particular, we introduce the concept of covert-accountability, which addresses adversaries that may act dishonestly but face potential identification and legal consequences. As a concrete instance of this design approach, we present the OTrace protocol, designed to provide traceable, accountable, consumer-control in third-party data sharing ecosystems. OTrace empowers consumers with the knowledge of where their data is, who has it, what it is being used for, and whom it is being shared with. By applying our alignment framework to OTrace, we demonstrate that OTrace's technical affordances can provide more confident, scalable regulatory oversight when combined with complementary legal mechanisms.",
        "field": "Edge Computing in Banking",
        "link": "http://arxiv.org/abs/2503.09823v1"
    },
    {
        "id": "2503.09823v1",
        "title": "Data Traceability for Privacy Alignment",
        "authors": [
            "Kevin Liao",
            "Shreya Thipireddy",
            "Daniel Weitzner"
        ],
        "published": "2025-03-12T20:42:23Z",
        "summary": "This paper offers a new privacy approach for the growing ecosystem of services--ranging from open banking to healthcare--dependent on sensitive personal data sharing between individuals and third-parties. While these services offer significant benefits, individuals want control over their data, transparency regarding how their data is used, and accountability from third-parties for misuse. However, existing legal and technical mechanisms are inadequate for supporting these needs. A comprehensive approach to the modern privacy challenges of accountable third-party data sharing requires a closer alignment of technical system architecture and legal institutional design. In order to achieve this privacy alignment, we extend traditional security threat modeling and analysis to encompass a broader range of privacy notions than has been typically considered. In particular, we introduce the concept of covert-accountability, which addresses adversaries that may act dishonestly but face potential identification and legal consequences. As a concrete instance of this design approach, we present the OTrace protocol, designed to provide traceable, accountable, consumer-control in third-party data sharing ecosystems. OTrace empowers consumers with the knowledge of where their data is, who has it, what it is being used for, and whom it is being shared with. By applying our alignment framework to OTrace, we demonstrate that OTrace's technical affordances can provide more confident, scalable regulatory oversight when combined with complementary legal mechanisms.",
        "field": "Internet of Things (IoT) in Banking",
        "link": "http://arxiv.org/abs/2503.09823v1"
    },
    {
        "id": "2503.09586v1",
        "title": "Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot",
        "authors": [
            "Andrew Crossman",
            "Andrew R. Plummer",
            "Chandra Sekharudu",
            "Deepak Warrier",
            "Mohammad Yekrangian"
        ],
        "published": "2025-03-12T17:54:18Z",
        "summary": "We present Auspex - a threat modeling system built using a specialized collection of generative artificial intelligence-based methods that capture threat modeling tradecraft. This new approach, called tradecraft prompting, centers on encoding the on-the-ground knowledge of threat modelers within the prompts that drive a generative AI-based threat modeling system. Auspex employs tradecraft prompts in two processing stages. The first stage centers on ingesting and processing system architecture information using prompts that encode threat modeling tradecraft knowledge pertaining to system decomposition and description. The second stage centers on chaining the resulting system analysis through a collection of prompts that encode tradecraft knowledge on threat identification, classification, and mitigation. The two-stage process yields a threat matrix for a system that specifies threat scenarios, threat types, information security categorizations and potential mitigations. Auspex produces formalized threat model output in minutes, relative to the weeks or months a manual process takes. More broadly, the focus on bespoke tradecraft prompting, as opposed to fine-tuning or agent-based add-ons, makes Auspex a lightweight, flexible, modular, and extensible foundational system capable of addressing the complexity, resource, and standardization limitations of both existing manual and automated threat modeling processes. In this connection, we establish the baseline value of Auspex to threat modelers through an evaluation procedure based on feedback collected from cybersecurity subject matter experts measuring the quality and utility of threat models generated by Auspex on real banking systems. We conclude with a discussion of system performance and plans for enhancements to Auspex.",
        "field": "Internet of Things (IoT) in Banking",
        "link": "http://arxiv.org/abs/2503.09586v1"
    },
    {
        "id": "2503.00070v1",
        "title": "Systematic Review of Cybersecurity in Banking: Evolution from Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain, Policies and Practice",
        "authors": [
            "Tue Nhi Tran"
        ],
        "published": "2025-02-27T14:17:06Z",
        "summary": "Throughout the history from pre-industry 4.0 to post-industry 4.0, cybersecurity at banks has undergone significant changes. Pre-industry 4.0 cyber security at banks relied on individual security methods that were highly manual and had low accuracy. When moving to post-industry 4.0, cybersecurity at banks had a major turning point with security methods that combined different technologies such as Artificial Intelligence (AI), Blockchain, IoT, automating necessary processes and significantly increasing the defence layer for banks. However, along with the development of new technologies, the current challenge of cybersecurity at banks lies in scalability, high costs and resources in both money and time for R&D of defence methods along with the threat of high-tech cybercriminals growing and expanding. This report goes from introducing the importance of cybersecurity at banks, analyzing their management, operational and business objectives, evaluating pre-industry 4.0 technologies used for cybersecurity at banks to assessing post-industry 4.0 technologies focusing on Artificial Intelligence and Blockchain, discussing current policies and practices and ending with discussing key advantages and challenges for 4.0 technologies and recommendations for further developing cybersecurity at banks.",
        "field": "Internet of Things (IoT) in Banking",
        "link": "http://arxiv.org/abs/2503.00070v1"
    },
    {
        "id": "2503.10644v1",
        "title": "Combined climate stress testing of supply-chain networks and the financial system with nation-wide firm-level emission estimates",
        "authors": [
            "Zlata TabachovÃ¡",
            "Christian Diem",
            "Johannes Stangl",
            "AndrÃ¡s Borsos",
            "Stefan Thurner"
        ],
        "published": "2025-02-25T20:25:47Z",
        "summary": "On the way towards carbon neutrality, climate stress testing provides estimates for the physical and transition risks that climate change poses to the economy and the financial system. Missing firm-level CO2 emissions data severely impedes the assessment of transition risks originating from carbon pricing. Based on the individual emissions of all Hungarian firms (410,523), as estimated from their fossil fuel purchases, we conduct a stress test of both actual and hypothetical carbon pricing policies. Using a simple 1:1 economic ABM and introducing the new carbon-to-profit ratio, we identify firms that become unprofitable and default, and estimate the respective loan write-offs. We find that 45% of all companies are directly exposed to carbon pricing. At a price of 45 EUR/t, direct economic losses of 1.3% of total sales and bank equity losses of 1.2% are expected. Secondary default cascades in supply chain networks could increase these losses by 300% to 4000%, depending on firms' ability to substitute essential inputs. To reduce transition risks, firms should reduce their dependence on essential inputs from supply chains with high CO2 exposure. We discuss the implications of different policy implementations on these transition risks.",
        "field": "Internet of Things (IoT) in Banking",
        "link": "http://arxiv.org/abs/2503.10644v1"
    },
    {
        "id": "2502.14766v2",
        "title": "Multi-Layer Deep xVA: Structural Credit Models, Measure Changes and Convergence Analysis",
        "authors": [
            "Kristoffer Andersson",
            "Alessandro Gnoatto"
        ],
        "published": "2025-02-20T17:41:55Z",
        "summary": "We propose a structural default model for portfolio-wide valuation adjustments (xVAs) and represent it as a system of coupled backward stochastic differential equations. The framework is divided into four layers, each capturing a key component: (i) clean values, (ii) initial margin and Collateral Valuation Adjustment (ColVA), (iii) Credit/Debit Valuation Adjustments (CVA/DVA) together with Margin Valuation Adjustment (MVA), and (iv) Funding Valuation Adjustment (FVA). Because these layers depend on one another through collateral and default effects, a naive Monte Carlo approach would require deeply nested simulations, making the problem computationally intractable. To address this challenge, we use an iterative deep BSDE approach, handling each layer sequentially so that earlier outputs serve as inputs to the subsequent layers. Initial margin is computed via deep quantile regression to reflect margin requirements over the Margin Period of Risk. We also adopt a change-of-measure method that highlights rare but significant defaults of the bank or counterparty, ensuring that these events are accurately captured in the training process. We further extend Han and Long's (2020) a posteriori error analysis to BSDEs on bounded domains. Due to the random exit from the domain, we obtain an order of convergence of $\\mathcal{O}(h^{1/4-\\epsilon})$ rather than the usual $\\mathcal{O}(h^{1/2})$. Numerical experiments illustrate that this method drastically reduces computational demands and successfully scales to high-dimensional, non-symmetric portfolios. The results confirm its effectiveness and accuracy, offering a practical alternative to nested Monte Carlo simulations in multi-counterparty xVA analyses.",
        "field": "Internet of Things (IoT) in Banking",
        "link": "http://arxiv.org/abs/2502.14766v2"
    },
    {
        "id": "2503.09823v1",
        "title": "Data Traceability for Privacy Alignment",
        "authors": [
            "Kevin Liao",
            "Shreya Thipireddy",
            "Daniel Weitzner"
        ],
        "published": "2025-03-12T20:42:23Z",
        "summary": "This paper offers a new privacy approach for the growing ecosystem of services--ranging from open banking to healthcare--dependent on sensitive personal data sharing between individuals and third-parties. While these services offer significant benefits, individuals want control over their data, transparency regarding how their data is used, and accountability from third-parties for misuse. However, existing legal and technical mechanisms are inadequate for supporting these needs. A comprehensive approach to the modern privacy challenges of accountable third-party data sharing requires a closer alignment of technical system architecture and legal institutional design. In order to achieve this privacy alignment, we extend traditional security threat modeling and analysis to encompass a broader range of privacy notions than has been typically considered. In particular, we introduce the concept of covert-accountability, which addresses adversaries that may act dishonestly but face potential identification and legal consequences. As a concrete instance of this design approach, we present the OTrace protocol, designed to provide traceable, accountable, consumer-control in third-party data sharing ecosystems. OTrace empowers consumers with the knowledge of where their data is, who has it, what it is being used for, and whom it is being shared with. By applying our alignment framework to OTrace, we demonstrate that OTrace's technical affordances can provide more confident, scalable regulatory oversight when combined with complementary legal mechanisms.",
        "field": "5G Technology in Banking",
        "link": "http://arxiv.org/abs/2503.09823v1"
    },
    {
        "id": "2503.09586v1",
        "title": "Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot",
        "authors": [
            "Andrew Crossman",
            "Andrew R. Plummer",
            "Chandra Sekharudu",
            "Deepak Warrier",
            "Mohammad Yekrangian"
        ],
        "published": "2025-03-12T17:54:18Z",
        "summary": "We present Auspex - a threat modeling system built using a specialized collection of generative artificial intelligence-based methods that capture threat modeling tradecraft. This new approach, called tradecraft prompting, centers on encoding the on-the-ground knowledge of threat modelers within the prompts that drive a generative AI-based threat modeling system. Auspex employs tradecraft prompts in two processing stages. The first stage centers on ingesting and processing system architecture information using prompts that encode threat modeling tradecraft knowledge pertaining to system decomposition and description. The second stage centers on chaining the resulting system analysis through a collection of prompts that encode tradecraft knowledge on threat identification, classification, and mitigation. The two-stage process yields a threat matrix for a system that specifies threat scenarios, threat types, information security categorizations and potential mitigations. Auspex produces formalized threat model output in minutes, relative to the weeks or months a manual process takes. More broadly, the focus on bespoke tradecraft prompting, as opposed to fine-tuning or agent-based add-ons, makes Auspex a lightweight, flexible, modular, and extensible foundational system capable of addressing the complexity, resource, and standardization limitations of both existing manual and automated threat modeling processes. In this connection, we establish the baseline value of Auspex to threat modelers through an evaluation procedure based on feedback collected from cybersecurity subject matter experts measuring the quality and utility of threat models generated by Auspex on real banking systems. We conclude with a discussion of system performance and plans for enhancements to Auspex.",
        "field": "5G Technology in Banking",
        "link": "http://arxiv.org/abs/2503.09586v1"
    },
    {
        "id": "2503.00070v1",
        "title": "Systematic Review of Cybersecurity in Banking: Evolution from Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain, Policies and Practice",
        "authors": [
            "Tue Nhi Tran"
        ],
        "published": "2025-02-27T14:17:06Z",
        "summary": "Throughout the history from pre-industry 4.0 to post-industry 4.0, cybersecurity at banks has undergone significant changes. Pre-industry 4.0 cyber security at banks relied on individual security methods that were highly manual and had low accuracy. When moving to post-industry 4.0, cybersecurity at banks had a major turning point with security methods that combined different technologies such as Artificial Intelligence (AI), Blockchain, IoT, automating necessary processes and significantly increasing the defence layer for banks. However, along with the development of new technologies, the current challenge of cybersecurity at banks lies in scalability, high costs and resources in both money and time for R&D of defence methods along with the threat of high-tech cybercriminals growing and expanding. This report goes from introducing the importance of cybersecurity at banks, analyzing their management, operational and business objectives, evaluating pre-industry 4.0 technologies used for cybersecurity at banks to assessing post-industry 4.0 technologies focusing on Artificial Intelligence and Blockchain, discussing current policies and practices and ending with discussing key advantages and challenges for 4.0 technologies and recommendations for further developing cybersecurity at banks.",
        "field": "5G Technology in Banking",
        "link": "http://arxiv.org/abs/2503.00070v1"
    },
    {
        "id": "2503.10644v1",
        "title": "Combined climate stress testing of supply-chain networks and the financial system with nation-wide firm-level emission estimates",
        "authors": [
            "Zlata TabachovÃ¡",
            "Christian Diem",
            "Johannes Stangl",
            "AndrÃ¡s Borsos",
            "Stefan Thurner"
        ],
        "published": "2025-02-25T20:25:47Z",
        "summary": "On the way towards carbon neutrality, climate stress testing provides estimates for the physical and transition risks that climate change poses to the economy and the financial system. Missing firm-level CO2 emissions data severely impedes the assessment of transition risks originating from carbon pricing. Based on the individual emissions of all Hungarian firms (410,523), as estimated from their fossil fuel purchases, we conduct a stress test of both actual and hypothetical carbon pricing policies. Using a simple 1:1 economic ABM and introducing the new carbon-to-profit ratio, we identify firms that become unprofitable and default, and estimate the respective loan write-offs. We find that 45% of all companies are directly exposed to carbon pricing. At a price of 45 EUR/t, direct economic losses of 1.3% of total sales and bank equity losses of 1.2% are expected. Secondary default cascades in supply chain networks could increase these losses by 300% to 4000%, depending on firms' ability to substitute essential inputs. To reduce transition risks, firms should reduce their dependence on essential inputs from supply chains with high CO2 exposure. We discuss the implications of different policy implementations on these transition risks.",
        "field": "5G Technology in Banking",
        "link": "http://arxiv.org/abs/2503.10644v1"
    },
    {
        "id": "2502.14766v2",
        "title": "Multi-Layer Deep xVA: Structural Credit Models, Measure Changes and Convergence Analysis",
        "authors": [
            "Kristoffer Andersson",
            "Alessandro Gnoatto"
        ],
        "published": "2025-02-20T17:41:55Z",
        "summary": "We propose a structural default model for portfolio-wide valuation adjustments (xVAs) and represent it as a system of coupled backward stochastic differential equations. The framework is divided into four layers, each capturing a key component: (i) clean values, (ii) initial margin and Collateral Valuation Adjustment (ColVA), (iii) Credit/Debit Valuation Adjustments (CVA/DVA) together with Margin Valuation Adjustment (MVA), and (iv) Funding Valuation Adjustment (FVA). Because these layers depend on one another through collateral and default effects, a naive Monte Carlo approach would require deeply nested simulations, making the problem computationally intractable. To address this challenge, we use an iterative deep BSDE approach, handling each layer sequentially so that earlier outputs serve as inputs to the subsequent layers. Initial margin is computed via deep quantile regression to reflect margin requirements over the Margin Period of Risk. We also adopt a change-of-measure method that highlights rare but significant defaults of the bank or counterparty, ensuring that these events are accurately captured in the training process. We further extend Han and Long's (2020) a posteriori error analysis to BSDEs on bounded domains. Due to the random exit from the domain, we obtain an order of convergence of $\\mathcal{O}(h^{1/4-\\epsilon})$ rather than the usual $\\mathcal{O}(h^{1/2})$. Numerical experiments illustrate that this method drastically reduces computational demands and successfully scales to high-dimensional, non-symmetric portfolios. The results confirm its effectiveness and accuracy, offering a practical alternative to nested Monte Carlo simulations in multi-counterparty xVA analyses.",
        "field": "5G Technology in Banking",
        "link": "http://arxiv.org/abs/2502.14766v2"
    },
    {
        "id": "2503.11619v1",
        "title": "Tit-for-Tat: Safeguarding Large Vision-Language Models Against Jailbreak Attacks via Adversarial Defense",
        "authors": [
            "Shuyang Hao",
            "Yiwei Wang",
            "Bryan Hooi",
            "Ming-Hsuan Yang",
            "Jun Liu",
            "Chengcheng Tang",
            "Zi Huang",
            "Yujun Cai"
        ],
        "published": "2025-03-14T17:39:45Z",
        "summary": "Deploying large vision-language models (LVLMs) introduces a unique vulnerability: susceptibility to malicious attacks via visual inputs. However, existing defense methods suffer from two key limitations: (1) They solely focus on textual defenses, fail to directly address threats in the visual domain where attacks originate, and (2) the additional processing steps often incur significant computational overhead or compromise model performance on benign tasks. Building on these insights, we propose ESIII (Embedding Security Instructions Into Images), a novel methodology for transforming the visual space from a source of vulnerability into an active defense mechanism. Initially, we embed security instructions into defensive images through gradient-based optimization, obtaining security instructions in the visual dimension. Subsequently, we integrate security instructions from visual and textual dimensions with the input query. The collaboration between security instructions from different dimensions ensures comprehensive security protection. Extensive experiments demonstrate that our approach effectively fortifies the robustness of LVLMs against such attacks while preserving their performance on standard benign tasks and incurring an imperceptible increase in time costs.",
        "field": "Neuromorphic Computing",
        "link": "http://arxiv.org/abs/2503.11619v1"
    },
    {
        "id": "2503.11573v1",
        "title": "Synthesizing Access Control Policies using Large Language Models",
        "authors": [
            "Adarsh Vatsa",
            "Pratyush Patel",
            "William Eiers"
        ],
        "published": "2025-03-14T16:40:25Z",
        "summary": "Cloud compute systems allow administrators to write access control policies that govern access to private data. While policies are written in convenient languages, such as AWS Identity and Access Management Policy Language, manually written policies often become complex and error prone. In this paper, we investigate whether and how well Large Language Models (LLMs) can be used to synthesize access control policies. Our investigation focuses on the task of taking an access control request specification and zero-shot prompting LLMs to synthesize a well-formed access control policy which correctly adheres to the request specification. We consider two scenarios, one which the request specification is given as a concrete list of requests to be allowed or denied, and another in which a natural language description is used to specify sets of requests to be allowed or denied. We then argue that for zero-shot prompting, more precise and structured prompts using a syntax based approach are necessary and experimentally show preliminary results validating our approach.",
        "field": "Neuromorphic Computing",
        "link": "http://arxiv.org/abs/2503.11573v1"
    },
    {
        "id": "2503.11404v1",
        "title": "Towards A Correct Usage of Cryptography in Semantic Watermarks for Diffusion Models",
        "authors": [
            "Jonas Thietke",
            "Andreas MÃ¼ller",
            "Denis Lukovnikov",
            "Asja Fischer",
            "Erwin Quiring"
        ],
        "published": "2025-03-14T13:45:46Z",
        "summary": "Semantic watermarking methods enable the direct integration of watermarks into the generation process of latent diffusion models by only modifying the initial latent noise. One line of approaches building on Gaussian Shading relies on cryptographic primitives to steer the sampling process of the latent noise. However, we identify several issues in the usage of cryptographic techniques in Gaussian Shading, particularly in its proof of lossless performance and key management, causing ambiguity in follow-up works, too. In this work, we therefore revisit the cryptographic primitives for semantic watermarking. We introduce a novel, general proof of lossless performance based on IND\\$-CPA security for semantic watermarks. We then discuss the configuration of the cryptographic primitives in semantic watermarks with respect to security, efficiency, and generation quality.",
        "field": "Neuromorphic Computing",
        "link": "http://arxiv.org/abs/2503.11404v1"
    },
    {
        "id": "2503.11216v1",
        "title": "Cross-Platform Benchmarking of the FHE Libraries: Novel Insights into SEAL and Openfhe",
        "authors": [
            "Faneela",
            "Jawad Ahmad",
            "Baraq Ghaleb",
            "Sana Ullah Jan",
            "William J. Buchanan"
        ],
        "published": "2025-03-14T09:08:30Z",
        "summary": "The rapid growth of cloud computing and data-driven applications has amplified privacy concerns, driven by the increasing demand to process sensitive data securely. Homomorphic encryption (HE) has become a vital solution for addressing these concerns by enabling computations on encrypted data without revealing its contents. This paper provides a comprehensive evaluation of two leading HE libraries, SEAL and OpenFHE, examining their performance, usability, and support for prominent HE schemes such as BGV and CKKS. Our analysis highlights computational efficiency, memory usage, and scalability across Linux and Windows platforms, emphasizing their applicability in real-world scenarios. Results reveal that Linux outperforms Windows in computation efficiency, with OpenFHE emerging as the optimal choice across diverse cryptographic settings. This paper provides valuable insights for researchers and practitioners to advance privacy-preserving applications using FHE.",
        "field": "Neuromorphic Computing",
        "link": "http://arxiv.org/abs/2503.11216v1"
    },
    {
        "id": "2503.11185v1",
        "title": "Align in Depth: Defending Jailbreak Attacks via Progressive Answer Detoxification",
        "authors": [
            "Yingjie Zhang",
            "Tong Liu",
            "Zhe Zhao",
            "Guozhu Meng",
            "Kai Chen"
        ],
        "published": "2025-03-14T08:32:12Z",
        "summary": "Large Language Models (LLMs) are vulnerable to jailbreak attacks, which use crafted prompts to elicit toxic responses. These attacks exploit LLMs' difficulty in dynamically detecting harmful intents during the generation process. Traditional safety alignment methods, often relying on the initial few generation steps, are ineffective due to limited computational budget. This paper proposes DEEPALIGN, a robust defense framework that fine-tunes LLMs to progressively detoxify generated content, significantly improving both the computational budget and effectiveness of mitigating harmful generation. Our approach uses a hybrid loss function operating on hidden states to directly improve LLMs' inherent awareness of toxity during generation. Furthermore, we redefine safe responses by generating semantically relevant answers to harmful queries, thereby increasing robustness against representation-mutation attacks. Evaluations across multiple LLMs demonstrate state-of-the-art defense performance against six different attack types, reducing Attack Success Rates by up to two orders of magnitude compared to previous state-of-the-art defense while preserving utility. This work advances LLM safety by addressing limitations of conventional alignment through dynamic, context-aware mitigation.",
        "field": "Neuromorphic Computing",
        "link": "http://arxiv.org/abs/2503.11185v1"
    },
    {
        "id": "2503.10269v1",
        "title": "Targeted Data Poisoning for Black-Box Audio Datasets Ownership Verification",
        "authors": [
            "Wassim Bouaziz",
            "El-Mahdi El-Mhamdi",
            "Nicolas Usunier"
        ],
        "published": "2025-03-13T11:25:25Z",
        "summary": "Protecting the use of audio datasets is a major concern for data owners, particularly with the recent rise of audio deep learning models. While watermarks can be used to protect the data itself, they do not allow to identify a deep learning model trained on a protected dataset. In this paper, we adapt to audio data the recently introduced data taggants approach. Data taggants is a method to verify if a neural network was trained on a protected image dataset with top-$k$ predictions access to the model only. This method relies on a targeted data poisoning scheme by discreetly altering a small fraction (1%) of the dataset as to induce a harmless behavior on out-of-distribution data called keys. We evaluate our method on the Speechcommands and the ESC50 datasets and state of the art transformer models, and show that we can detect the use of the dataset with high confidence without loss of performance. We also show the robustness of our method against common data augmentation techniques, making it a practical method to protect audio datasets.",
        "field": "Digital Identity Verification",
        "link": "http://arxiv.org/abs/2503.10269v1"
    },
    {
        "id": "2503.10171v1",
        "title": "Verifiable, Efficient and Confidentiality-Preserving Graph Search with Transparency",
        "authors": [
            "Qiuhao Wang",
            "Xu Yang",
            "Yiwei Liu",
            "Saiyu Qi",
            "Hongguang Zhao",
            "Ke Li",
            "Yong Qi"
        ],
        "published": "2025-03-13T08:53:53Z",
        "summary": "Graph databases have garnered extensive attention and research due to their ability to manage relationships between entities efficiently. Today, many graph search services have been outsourced to a third-party server to facilitate storage and computational support. Nevertheless, the outsourcing paradigm may invade the privacy of graphs. PeGraph is the latest scheme achieving encrypted search over social graphs to address the privacy leakage, which maintains two data structures XSet and TSet motivated by the OXT technology to support encrypted conjunctive search. However, PeGraph still exhibits limitations inherent to the underlying OXT. It does not provide transparent search capabilities, suffers from expensive computation and result pattern leakages, and it fails to support search over dynamic encrypted graph database and results verification. In this paper, we propose SecGraph to address the first two limitations, which adopts a novel system architecture that leverages an SGX-enabled cloud server to provide users with secure and transparent search services since the secret key protection and computational overhead have been offloaded to the cloud server. Besides, we design an LDCF-encoded XSet based on the Logarithmic Dynamic Cuckoo Filter to facilitate efficient plaintext computation in trusted memory, effectively mitigating the risks of result pattern leakage and performance degradation due to exceeding the limited trusted memory capacity. Finally, we design a new dynamic version of TSet named Twin-TSet to enable conjunctive search over dynamic encrypted graph database. In order to support verifiable search, we further propose VSecGraph, which utilizes a procedure-oriented verification method to verify all data structures loaded into the trusted memory, thus bypassing the computational overhead associated with the client's local verification.",
        "field": "Digital Identity Verification",
        "link": "http://arxiv.org/abs/2503.10171v1"
    },
    {
        "id": "2503.09513v1",
        "title": "RESTRAIN: Reinforcement Learning-Based Secure Framework for Trigger-Action IoT Environment",
        "authors": [
            "Md Morshed Alam",
            "Lokesh Chandra Das",
            "Sandip Roy",
            "Sachin Shetty",
            "Weichao Wang"
        ],
        "published": "2025-03-12T16:23:14Z",
        "summary": "Internet of Things (IoT) platforms with trigger-action capability allow event conditions to trigger actions in IoT devices autonomously by creating a chain of interactions. Adversaries exploit this chain of interactions to maliciously inject fake event conditions into IoT hubs, triggering unauthorized actions on target IoT devices to implement remote injection attacks. Existing defense mechanisms focus mainly on the verification of event transactions using physical event fingerprints to enforce the security policies to block unsafe event transactions. These approaches are designed to provide offline defense against injection attacks. The state-of-the-art online defense mechanisms offer real-time defense, but extensive reliability on the inference of attack impacts on the IoT network limits the generalization capability of these approaches. In this paper, we propose a platform-independent multi-agent online defense system, namely RESTRAIN, to counter remote injection attacks at runtime. RESTRAIN allows the defense agent to profile attack actions at runtime and leverages reinforcement learning to optimize a defense policy that complies with the security requirements of the IoT network. The experimental results show that the defense agent effectively takes real-time defense actions against complex and dynamic remote injection attacks and maximizes the security gain with minimal computational overhead.",
        "field": "Digital Identity Verification",
        "link": "http://arxiv.org/abs/2503.09513v1"
    },
    {
        "id": "2503.09433v1",
        "title": "CASTLE: Benchmarking Dataset for Static Code Analyzers and LLMs towards CWE Detection",
        "authors": [
            "Richard A. Dubniczky",
            "Krisztofer ZoltÃ¡n HorvÃ¡t",
            "TamÃ¡s Bisztray",
            "Mohamed Amine Ferrag",
            "Lucas C. Cordeiro",
            "Norbert Tihanyi"
        ],
        "published": "2025-03-12T14:30:05Z",
        "summary": "Identifying vulnerabilities in source code is crucial, especially in critical software components. Existing methods such as static analysis, dynamic analysis, formal verification, and recently Large Language Models are widely used to detect security flaws. This paper introduces CASTLE (CWE Automated Security Testing and Low-Level Evaluation), a benchmarking framework for evaluating the vulnerability detection capabilities of different methods. We assess 13 static analysis tools, 10 LLMs, and 2 formal verification tools using a hand-crafted dataset of 250 micro-benchmark programs covering 25 common CWEs. We propose the CASTLE Score, a novel evaluation metric to ensure fair comparison. Our results reveal key differences: ESBMC (a formal verification tool) minimizes false positives but struggles with vulnerabilities beyond model checking, such as weak cryptography or SQL injection. Static analyzers suffer from high false positives, increasing manual validation efforts for developers. LLMs perform exceptionally well in the CASTLE dataset when identifying vulnerabilities in small code snippets. However, their accuracy declines, and hallucinations increase as the code size grows. These results suggest that LLMs could play a pivotal role in future security solutions, particularly within code completion frameworks, where they can provide real-time guidance to prevent vulnerabilities. The dataset is accessible at https://github.com/CASTLE-Benchmark.",
        "field": "Digital Identity Verification",
        "link": "http://arxiv.org/abs/2503.09433v1"
    },
    {
        "id": "2503.08923v1",
        "title": "Enhancing Large Language Models for Hardware Verification: A Novel SystemVerilog Assertion Dataset",
        "authors": [
            "Anand Menon",
            "Samit S Miftah",
            "Shamik Kundu",
            "Souvik Kundu",
            "Amisha Srivastava",
            "Arnab Raha",
            "Gabriel Theodor Sonnenschein",
            "Suvadeep Banerjee",
            "Deepak Mathaikutty",
            "Kanad Basu"
        ],
        "published": "2025-03-11T22:13:26Z",
        "summary": "Hardware verification is crucial in modern SoC design, consuming around 70% of development time. SystemVerilog assertions ensure correct functionality. However, existing industrial practices rely on manual efforts for assertion generation, which becomes increasingly untenable as hardware systems become complex. Recent research shows that Large Language Models (LLMs) can automate this process. However, proprietary SOTA models like GPT-4o often generate inaccurate assertions and require expensive licenses, while smaller open-source LLMs need fine-tuning to manage HDL code complexities. To address these issues, we introduce **VERT**, an open-source dataset designed to enhance SystemVerilog assertion generation using LLMs. VERT enables researchers in academia and industry to fine-tune open-source models, outperforming larger proprietary ones in both accuracy and efficiency while ensuring data privacy through local fine-tuning and eliminating costly licenses. The dataset is curated by systematically augmenting variables from open-source HDL repositories to generate synthetic code snippets paired with corresponding assertions. Experimental results demonstrate that fine-tuned models like Deepseek Coder 6.7B and Llama 3.1 8B outperform GPT-4o, achieving up to 96.88% improvement over base models and 24.14% over GPT-4o on platforms including OpenTitan, CVA6, OpenPiton and Pulpissimo. VERT is available at https://github.com/AnandMenon12/VERT.",
        "field": "Digital Identity Verification",
        "link": "http://arxiv.org/abs/2503.08923v1"
    },
    {
        "id": "2503.11053v1",
        "title": "Pricing American Parisian Options under General Time-Inhomogeneous Markov Models",
        "authors": [
            "Yuhao Liu",
            "Nian Yang",
            "Gongqiu Zhang"
        ],
        "published": "2025-03-14T03:45:18Z",
        "summary": "This paper develops general approaches for pricing various types of American-style Parisian options (down-in/-out, perpetual/finite-maturity) with general payoff functions based on continuous-time Markov chain (CTMC) approximation under general 1D time-inhomogeneous Markov models. For the down-in types, by conditioning on the Parisian stopping time, we reduce the pricing problem to that of a series of vanilla American options with different maturities and their prices integrated with the distribution function of the Parisian stopping time yield the American Parisian down-in option price. This facilitates an efficient application of CTMC approximation to obtain the approximate option price by calculating the required quantities. For the perpetual down-in cases under time-homogeneous models, significant computational cost can be reduced. The down-out cases are more complicated, for which we use the state augmentation approach to record the excursion duration and then the approximate option price is obtained by solving a series of variational inequalities recursively with the Lemke's pivoting method. We show the convergence of CTMC approximation for all the types of American Parisian options under general time-inhomogeneous Markov models, and the accuracy and efficiency of our algorithms are confirmed with extensive numerical experiments.",
        "field": "Autonomous Finance",
        "link": "http://arxiv.org/abs/2503.11053v1"
    },
    {
        "id": "2503.09988v1",
        "title": "Label Unbalance in High-frequency Trading",
        "authors": [
            "Zijian Zhao",
            "Xuming Chen",
            "Jiayu Wen",
            "Mingwen Liu",
            "Xiaoteng Ma"
        ],
        "published": "2025-03-13T02:55:06Z",
        "summary": "In financial trading, return prediction is one of the foundation for a successful trading system. By the fast development of the deep learning in various areas such as graphical processing, natural language, it has also demonstrate significant edge in handling with financial data. While the success of the deep learning relies on huge amount of labeled sample, labeling each time/event as profitable or unprofitable, under the transaction cost, especially in the high-frequency trading world, suffers from serious label imbalance issue.In this paper, we adopts rigurious end-to-end deep learning framework with comprehensive label imbalance adjustment methods and succeed in predicting in high-frequency return in the Chinese future market. The code for our method is publicly available at https://github.com/RS2002/Label-Unbalance-in-High-Frequency-Trading .",
        "field": "Autonomous Finance",
        "link": "http://arxiv.org/abs/2503.09988v1"
    },
    {
        "id": "2503.09934v1",
        "title": "A Pharmacy Benefit Manager Insurance Business Model",
        "authors": [
            "Lawrence W. Abrams"
        ],
        "published": "2025-03-13T01:13:16Z",
        "summary": "It is time to move on from attempts to make the pharmacy benefit manager (PBM) reseller business model more transparent. Time and time again the Big 3 PBMs have developed opaque alternatives to piece-meal 100% pass-through mandates. Time and time again PBMs have demonstrated expertise in finding loopholes in state government disclosure laws. The purpose of this paper is to provide quantitative estimates of two transparent insurance business models as a solution to the PBM agency issue. The key parameter used is an 8% gross profit margin figure disclosed by the Big 3 PBMs themselves. Based on reported drug trend delivered to plans, we use a $1,200 to $1,500 per member per year (PMPY) as the range for this key performance indicator (KPI). We propose that discussions of PBM insurance business models start with the following figures: (1) a fixed premium model with medical loss ratio ranging from 92% to 85%; (2) a fee-for-service model ranging from $96 to $180 PMPY with risk sharing of deviations from a contracted PMPY delivered drug spend.",
        "field": "Autonomous Finance",
        "link": "http://arxiv.org/abs/2503.09934v1"
    },
    {
        "id": "2503.06707v1",
        "title": "Axes that matter: PCA with a difference",
        "authors": [
            "Brian Huge",
            "Antoine Savine"
        ],
        "published": "2025-03-09T17:47:25Z",
        "summary": "We extend the scope of differential machine learning and introduce a new breed of supervised principal component analysis to reduce dimensionality of Derivatives problems. Applications include the specification and calibration of pricing models, the identification of regression features in least-square Monte-Carlo, and the pre-processing of simulated datasets for (differential) machine learning.",
        "field": "Autonomous Finance",
        "link": "http://arxiv.org/abs/2503.06707v1"
    },
    {
        "id": "2503.06279v1",
        "title": "Mitigating Blockchain extractable value (BEV) threats by Distributed Transaction Sequencing in Blockchains",
        "authors": [
            "Xiongfei Zhao",
            "Hou-Wan Long",
            "Zhengzhe Li",
            "Jiangchuan Liu",
            "Yain-Whar Si"
        ],
        "published": "2025-03-08T16:55:52Z",
        "summary": "The rapid growth of Blockchain and Decentralized Finance (DeFi) has introduced new challenges and vulnerabilities that threaten the integrity and efficiency of the ecosystem. This study identifies critical issues such as Transaction Order Dependence (TOD), Blockchain Extractable Value (BEV), and Transaction Importance Diversity (TID), which collectively undermine the fairness and security of DeFi systems. BEV-related activities, including Sandwich attacks, Liquidations, and Transaction Replay, have emerged as significant threats, collectively generating $540.54 million in losses over 32 months across 11,289 addresses, involving 49,691 cryptocurrencies and 60,830 on-chain markets. These attacks exploit transaction mechanics to manipulate asset prices and extract value at the expense of other participants, with Sandwich attacks being particularly impactful. Additionally, the growing adoption of Blockchain in traditional finance highlights the challenge of TID, where high transaction volumes can strain systems and compromise time-sensitive operations. To address these pressing issues, we propose a novel Distributed Transaction Sequencing Strategy (DTSS), which combines forking mechanisms and the Analytic Hierarchy Process (AHP) to enforce fair and transparent transaction ordering in a decentralized manner. Our approach is further enhanced by an optimization framework and the introduction of the Normalized Allocation Disparity Metric (NADM), which ensures optimal parameter selection for transaction prioritization. Experimental evaluations demonstrate that DTSS effectively mitigates BEV risks, enhances transaction fairness, and significantly improves the security and transparency of DeFi ecosystems. This work is essential for protecting the future of decentralized finance and promoting its integration into global financial systems.",
        "field": "Autonomous Finance",
        "link": "http://arxiv.org/abs/2503.06279v1"
    },
    {
        "id": "2503.09823v1",
        "title": "Data Traceability for Privacy Alignment",
        "authors": [
            "Kevin Liao",
            "Shreya Thipireddy",
            "Daniel Weitzner"
        ],
        "published": "2025-03-12T20:42:23Z",
        "summary": "This paper offers a new privacy approach for the growing ecosystem of services--ranging from open banking to healthcare--dependent on sensitive personal data sharing between individuals and third-parties. While these services offer significant benefits, individuals want control over their data, transparency regarding how their data is used, and accountability from third-parties for misuse. However, existing legal and technical mechanisms are inadequate for supporting these needs. A comprehensive approach to the modern privacy challenges of accountable third-party data sharing requires a closer alignment of technical system architecture and legal institutional design. In order to achieve this privacy alignment, we extend traditional security threat modeling and analysis to encompass a broader range of privacy notions than has been typically considered. In particular, we introduce the concept of covert-accountability, which addresses adversaries that may act dishonestly but face potential identification and legal consequences. As a concrete instance of this design approach, we present the OTrace protocol, designed to provide traceable, accountable, consumer-control in third-party data sharing ecosystems. OTrace empowers consumers with the knowledge of where their data is, who has it, what it is being used for, and whom it is being shared with. By applying our alignment framework to OTrace, we demonstrate that OTrace's technical affordances can provide more confident, scalable regulatory oversight when combined with complementary legal mechanisms.",
        "field": "Synthetic Data in Banking",
        "link": "http://arxiv.org/abs/2503.09823v1"
    },
    {
        "id": "2503.09586v1",
        "title": "Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot",
        "authors": [
            "Andrew Crossman",
            "Andrew R. Plummer",
            "Chandra Sekharudu",
            "Deepak Warrier",
            "Mohammad Yekrangian"
        ],
        "published": "2025-03-12T17:54:18Z",
        "summary": "We present Auspex - a threat modeling system built using a specialized collection of generative artificial intelligence-based methods that capture threat modeling tradecraft. This new approach, called tradecraft prompting, centers on encoding the on-the-ground knowledge of threat modelers within the prompts that drive a generative AI-based threat modeling system. Auspex employs tradecraft prompts in two processing stages. The first stage centers on ingesting and processing system architecture information using prompts that encode threat modeling tradecraft knowledge pertaining to system decomposition and description. The second stage centers on chaining the resulting system analysis through a collection of prompts that encode tradecraft knowledge on threat identification, classification, and mitigation. The two-stage process yields a threat matrix for a system that specifies threat scenarios, threat types, information security categorizations and potential mitigations. Auspex produces formalized threat model output in minutes, relative to the weeks or months a manual process takes. More broadly, the focus on bespoke tradecraft prompting, as opposed to fine-tuning or agent-based add-ons, makes Auspex a lightweight, flexible, modular, and extensible foundational system capable of addressing the complexity, resource, and standardization limitations of both existing manual and automated threat modeling processes. In this connection, we establish the baseline value of Auspex to threat modelers through an evaluation procedure based on feedback collected from cybersecurity subject matter experts measuring the quality and utility of threat models generated by Auspex on real banking systems. We conclude with a discussion of system performance and plans for enhancements to Auspex.",
        "field": "Synthetic Data in Banking",
        "link": "http://arxiv.org/abs/2503.09586v1"
    },
    {
        "id": "2503.00070v1",
        "title": "Systematic Review of Cybersecurity in Banking: Evolution from Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain, Policies and Practice",
        "authors": [
            "Tue Nhi Tran"
        ],
        "published": "2025-02-27T14:17:06Z",
        "summary": "Throughout the history from pre-industry 4.0 to post-industry 4.0, cybersecurity at banks has undergone significant changes. Pre-industry 4.0 cyber security at banks relied on individual security methods that were highly manual and had low accuracy. When moving to post-industry 4.0, cybersecurity at banks had a major turning point with security methods that combined different technologies such as Artificial Intelligence (AI), Blockchain, IoT, automating necessary processes and significantly increasing the defence layer for banks. However, along with the development of new technologies, the current challenge of cybersecurity at banks lies in scalability, high costs and resources in both money and time for R&D of defence methods along with the threat of high-tech cybercriminals growing and expanding. This report goes from introducing the importance of cybersecurity at banks, analyzing their management, operational and business objectives, evaluating pre-industry 4.0 technologies used for cybersecurity at banks to assessing post-industry 4.0 technologies focusing on Artificial Intelligence and Blockchain, discussing current policies and practices and ending with discussing key advantages and challenges for 4.0 technologies and recommendations for further developing cybersecurity at banks.",
        "field": "Synthetic Data in Banking",
        "link": "http://arxiv.org/abs/2503.00070v1"
    },
    {
        "id": "2503.10644v1",
        "title": "Combined climate stress testing of supply-chain networks and the financial system with nation-wide firm-level emission estimates",
        "authors": [
            "Zlata TabachovÃ¡",
            "Christian Diem",
            "Johannes Stangl",
            "AndrÃ¡s Borsos",
            "Stefan Thurner"
        ],
        "published": "2025-02-25T20:25:47Z",
        "summary": "On the way towards carbon neutrality, climate stress testing provides estimates for the physical and transition risks that climate change poses to the economy and the financial system. Missing firm-level CO2 emissions data severely impedes the assessment of transition risks originating from carbon pricing. Based on the individual emissions of all Hungarian firms (410,523), as estimated from their fossil fuel purchases, we conduct a stress test of both actual and hypothetical carbon pricing policies. Using a simple 1:1 economic ABM and introducing the new carbon-to-profit ratio, we identify firms that become unprofitable and default, and estimate the respective loan write-offs. We find that 45% of all companies are directly exposed to carbon pricing. At a price of 45 EUR/t, direct economic losses of 1.3% of total sales and bank equity losses of 1.2% are expected. Secondary default cascades in supply chain networks could increase these losses by 300% to 4000%, depending on firms' ability to substitute essential inputs. To reduce transition risks, firms should reduce their dependence on essential inputs from supply chains with high CO2 exposure. We discuss the implications of different policy implementations on these transition risks.",
        "field": "Synthetic Data in Banking",
        "link": "http://arxiv.org/abs/2503.10644v1"
    },
    {
        "id": "2502.14766v2",
        "title": "Multi-Layer Deep xVA: Structural Credit Models, Measure Changes and Convergence Analysis",
        "authors": [
            "Kristoffer Andersson",
            "Alessandro Gnoatto"
        ],
        "published": "2025-02-20T17:41:55Z",
        "summary": "We propose a structural default model for portfolio-wide valuation adjustments (xVAs) and represent it as a system of coupled backward stochastic differential equations. The framework is divided into four layers, each capturing a key component: (i) clean values, (ii) initial margin and Collateral Valuation Adjustment (ColVA), (iii) Credit/Debit Valuation Adjustments (CVA/DVA) together with Margin Valuation Adjustment (MVA), and (iv) Funding Valuation Adjustment (FVA). Because these layers depend on one another through collateral and default effects, a naive Monte Carlo approach would require deeply nested simulations, making the problem computationally intractable. To address this challenge, we use an iterative deep BSDE approach, handling each layer sequentially so that earlier outputs serve as inputs to the subsequent layers. Initial margin is computed via deep quantile regression to reflect margin requirements over the Margin Period of Risk. We also adopt a change-of-measure method that highlights rare but significant defaults of the bank or counterparty, ensuring that these events are accurately captured in the training process. We further extend Han and Long's (2020) a posteriori error analysis to BSDEs on bounded domains. Due to the random exit from the domain, we obtain an order of convergence of $\\mathcal{O}(h^{1/4-\\epsilon})$ rather than the usual $\\mathcal{O}(h^{1/2})$. Numerical experiments illustrate that this method drastically reduces computational demands and successfully scales to high-dimensional, non-symmetric portfolios. The results confirm its effectiveness and accuracy, offering a practical alternative to nested Monte Carlo simulations in multi-counterparty xVA analyses.",
        "field": "Synthetic Data in Banking",
        "link": "http://arxiv.org/abs/2502.14766v2"
    },
    {
        "id": "2503.10207v1",
        "title": "Efficient Implementation of CRYSTALS-KYBER Key Encapsulation Mechanism on ESP32",
        "authors": [
            "Fabian Segatz",
            "Muhammad Ihsan Al Hafiz"
        ],
        "published": "2025-03-13T09:45:31Z",
        "summary": "Kyber, an IND-CCA2-secure lattice-based post-quantum key-encapsulation mechanism, is the winner of the first post-quantum cryptography standardization process of the US National Institute of Standards and Technology. In this work, we provide an efficient implementation of Kyber on ESP32, a very popular microcontroller for Internet of Things applications. We hand-partition the Kyber algorithm to enable utilization of the ESP32 dual-core architecture, which allows us to speed up its execution by 1.21x (keygen), 1.22x (encaps) and 1.20x (decaps). We also explore the possibility of gaining further improvement by utilizing the ESP32 SHA and AES coprocessor and achieve a culminated speed-up of 1.72x (keygen), 1.84x (encaps) and 1.69x (decaps).",
        "field": "Green Finance Technology",
        "link": "http://arxiv.org/abs/2503.10207v1"
    },
    {
        "id": "2503.10171v1",
        "title": "Verifiable, Efficient and Confidentiality-Preserving Graph Search with Transparency",
        "authors": [
            "Qiuhao Wang",
            "Xu Yang",
            "Yiwei Liu",
            "Saiyu Qi",
            "Hongguang Zhao",
            "Ke Li",
            "Yong Qi"
        ],
        "published": "2025-03-13T08:53:53Z",
        "summary": "Graph databases have garnered extensive attention and research due to their ability to manage relationships between entities efficiently. Today, many graph search services have been outsourced to a third-party server to facilitate storage and computational support. Nevertheless, the outsourcing paradigm may invade the privacy of graphs. PeGraph is the latest scheme achieving encrypted search over social graphs to address the privacy leakage, which maintains two data structures XSet and TSet motivated by the OXT technology to support encrypted conjunctive search. However, PeGraph still exhibits limitations inherent to the underlying OXT. It does not provide transparent search capabilities, suffers from expensive computation and result pattern leakages, and it fails to support search over dynamic encrypted graph database and results verification. In this paper, we propose SecGraph to address the first two limitations, which adopts a novel system architecture that leverages an SGX-enabled cloud server to provide users with secure and transparent search services since the secret key protection and computational overhead have been offloaded to the cloud server. Besides, we design an LDCF-encoded XSet based on the Logarithmic Dynamic Cuckoo Filter to facilitate efficient plaintext computation in trusted memory, effectively mitigating the risks of result pattern leakage and performance degradation due to exceeding the limited trusted memory capacity. Finally, we design a new dynamic version of TSet named Twin-TSet to enable conjunctive search over dynamic encrypted graph database. In order to support verifiable search, we further propose VSecGraph, which utilizes a procedure-oriented verification method to verify all data structures loaded into the trusted memory, thus bypassing the computational overhead associated with the client's local verification.",
        "field": "Green Finance Technology",
        "link": "http://arxiv.org/abs/2503.10171v1"
    },
    {
        "id": "2503.09666v1",
        "title": "Blockchain-Enabled Management Framework for Federated Coalition Networks",
        "authors": [
            "Jorge Ãlvaro GonzÃ¡lez",
            "Ana MarÃ­a Saiz GarcÃ­a",
            "Victor Monzon Baeza"
        ],
        "published": "2025-03-12T16:59:23Z",
        "summary": "In a globalized and interconnected world, interoperability has become a key concept for advancing tactical scenarios. Federated Coalition Networks (FCN) enable cooperation between entities from multiple nations while allowing each to maintain control over their systems. However, this interoperability necessitates the sharing of increasing amounts of information between different tactical assets, raising the need for higher security measures. Emerging technologies like blockchain drive a revolution in secure communications, paving the way for new tactical scenarios. In this work, we propose a blockchain-based framework to enhance the resilience and security of the management of these networks. We offer a guide to FCN design to help a broad audience understand the military networks in international missions by a use case and key functions applied to a proposed architecture. We evaluate its effectiveness and performance in information encryption to validate this framework.",
        "field": "Green Finance Technology",
        "link": "http://arxiv.org/abs/2503.09666v1"
    },
    {
        "id": "2503.09375v1",
        "title": "Quantum Computing and Cybersecurity Education: A Novel Curriculum for Enhancing Graduate STEM Learning",
        "authors": [
            "Suryansh Upadhyay",
            "Koustubh Phalak",
            "Jungeun Lee",
            "Kathleen Mitchell Hill",
            "Swaroop Ghosh"
        ],
        "published": "2025-03-12T13:26:54Z",
        "summary": "Quantum computing is an emerging paradigm with the potential to transform numerous application areas by addressing problems considered intractable in the classical domain. However, its integration into cyberspace introduces significant security and privacy challenges. The exponential rise in cyber attacks, further complicated by quantum capabilities, poses serious risks to financial systems and national security. The scope of quantum threats extends beyond traditional software, operating system, and network vulnerabilities, necessitating a shift in cybersecurity education. Traditional cybersecurity education, often reliant on didactic methods, lacks hands on, student centered learning experiences necessary to prepare students for these evolving challenges. There is an urgent need for curricula that address both classical and quantum security threats through experiential learning. In this work, we present the design and evaluation of EE 597: Introduction to Hardware Security, a graduate level course integrating hands-on quantum security learning with classical security concepts through simulations and cloud-based quantum hardware. Unlike conventional courses focused on quantum threats to cryptographic systems, EE 597 explores security challenges specific to quantum computing itself. We employ a mixed-methods evaluation using pre and post surveys to assess student learning outcomes and engagement. Results indicate significant improvements in students' understanding of quantum and hardware security, with strong positive feedback on course structure and remote instruction (mean scores: 3.33 to 3.83 on a 4 point scale).",
        "field": "Green Finance Technology",
        "link": "http://arxiv.org/abs/2503.09375v1"
    },
    {
        "id": "2503.09327v1",
        "title": "Heuristic-Based Address Clustering in Cardano Blockchain",
        "authors": [
            "Mostafa Chegenizadeh",
            "Sina Rafati Niya",
            "Claudio J. Tessone"
        ],
        "published": "2025-03-12T12:22:26Z",
        "summary": "Blockchain technology has recently gained widespread popularity as a practical method of storing immutable data while preserving the privacy of users by anonymizing their real identities. This anonymization approach, however, significantly complicates the analysis of blockchain data. To address this problem, heuristic-based clustering algorithms as an effective way of linking all addresses controlled by the same entity have been presented in the literature. In this paper, considering the particular features of the Extended Unspent Transaction Outputs accounting model introduced by the Cardano blockchain, two new clustering heuristics are proposed for clustering the Cardano payment addresses. Applying these heuristics and employing the UnionFind algorithm, we efficiently cluster all the addresses that have appeared on the Cardano blockchain from September 2017 to January 2023, where each cluster represents a distinct entity. The results show that each medium-sized entity in the Cardano network owns and controls 9.67 payment addresses on average. The results also confirm that a power law distribution is fitted to the distribution of entity sizes recognized using our proposed heuristics.",
        "field": "Green Finance Technology",
        "link": "http://arxiv.org/abs/2503.09327v1"
    },
    {
        "id": "2503.09823v1",
        "title": "Data Traceability for Privacy Alignment",
        "authors": [
            "Kevin Liao",
            "Shreya Thipireddy",
            "Daniel Weitzner"
        ],
        "published": "2025-03-12T20:42:23Z",
        "summary": "This paper offers a new privacy approach for the growing ecosystem of services--ranging from open banking to healthcare--dependent on sensitive personal data sharing between individuals and third-parties. While these services offer significant benefits, individuals want control over their data, transparency regarding how their data is used, and accountability from third-parties for misuse. However, existing legal and technical mechanisms are inadequate for supporting these needs. A comprehensive approach to the modern privacy challenges of accountable third-party data sharing requires a closer alignment of technical system architecture and legal institutional design. In order to achieve this privacy alignment, we extend traditional security threat modeling and analysis to encompass a broader range of privacy notions than has been typically considered. In particular, we introduce the concept of covert-accountability, which addresses adversaries that may act dishonestly but face potential identification and legal consequences. As a concrete instance of this design approach, we present the OTrace protocol, designed to provide traceable, accountable, consumer-control in third-party data sharing ecosystems. OTrace empowers consumers with the knowledge of where their data is, who has it, what it is being used for, and whom it is being shared with. By applying our alignment framework to OTrace, we demonstrate that OTrace's technical affordances can provide more confident, scalable regulatory oversight when combined with complementary legal mechanisms.",
        "field": "Digital Twin Technology in Banking",
        "link": "http://arxiv.org/abs/2503.09823v1"
    },
    {
        "id": "2503.09586v1",
        "title": "Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot",
        "authors": [
            "Andrew Crossman",
            "Andrew R. Plummer",
            "Chandra Sekharudu",
            "Deepak Warrier",
            "Mohammad Yekrangian"
        ],
        "published": "2025-03-12T17:54:18Z",
        "summary": "We present Auspex - a threat modeling system built using a specialized collection of generative artificial intelligence-based methods that capture threat modeling tradecraft. This new approach, called tradecraft prompting, centers on encoding the on-the-ground knowledge of threat modelers within the prompts that drive a generative AI-based threat modeling system. Auspex employs tradecraft prompts in two processing stages. The first stage centers on ingesting and processing system architecture information using prompts that encode threat modeling tradecraft knowledge pertaining to system decomposition and description. The second stage centers on chaining the resulting system analysis through a collection of prompts that encode tradecraft knowledge on threat identification, classification, and mitigation. The two-stage process yields a threat matrix for a system that specifies threat scenarios, threat types, information security categorizations and potential mitigations. Auspex produces formalized threat model output in minutes, relative to the weeks or months a manual process takes. More broadly, the focus on bespoke tradecraft prompting, as opposed to fine-tuning or agent-based add-ons, makes Auspex a lightweight, flexible, modular, and extensible foundational system capable of addressing the complexity, resource, and standardization limitations of both existing manual and automated threat modeling processes. In this connection, we establish the baseline value of Auspex to threat modelers through an evaluation procedure based on feedback collected from cybersecurity subject matter experts measuring the quality and utility of threat models generated by Auspex on real banking systems. We conclude with a discussion of system performance and plans for enhancements to Auspex.",
        "field": "Digital Twin Technology in Banking",
        "link": "http://arxiv.org/abs/2503.09586v1"
    },
    {
        "id": "2503.00070v1",
        "title": "Systematic Review of Cybersecurity in Banking: Evolution from Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain, Policies and Practice",
        "authors": [
            "Tue Nhi Tran"
        ],
        "published": "2025-02-27T14:17:06Z",
        "summary": "Throughout the history from pre-industry 4.0 to post-industry 4.0, cybersecurity at banks has undergone significant changes. Pre-industry 4.0 cyber security at banks relied on individual security methods that were highly manual and had low accuracy. When moving to post-industry 4.0, cybersecurity at banks had a major turning point with security methods that combined different technologies such as Artificial Intelligence (AI), Blockchain, IoT, automating necessary processes and significantly increasing the defence layer for banks. However, along with the development of new technologies, the current challenge of cybersecurity at banks lies in scalability, high costs and resources in both money and time for R&D of defence methods along with the threat of high-tech cybercriminals growing and expanding. This report goes from introducing the importance of cybersecurity at banks, analyzing their management, operational and business objectives, evaluating pre-industry 4.0 technologies used for cybersecurity at banks to assessing post-industry 4.0 technologies focusing on Artificial Intelligence and Blockchain, discussing current policies and practices and ending with discussing key advantages and challenges for 4.0 technologies and recommendations for further developing cybersecurity at banks.",
        "field": "Digital Twin Technology in Banking",
        "link": "http://arxiv.org/abs/2503.00070v1"
    },
    {
        "id": "2503.10644v1",
        "title": "Combined climate stress testing of supply-chain networks and the financial system with nation-wide firm-level emission estimates",
        "authors": [
            "Zlata TabachovÃ¡",
            "Christian Diem",
            "Johannes Stangl",
            "AndrÃ¡s Borsos",
            "Stefan Thurner"
        ],
        "published": "2025-02-25T20:25:47Z",
        "summary": "On the way towards carbon neutrality, climate stress testing provides estimates for the physical and transition risks that climate change poses to the economy and the financial system. Missing firm-level CO2 emissions data severely impedes the assessment of transition risks originating from carbon pricing. Based on the individual emissions of all Hungarian firms (410,523), as estimated from their fossil fuel purchases, we conduct a stress test of both actual and hypothetical carbon pricing policies. Using a simple 1:1 economic ABM and introducing the new carbon-to-profit ratio, we identify firms that become unprofitable and default, and estimate the respective loan write-offs. We find that 45% of all companies are directly exposed to carbon pricing. At a price of 45 EUR/t, direct economic losses of 1.3% of total sales and bank equity losses of 1.2% are expected. Secondary default cascades in supply chain networks could increase these losses by 300% to 4000%, depending on firms' ability to substitute essential inputs. To reduce transition risks, firms should reduce their dependence on essential inputs from supply chains with high CO2 exposure. We discuss the implications of different policy implementations on these transition risks.",
        "field": "Digital Twin Technology in Banking",
        "link": "http://arxiv.org/abs/2503.10644v1"
    },
    {
        "id": "2502.14766v2",
        "title": "Multi-Layer Deep xVA: Structural Credit Models, Measure Changes and Convergence Analysis",
        "authors": [
            "Kristoffer Andersson",
            "Alessandro Gnoatto"
        ],
        "published": "2025-02-20T17:41:55Z",
        "summary": "We propose a structural default model for portfolio-wide valuation adjustments (xVAs) and represent it as a system of coupled backward stochastic differential equations. The framework is divided into four layers, each capturing a key component: (i) clean values, (ii) initial margin and Collateral Valuation Adjustment (ColVA), (iii) Credit/Debit Valuation Adjustments (CVA/DVA) together with Margin Valuation Adjustment (MVA), and (iv) Funding Valuation Adjustment (FVA). Because these layers depend on one another through collateral and default effects, a naive Monte Carlo approach would require deeply nested simulations, making the problem computationally intractable. To address this challenge, we use an iterative deep BSDE approach, handling each layer sequentially so that earlier outputs serve as inputs to the subsequent layers. Initial margin is computed via deep quantile regression to reflect margin requirements over the Margin Period of Risk. We also adopt a change-of-measure method that highlights rare but significant defaults of the bank or counterparty, ensuring that these events are accurately captured in the training process. We further extend Han and Long's (2020) a posteriori error analysis to BSDEs on bounded domains. Due to the random exit from the domain, we obtain an order of convergence of $\\mathcal{O}(h^{1/4-\\epsilon})$ rather than the usual $\\mathcal{O}(h^{1/2})$. Numerical experiments illustrate that this method drastically reduces computational demands and successfully scales to high-dimensional, non-symmetric portfolios. The results confirm its effectiveness and accuracy, offering a practical alternative to nested Monte Carlo simulations in multi-counterparty xVA analyses.",
        "field": "Digital Twin Technology in Banking",
        "link": "http://arxiv.org/abs/2502.14766v2"
    },
    {
        "id": "2503.09823v1",
        "title": "Data Traceability for Privacy Alignment",
        "authors": [
            "Kevin Liao",
            "Shreya Thipireddy",
            "Daniel Weitzner"
        ],
        "published": "2025-03-12T20:42:23Z",
        "summary": "This paper offers a new privacy approach for the growing ecosystem of services--ranging from open banking to healthcare--dependent on sensitive personal data sharing between individuals and third-parties. While these services offer significant benefits, individuals want control over their data, transparency regarding how their data is used, and accountability from third-parties for misuse. However, existing legal and technical mechanisms are inadequate for supporting these needs. A comprehensive approach to the modern privacy challenges of accountable third-party data sharing requires a closer alignment of technical system architecture and legal institutional design. In order to achieve this privacy alignment, we extend traditional security threat modeling and analysis to encompass a broader range of privacy notions than has been typically considered. In particular, we introduce the concept of covert-accountability, which addresses adversaries that may act dishonestly but face potential identification and legal consequences. As a concrete instance of this design approach, we present the OTrace protocol, designed to provide traceable, accountable, consumer-control in third-party data sharing ecosystems. OTrace empowers consumers with the knowledge of where their data is, who has it, what it is being used for, and whom it is being shared with. By applying our alignment framework to OTrace, we demonstrate that OTrace's technical affordances can provide more confident, scalable regulatory oversight when combined with complementary legal mechanisms.",
        "field": "Quantum Machine Learning in Banking",
        "link": "http://arxiv.org/abs/2503.09823v1"
    },
    {
        "id": "2503.09586v1",
        "title": "Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot",
        "authors": [
            "Andrew Crossman",
            "Andrew R. Plummer",
            "Chandra Sekharudu",
            "Deepak Warrier",
            "Mohammad Yekrangian"
        ],
        "published": "2025-03-12T17:54:18Z",
        "summary": "We present Auspex - a threat modeling system built using a specialized collection of generative artificial intelligence-based methods that capture threat modeling tradecraft. This new approach, called tradecraft prompting, centers on encoding the on-the-ground knowledge of threat modelers within the prompts that drive a generative AI-based threat modeling system. Auspex employs tradecraft prompts in two processing stages. The first stage centers on ingesting and processing system architecture information using prompts that encode threat modeling tradecraft knowledge pertaining to system decomposition and description. The second stage centers on chaining the resulting system analysis through a collection of prompts that encode tradecraft knowledge on threat identification, classification, and mitigation. The two-stage process yields a threat matrix for a system that specifies threat scenarios, threat types, information security categorizations and potential mitigations. Auspex produces formalized threat model output in minutes, relative to the weeks or months a manual process takes. More broadly, the focus on bespoke tradecraft prompting, as opposed to fine-tuning or agent-based add-ons, makes Auspex a lightweight, flexible, modular, and extensible foundational system capable of addressing the complexity, resource, and standardization limitations of both existing manual and automated threat modeling processes. In this connection, we establish the baseline value of Auspex to threat modelers through an evaluation procedure based on feedback collected from cybersecurity subject matter experts measuring the quality and utility of threat models generated by Auspex on real banking systems. We conclude with a discussion of system performance and plans for enhancements to Auspex.",
        "field": "Quantum Machine Learning in Banking",
        "link": "http://arxiv.org/abs/2503.09586v1"
    },
    {
        "id": "2503.00070v1",
        "title": "Systematic Review of Cybersecurity in Banking: Evolution from Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain, Policies and Practice",
        "authors": [
            "Tue Nhi Tran"
        ],
        "published": "2025-02-27T14:17:06Z",
        "summary": "Throughout the history from pre-industry 4.0 to post-industry 4.0, cybersecurity at banks has undergone significant changes. Pre-industry 4.0 cyber security at banks relied on individual security methods that were highly manual and had low accuracy. When moving to post-industry 4.0, cybersecurity at banks had a major turning point with security methods that combined different technologies such as Artificial Intelligence (AI), Blockchain, IoT, automating necessary processes and significantly increasing the defence layer for banks. However, along with the development of new technologies, the current challenge of cybersecurity at banks lies in scalability, high costs and resources in both money and time for R&D of defence methods along with the threat of high-tech cybercriminals growing and expanding. This report goes from introducing the importance of cybersecurity at banks, analyzing their management, operational and business objectives, evaluating pre-industry 4.0 technologies used for cybersecurity at banks to assessing post-industry 4.0 technologies focusing on Artificial Intelligence and Blockchain, discussing current policies and practices and ending with discussing key advantages and challenges for 4.0 technologies and recommendations for further developing cybersecurity at banks.",
        "field": "Quantum Machine Learning in Banking",
        "link": "http://arxiv.org/abs/2503.00070v1"
    },
    {
        "id": "2503.10644v1",
        "title": "Combined climate stress testing of supply-chain networks and the financial system with nation-wide firm-level emission estimates",
        "authors": [
            "Zlata TabachovÃ¡",
            "Christian Diem",
            "Johannes Stangl",
            "AndrÃ¡s Borsos",
            "Stefan Thurner"
        ],
        "published": "2025-02-25T20:25:47Z",
        "summary": "On the way towards carbon neutrality, climate stress testing provides estimates for the physical and transition risks that climate change poses to the economy and the financial system. Missing firm-level CO2 emissions data severely impedes the assessment of transition risks originating from carbon pricing. Based on the individual emissions of all Hungarian firms (410,523), as estimated from their fossil fuel purchases, we conduct a stress test of both actual and hypothetical carbon pricing policies. Using a simple 1:1 economic ABM and introducing the new carbon-to-profit ratio, we identify firms that become unprofitable and default, and estimate the respective loan write-offs. We find that 45% of all companies are directly exposed to carbon pricing. At a price of 45 EUR/t, direct economic losses of 1.3% of total sales and bank equity losses of 1.2% are expected. Secondary default cascades in supply chain networks could increase these losses by 300% to 4000%, depending on firms' ability to substitute essential inputs. To reduce transition risks, firms should reduce their dependence on essential inputs from supply chains with high CO2 exposure. We discuss the implications of different policy implementations on these transition risks.",
        "field": "Quantum Machine Learning in Banking",
        "link": "http://arxiv.org/abs/2503.10644v1"
    },
    {
        "id": "2502.14766v2",
        "title": "Multi-Layer Deep xVA: Structural Credit Models, Measure Changes and Convergence Analysis",
        "authors": [
            "Kristoffer Andersson",
            "Alessandro Gnoatto"
        ],
        "published": "2025-02-20T17:41:55Z",
        "summary": "We propose a structural default model for portfolio-wide valuation adjustments (xVAs) and represent it as a system of coupled backward stochastic differential equations. The framework is divided into four layers, each capturing a key component: (i) clean values, (ii) initial margin and Collateral Valuation Adjustment (ColVA), (iii) Credit/Debit Valuation Adjustments (CVA/DVA) together with Margin Valuation Adjustment (MVA), and (iv) Funding Valuation Adjustment (FVA). Because these layers depend on one another through collateral and default effects, a naive Monte Carlo approach would require deeply nested simulations, making the problem computationally intractable. To address this challenge, we use an iterative deep BSDE approach, handling each layer sequentially so that earlier outputs serve as inputs to the subsequent layers. Initial margin is computed via deep quantile regression to reflect margin requirements over the Margin Period of Risk. We also adopt a change-of-measure method that highlights rare but significant defaults of the bank or counterparty, ensuring that these events are accurately captured in the training process. We further extend Han and Long's (2020) a posteriori error analysis to BSDEs on bounded domains. Due to the random exit from the domain, we obtain an order of convergence of $\\mathcal{O}(h^{1/4-\\epsilon})$ rather than the usual $\\mathcal{O}(h^{1/2})$. Numerical experiments illustrate that this method drastically reduces computational demands and successfully scales to high-dimensional, non-symmetric portfolios. The results confirm its effectiveness and accuracy, offering a practical alternative to nested Monte Carlo simulations in multi-counterparty xVA analyses.",
        "field": "Quantum Machine Learning in Banking",
        "link": "http://arxiv.org/abs/2502.14766v2"
    },
    {
        "id": "2503.09823v1",
        "title": "Data Traceability for Privacy Alignment",
        "authors": [
            "Kevin Liao",
            "Shreya Thipireddy",
            "Daniel Weitzner"
        ],
        "published": "2025-03-12T20:42:23Z",
        "summary": "This paper offers a new privacy approach for the growing ecosystem of services--ranging from open banking to healthcare--dependent on sensitive personal data sharing between individuals and third-parties. While these services offer significant benefits, individuals want control over their data, transparency regarding how their data is used, and accountability from third-parties for misuse. However, existing legal and technical mechanisms are inadequate for supporting these needs. A comprehensive approach to the modern privacy challenges of accountable third-party data sharing requires a closer alignment of technical system architecture and legal institutional design. In order to achieve this privacy alignment, we extend traditional security threat modeling and analysis to encompass a broader range of privacy notions than has been typically considered. In particular, we introduce the concept of covert-accountability, which addresses adversaries that may act dishonestly but face potential identification and legal consequences. As a concrete instance of this design approach, we present the OTrace protocol, designed to provide traceable, accountable, consumer-control in third-party data sharing ecosystems. OTrace empowers consumers with the knowledge of where their data is, who has it, what it is being used for, and whom it is being shared with. By applying our alignment framework to OTrace, we demonstrate that OTrace's technical affordances can provide more confident, scalable regulatory oversight when combined with complementary legal mechanisms.",
        "field": "Augmented Reality (AR) in Banking",
        "link": "http://arxiv.org/abs/2503.09823v1"
    },
    {
        "id": "2503.09586v1",
        "title": "Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot",
        "authors": [
            "Andrew Crossman",
            "Andrew R. Plummer",
            "Chandra Sekharudu",
            "Deepak Warrier",
            "Mohammad Yekrangian"
        ],
        "published": "2025-03-12T17:54:18Z",
        "summary": "We present Auspex - a threat modeling system built using a specialized collection of generative artificial intelligence-based methods that capture threat modeling tradecraft. This new approach, called tradecraft prompting, centers on encoding the on-the-ground knowledge of threat modelers within the prompts that drive a generative AI-based threat modeling system. Auspex employs tradecraft prompts in two processing stages. The first stage centers on ingesting and processing system architecture information using prompts that encode threat modeling tradecraft knowledge pertaining to system decomposition and description. The second stage centers on chaining the resulting system analysis through a collection of prompts that encode tradecraft knowledge on threat identification, classification, and mitigation. The two-stage process yields a threat matrix for a system that specifies threat scenarios, threat types, information security categorizations and potential mitigations. Auspex produces formalized threat model output in minutes, relative to the weeks or months a manual process takes. More broadly, the focus on bespoke tradecraft prompting, as opposed to fine-tuning or agent-based add-ons, makes Auspex a lightweight, flexible, modular, and extensible foundational system capable of addressing the complexity, resource, and standardization limitations of both existing manual and automated threat modeling processes. In this connection, we establish the baseline value of Auspex to threat modelers through an evaluation procedure based on feedback collected from cybersecurity subject matter experts measuring the quality and utility of threat models generated by Auspex on real banking systems. We conclude with a discussion of system performance and plans for enhancements to Auspex.",
        "field": "Augmented Reality (AR) in Banking",
        "link": "http://arxiv.org/abs/2503.09586v1"
    },
    {
        "id": "2503.00070v1",
        "title": "Systematic Review of Cybersecurity in Banking: Evolution from Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain, Policies and Practice",
        "authors": [
            "Tue Nhi Tran"
        ],
        "published": "2025-02-27T14:17:06Z",
        "summary": "Throughout the history from pre-industry 4.0 to post-industry 4.0, cybersecurity at banks has undergone significant changes. Pre-industry 4.0 cyber security at banks relied on individual security methods that were highly manual and had low accuracy. When moving to post-industry 4.0, cybersecurity at banks had a major turning point with security methods that combined different technologies such as Artificial Intelligence (AI), Blockchain, IoT, automating necessary processes and significantly increasing the defence layer for banks. However, along with the development of new technologies, the current challenge of cybersecurity at banks lies in scalability, high costs and resources in both money and time for R&D of defence methods along with the threat of high-tech cybercriminals growing and expanding. This report goes from introducing the importance of cybersecurity at banks, analyzing their management, operational and business objectives, evaluating pre-industry 4.0 technologies used for cybersecurity at banks to assessing post-industry 4.0 technologies focusing on Artificial Intelligence and Blockchain, discussing current policies and practices and ending with discussing key advantages and challenges for 4.0 technologies and recommendations for further developing cybersecurity at banks.",
        "field": "Augmented Reality (AR) in Banking",
        "link": "http://arxiv.org/abs/2503.00070v1"
    },
    {
        "id": "2503.10644v1",
        "title": "Combined climate stress testing of supply-chain networks and the financial system with nation-wide firm-level emission estimates",
        "authors": [
            "Zlata TabachovÃ¡",
            "Christian Diem",
            "Johannes Stangl",
            "AndrÃ¡s Borsos",
            "Stefan Thurner"
        ],
        "published": "2025-02-25T20:25:47Z",
        "summary": "On the way towards carbon neutrality, climate stress testing provides estimates for the physical and transition risks that climate change poses to the economy and the financial system. Missing firm-level CO2 emissions data severely impedes the assessment of transition risks originating from carbon pricing. Based on the individual emissions of all Hungarian firms (410,523), as estimated from their fossil fuel purchases, we conduct a stress test of both actual and hypothetical carbon pricing policies. Using a simple 1:1 economic ABM and introducing the new carbon-to-profit ratio, we identify firms that become unprofitable and default, and estimate the respective loan write-offs. We find that 45% of all companies are directly exposed to carbon pricing. At a price of 45 EUR/t, direct economic losses of 1.3% of total sales and bank equity losses of 1.2% are expected. Secondary default cascades in supply chain networks could increase these losses by 300% to 4000%, depending on firms' ability to substitute essential inputs. To reduce transition risks, firms should reduce their dependence on essential inputs from supply chains with high CO2 exposure. We discuss the implications of different policy implementations on these transition risks.",
        "field": "Augmented Reality (AR) in Banking",
        "link": "http://arxiv.org/abs/2503.10644v1"
    },
    {
        "id": "2502.14766v2",
        "title": "Multi-Layer Deep xVA: Structural Credit Models, Measure Changes and Convergence Analysis",
        "authors": [
            "Kristoffer Andersson",
            "Alessandro Gnoatto"
        ],
        "published": "2025-02-20T17:41:55Z",
        "summary": "We propose a structural default model for portfolio-wide valuation adjustments (xVAs) and represent it as a system of coupled backward stochastic differential equations. The framework is divided into four layers, each capturing a key component: (i) clean values, (ii) initial margin and Collateral Valuation Adjustment (ColVA), (iii) Credit/Debit Valuation Adjustments (CVA/DVA) together with Margin Valuation Adjustment (MVA), and (iv) Funding Valuation Adjustment (FVA). Because these layers depend on one another through collateral and default effects, a naive Monte Carlo approach would require deeply nested simulations, making the problem computationally intractable. To address this challenge, we use an iterative deep BSDE approach, handling each layer sequentially so that earlier outputs serve as inputs to the subsequent layers. Initial margin is computed via deep quantile regression to reflect margin requirements over the Margin Period of Risk. We also adopt a change-of-measure method that highlights rare but significant defaults of the bank or counterparty, ensuring that these events are accurately captured in the training process. We further extend Han and Long's (2020) a posteriori error analysis to BSDEs on bounded domains. Due to the random exit from the domain, we obtain an order of convergence of $\\mathcal{O}(h^{1/4-\\epsilon})$ rather than the usual $\\mathcal{O}(h^{1/2})$. Numerical experiments illustrate that this method drastically reduces computational demands and successfully scales to high-dimensional, non-symmetric portfolios. The results confirm its effectiveness and accuracy, offering a practical alternative to nested Monte Carlo simulations in multi-counterparty xVA analyses.",
        "field": "Augmented Reality (AR) in Banking",
        "link": "http://arxiv.org/abs/2502.14766v2"
    },
    {
        "id": "2503.09823v1",
        "title": "Data Traceability for Privacy Alignment",
        "authors": [
            "Kevin Liao",
            "Shreya Thipireddy",
            "Daniel Weitzner"
        ],
        "published": "2025-03-12T20:42:23Z",
        "summary": "This paper offers a new privacy approach for the growing ecosystem of services--ranging from open banking to healthcare--dependent on sensitive personal data sharing between individuals and third-parties. While these services offer significant benefits, individuals want control over their data, transparency regarding how their data is used, and accountability from third-parties for misuse. However, existing legal and technical mechanisms are inadequate for supporting these needs. A comprehensive approach to the modern privacy challenges of accountable third-party data sharing requires a closer alignment of technical system architecture and legal institutional design. In order to achieve this privacy alignment, we extend traditional security threat modeling and analysis to encompass a broader range of privacy notions than has been typically considered. In particular, we introduce the concept of covert-accountability, which addresses adversaries that may act dishonestly but face potential identification and legal consequences. As a concrete instance of this design approach, we present the OTrace protocol, designed to provide traceable, accountable, consumer-control in third-party data sharing ecosystems. OTrace empowers consumers with the knowledge of where their data is, who has it, what it is being used for, and whom it is being shared with. By applying our alignment framework to OTrace, we demonstrate that OTrace's technical affordances can provide more confident, scalable regulatory oversight when combined with complementary legal mechanisms.",
        "field": "Federated Learning in Banking",
        "link": "http://arxiv.org/abs/2503.09823v1"
    },
    {
        "id": "2503.09586v1",
        "title": "Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot",
        "authors": [
            "Andrew Crossman",
            "Andrew R. Plummer",
            "Chandra Sekharudu",
            "Deepak Warrier",
            "Mohammad Yekrangian"
        ],
        "published": "2025-03-12T17:54:18Z",
        "summary": "We present Auspex - a threat modeling system built using a specialized collection of generative artificial intelligence-based methods that capture threat modeling tradecraft. This new approach, called tradecraft prompting, centers on encoding the on-the-ground knowledge of threat modelers within the prompts that drive a generative AI-based threat modeling system. Auspex employs tradecraft prompts in two processing stages. The first stage centers on ingesting and processing system architecture information using prompts that encode threat modeling tradecraft knowledge pertaining to system decomposition and description. The second stage centers on chaining the resulting system analysis through a collection of prompts that encode tradecraft knowledge on threat identification, classification, and mitigation. The two-stage process yields a threat matrix for a system that specifies threat scenarios, threat types, information security categorizations and potential mitigations. Auspex produces formalized threat model output in minutes, relative to the weeks or months a manual process takes. More broadly, the focus on bespoke tradecraft prompting, as opposed to fine-tuning or agent-based add-ons, makes Auspex a lightweight, flexible, modular, and extensible foundational system capable of addressing the complexity, resource, and standardization limitations of both existing manual and automated threat modeling processes. In this connection, we establish the baseline value of Auspex to threat modelers through an evaluation procedure based on feedback collected from cybersecurity subject matter experts measuring the quality and utility of threat models generated by Auspex on real banking systems. We conclude with a discussion of system performance and plans for enhancements to Auspex.",
        "field": "Federated Learning in Banking",
        "link": "http://arxiv.org/abs/2503.09586v1"
    },
    {
        "id": "2503.00070v1",
        "title": "Systematic Review of Cybersecurity in Banking: Evolution from Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain, Policies and Practice",
        "authors": [
            "Tue Nhi Tran"
        ],
        "published": "2025-02-27T14:17:06Z",
        "summary": "Throughout the history from pre-industry 4.0 to post-industry 4.0, cybersecurity at banks has undergone significant changes. Pre-industry 4.0 cyber security at banks relied on individual security methods that were highly manual and had low accuracy. When moving to post-industry 4.0, cybersecurity at banks had a major turning point with security methods that combined different technologies such as Artificial Intelligence (AI), Blockchain, IoT, automating necessary processes and significantly increasing the defence layer for banks. However, along with the development of new technologies, the current challenge of cybersecurity at banks lies in scalability, high costs and resources in both money and time for R&D of defence methods along with the threat of high-tech cybercriminals growing and expanding. This report goes from introducing the importance of cybersecurity at banks, analyzing their management, operational and business objectives, evaluating pre-industry 4.0 technologies used for cybersecurity at banks to assessing post-industry 4.0 technologies focusing on Artificial Intelligence and Blockchain, discussing current policies and practices and ending with discussing key advantages and challenges for 4.0 technologies and recommendations for further developing cybersecurity at banks.",
        "field": "Federated Learning in Banking",
        "link": "http://arxiv.org/abs/2503.00070v1"
    },
    {
        "id": "2503.10644v1",
        "title": "Combined climate stress testing of supply-chain networks and the financial system with nation-wide firm-level emission estimates",
        "authors": [
            "Zlata TabachovÃ¡",
            "Christian Diem",
            "Johannes Stangl",
            "AndrÃ¡s Borsos",
            "Stefan Thurner"
        ],
        "published": "2025-02-25T20:25:47Z",
        "summary": "On the way towards carbon neutrality, climate stress testing provides estimates for the physical and transition risks that climate change poses to the economy and the financial system. Missing firm-level CO2 emissions data severely impedes the assessment of transition risks originating from carbon pricing. Based on the individual emissions of all Hungarian firms (410,523), as estimated from their fossil fuel purchases, we conduct a stress test of both actual and hypothetical carbon pricing policies. Using a simple 1:1 economic ABM and introducing the new carbon-to-profit ratio, we identify firms that become unprofitable and default, and estimate the respective loan write-offs. We find that 45% of all companies are directly exposed to carbon pricing. At a price of 45 EUR/t, direct economic losses of 1.3% of total sales and bank equity losses of 1.2% are expected. Secondary default cascades in supply chain networks could increase these losses by 300% to 4000%, depending on firms' ability to substitute essential inputs. To reduce transition risks, firms should reduce their dependence on essential inputs from supply chains with high CO2 exposure. We discuss the implications of different policy implementations on these transition risks.",
        "field": "Federated Learning in Banking",
        "link": "http://arxiv.org/abs/2503.10644v1"
    },
    {
        "id": "2502.14766v2",
        "title": "Multi-Layer Deep xVA: Structural Credit Models, Measure Changes and Convergence Analysis",
        "authors": [
            "Kristoffer Andersson",
            "Alessandro Gnoatto"
        ],
        "published": "2025-02-20T17:41:55Z",
        "summary": "We propose a structural default model for portfolio-wide valuation adjustments (xVAs) and represent it as a system of coupled backward stochastic differential equations. The framework is divided into four layers, each capturing a key component: (i) clean values, (ii) initial margin and Collateral Valuation Adjustment (ColVA), (iii) Credit/Debit Valuation Adjustments (CVA/DVA) together with Margin Valuation Adjustment (MVA), and (iv) Funding Valuation Adjustment (FVA). Because these layers depend on one another through collateral and default effects, a naive Monte Carlo approach would require deeply nested simulations, making the problem computationally intractable. To address this challenge, we use an iterative deep BSDE approach, handling each layer sequentially so that earlier outputs serve as inputs to the subsequent layers. Initial margin is computed via deep quantile regression to reflect margin requirements over the Margin Period of Risk. We also adopt a change-of-measure method that highlights rare but significant defaults of the bank or counterparty, ensuring that these events are accurately captured in the training process. We further extend Han and Long's (2020) a posteriori error analysis to BSDEs on bounded domains. Due to the random exit from the domain, we obtain an order of convergence of $\\mathcal{O}(h^{1/4-\\epsilon})$ rather than the usual $\\mathcal{O}(h^{1/2})$. Numerical experiments illustrate that this method drastically reduces computational demands and successfully scales to high-dimensional, non-symmetric portfolios. The results confirm its effectiveness and accuracy, offering a practical alternative to nested Monte Carlo simulations in multi-counterparty xVA analyses.",
        "field": "Federated Learning in Banking",
        "link": "http://arxiv.org/abs/2502.14766v2"
    },
    {
        "id": "1603.00991v2",
        "title": "Financial Services, Economic Growth and Well-Being: A Four-Pronged Study",
        "authors": [
            "Ravi Kashyap"
        ],
        "published": "2016-03-03T06:35:43Z",
        "summary": "A four-pronged approach to dealing with Social Science Phenomenon is outlined. This methodology is applied to Financial Services, Economic Growth and Well-Being. The four prongs are like the four directions for an army general looking for victory. Just like the four directions, we need to be aware that there is a degree of interconnectedness in the below four prongs. -Uncertainty Principle of the Social Sciences -Responsibilities of Fiscal Janitors -Need for Smaller Organizations -Redirecting Growth that Generates Garbage The importance of gaining a more profound comprehension of welfare and delineating its components into those that result from an increase in goods and services, and hence can be attributed to economic growth, and into those that are not related to economic growth but lead to a better quality of life, is highlighted. The reasoning being that economic growth alone is an inadequate indicator of well-being. Hand in hand with a better understanding of the characteristics of welfare, comes the need to consider the metrics we currently have that gauge economic growth and supplement those with measures that capture well-being more holistically.",
        "field": "Digital Therapeutics in Financial Well-being",
        "link": "http://arxiv.org/abs/1603.00991v2"
    },
    {
        "id": "2503.09823v1",
        "title": "Data Traceability for Privacy Alignment",
        "authors": [
            "Kevin Liao",
            "Shreya Thipireddy",
            "Daniel Weitzner"
        ],
        "published": "2025-03-12T20:42:23Z",
        "summary": "This paper offers a new privacy approach for the growing ecosystem of services--ranging from open banking to healthcare--dependent on sensitive personal data sharing between individuals and third-parties. While these services offer significant benefits, individuals want control over their data, transparency regarding how their data is used, and accountability from third-parties for misuse. However, existing legal and technical mechanisms are inadequate for supporting these needs. A comprehensive approach to the modern privacy challenges of accountable third-party data sharing requires a closer alignment of technical system architecture and legal institutional design. In order to achieve this privacy alignment, we extend traditional security threat modeling and analysis to encompass a broader range of privacy notions than has been typically considered. In particular, we introduce the concept of covert-accountability, which addresses adversaries that may act dishonestly but face potential identification and legal consequences. As a concrete instance of this design approach, we present the OTrace protocol, designed to provide traceable, accountable, consumer-control in third-party data sharing ecosystems. OTrace empowers consumers with the knowledge of where their data is, who has it, what it is being used for, and whom it is being shared with. By applying our alignment framework to OTrace, we demonstrate that OTrace's technical affordances can provide more confident, scalable regulatory oversight when combined with complementary legal mechanisms.",
        "field": "Blockchain Interoperability in Banking",
        "link": "http://arxiv.org/abs/2503.09823v1"
    },
    {
        "id": "2503.09586v1",
        "title": "Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot",
        "authors": [
            "Andrew Crossman",
            "Andrew R. Plummer",
            "Chandra Sekharudu",
            "Deepak Warrier",
            "Mohammad Yekrangian"
        ],
        "published": "2025-03-12T17:54:18Z",
        "summary": "We present Auspex - a threat modeling system built using a specialized collection of generative artificial intelligence-based methods that capture threat modeling tradecraft. This new approach, called tradecraft prompting, centers on encoding the on-the-ground knowledge of threat modelers within the prompts that drive a generative AI-based threat modeling system. Auspex employs tradecraft prompts in two processing stages. The first stage centers on ingesting and processing system architecture information using prompts that encode threat modeling tradecraft knowledge pertaining to system decomposition and description. The second stage centers on chaining the resulting system analysis through a collection of prompts that encode tradecraft knowledge on threat identification, classification, and mitigation. The two-stage process yields a threat matrix for a system that specifies threat scenarios, threat types, information security categorizations and potential mitigations. Auspex produces formalized threat model output in minutes, relative to the weeks or months a manual process takes. More broadly, the focus on bespoke tradecraft prompting, as opposed to fine-tuning or agent-based add-ons, makes Auspex a lightweight, flexible, modular, and extensible foundational system capable of addressing the complexity, resource, and standardization limitations of both existing manual and automated threat modeling processes. In this connection, we establish the baseline value of Auspex to threat modelers through an evaluation procedure based on feedback collected from cybersecurity subject matter experts measuring the quality and utility of threat models generated by Auspex on real banking systems. We conclude with a discussion of system performance and plans for enhancements to Auspex.",
        "field": "Blockchain Interoperability in Banking",
        "link": "http://arxiv.org/abs/2503.09586v1"
    },
    {
        "id": "2503.00070v1",
        "title": "Systematic Review of Cybersecurity in Banking: Evolution from Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain, Policies and Practice",
        "authors": [
            "Tue Nhi Tran"
        ],
        "published": "2025-02-27T14:17:06Z",
        "summary": "Throughout the history from pre-industry 4.0 to post-industry 4.0, cybersecurity at banks has undergone significant changes. Pre-industry 4.0 cyber security at banks relied on individual security methods that were highly manual and had low accuracy. When moving to post-industry 4.0, cybersecurity at banks had a major turning point with security methods that combined different technologies such as Artificial Intelligence (AI), Blockchain, IoT, automating necessary processes and significantly increasing the defence layer for banks. However, along with the development of new technologies, the current challenge of cybersecurity at banks lies in scalability, high costs and resources in both money and time for R&D of defence methods along with the threat of high-tech cybercriminals growing and expanding. This report goes from introducing the importance of cybersecurity at banks, analyzing their management, operational and business objectives, evaluating pre-industry 4.0 technologies used for cybersecurity at banks to assessing post-industry 4.0 technologies focusing on Artificial Intelligence and Blockchain, discussing current policies and practices and ending with discussing key advantages and challenges for 4.0 technologies and recommendations for further developing cybersecurity at banks.",
        "field": "Blockchain Interoperability in Banking",
        "link": "http://arxiv.org/abs/2503.00070v1"
    },
    {
        "id": "2503.10644v1",
        "title": "Combined climate stress testing of supply-chain networks and the financial system with nation-wide firm-level emission estimates",
        "authors": [
            "Zlata TabachovÃ¡",
            "Christian Diem",
            "Johannes Stangl",
            "AndrÃ¡s Borsos",
            "Stefan Thurner"
        ],
        "published": "2025-02-25T20:25:47Z",
        "summary": "On the way towards carbon neutrality, climate stress testing provides estimates for the physical and transition risks that climate change poses to the economy and the financial system. Missing firm-level CO2 emissions data severely impedes the assessment of transition risks originating from carbon pricing. Based on the individual emissions of all Hungarian firms (410,523), as estimated from their fossil fuel purchases, we conduct a stress test of both actual and hypothetical carbon pricing policies. Using a simple 1:1 economic ABM and introducing the new carbon-to-profit ratio, we identify firms that become unprofitable and default, and estimate the respective loan write-offs. We find that 45% of all companies are directly exposed to carbon pricing. At a price of 45 EUR/t, direct economic losses of 1.3% of total sales and bank equity losses of 1.2% are expected. Secondary default cascades in supply chain networks could increase these losses by 300% to 4000%, depending on firms' ability to substitute essential inputs. To reduce transition risks, firms should reduce their dependence on essential inputs from supply chains with high CO2 exposure. We discuss the implications of different policy implementations on these transition risks.",
        "field": "Blockchain Interoperability in Banking",
        "link": "http://arxiv.org/abs/2503.10644v1"
    },
    {
        "id": "2502.14766v2",
        "title": "Multi-Layer Deep xVA: Structural Credit Models, Measure Changes and Convergence Analysis",
        "authors": [
            "Kristoffer Andersson",
            "Alessandro Gnoatto"
        ],
        "published": "2025-02-20T17:41:55Z",
        "summary": "We propose a structural default model for portfolio-wide valuation adjustments (xVAs) and represent it as a system of coupled backward stochastic differential equations. The framework is divided into four layers, each capturing a key component: (i) clean values, (ii) initial margin and Collateral Valuation Adjustment (ColVA), (iii) Credit/Debit Valuation Adjustments (CVA/DVA) together with Margin Valuation Adjustment (MVA), and (iv) Funding Valuation Adjustment (FVA). Because these layers depend on one another through collateral and default effects, a naive Monte Carlo approach would require deeply nested simulations, making the problem computationally intractable. To address this challenge, we use an iterative deep BSDE approach, handling each layer sequentially so that earlier outputs serve as inputs to the subsequent layers. Initial margin is computed via deep quantile regression to reflect margin requirements over the Margin Period of Risk. We also adopt a change-of-measure method that highlights rare but significant defaults of the bank or counterparty, ensuring that these events are accurately captured in the training process. We further extend Han and Long's (2020) a posteriori error analysis to BSDEs on bounded domains. Due to the random exit from the domain, we obtain an order of convergence of $\\mathcal{O}(h^{1/4-\\epsilon})$ rather than the usual $\\mathcal{O}(h^{1/2})$. Numerical experiments illustrate that this method drastically reduces computational demands and successfully scales to high-dimensional, non-symmetric portfolios. The results confirm its effectiveness and accuracy, offering a practical alternative to nested Monte Carlo simulations in multi-counterparty xVA analyses.",
        "field": "Blockchain Interoperability in Banking",
        "link": "http://arxiv.org/abs/2502.14766v2"
    },
    {
        "id": "2503.09823v1",
        "title": "Data Traceability for Privacy Alignment",
        "authors": [
            "Kevin Liao",
            "Shreya Thipireddy",
            "Daniel Weitzner"
        ],
        "published": "2025-03-12T20:42:23Z",
        "summary": "This paper offers a new privacy approach for the growing ecosystem of services--ranging from open banking to healthcare--dependent on sensitive personal data sharing between individuals and third-parties. While these services offer significant benefits, individuals want control over their data, transparency regarding how their data is used, and accountability from third-parties for misuse. However, existing legal and technical mechanisms are inadequate for supporting these needs. A comprehensive approach to the modern privacy challenges of accountable third-party data sharing requires a closer alignment of technical system architecture and legal institutional design. In order to achieve this privacy alignment, we extend traditional security threat modeling and analysis to encompass a broader range of privacy notions than has been typically considered. In particular, we introduce the concept of covert-accountability, which addresses adversaries that may act dishonestly but face potential identification and legal consequences. As a concrete instance of this design approach, we present the OTrace protocol, designed to provide traceable, accountable, consumer-control in third-party data sharing ecosystems. OTrace empowers consumers with the knowledge of where their data is, who has it, what it is being used for, and whom it is being shared with. By applying our alignment framework to OTrace, we demonstrate that OTrace's technical affordances can provide more confident, scalable regulatory oversight when combined with complementary legal mechanisms.",
        "field": "Metaverse Banking",
        "link": "http://arxiv.org/abs/2503.09823v1"
    },
    {
        "id": "2503.09586v1",
        "title": "Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot",
        "authors": [
            "Andrew Crossman",
            "Andrew R. Plummer",
            "Chandra Sekharudu",
            "Deepak Warrier",
            "Mohammad Yekrangian"
        ],
        "published": "2025-03-12T17:54:18Z",
        "summary": "We present Auspex - a threat modeling system built using a specialized collection of generative artificial intelligence-based methods that capture threat modeling tradecraft. This new approach, called tradecraft prompting, centers on encoding the on-the-ground knowledge of threat modelers within the prompts that drive a generative AI-based threat modeling system. Auspex employs tradecraft prompts in two processing stages. The first stage centers on ingesting and processing system architecture information using prompts that encode threat modeling tradecraft knowledge pertaining to system decomposition and description. The second stage centers on chaining the resulting system analysis through a collection of prompts that encode tradecraft knowledge on threat identification, classification, and mitigation. The two-stage process yields a threat matrix for a system that specifies threat scenarios, threat types, information security categorizations and potential mitigations. Auspex produces formalized threat model output in minutes, relative to the weeks or months a manual process takes. More broadly, the focus on bespoke tradecraft prompting, as opposed to fine-tuning or agent-based add-ons, makes Auspex a lightweight, flexible, modular, and extensible foundational system capable of addressing the complexity, resource, and standardization limitations of both existing manual and automated threat modeling processes. In this connection, we establish the baseline value of Auspex to threat modelers through an evaluation procedure based on feedback collected from cybersecurity subject matter experts measuring the quality and utility of threat models generated by Auspex on real banking systems. We conclude with a discussion of system performance and plans for enhancements to Auspex.",
        "field": "Metaverse Banking",
        "link": "http://arxiv.org/abs/2503.09586v1"
    },
    {
        "id": "2503.00070v1",
        "title": "Systematic Review of Cybersecurity in Banking: Evolution from Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain, Policies and Practice",
        "authors": [
            "Tue Nhi Tran"
        ],
        "published": "2025-02-27T14:17:06Z",
        "summary": "Throughout the history from pre-industry 4.0 to post-industry 4.0, cybersecurity at banks has undergone significant changes. Pre-industry 4.0 cyber security at banks relied on individual security methods that were highly manual and had low accuracy. When moving to post-industry 4.0, cybersecurity at banks had a major turning point with security methods that combined different technologies such as Artificial Intelligence (AI), Blockchain, IoT, automating necessary processes and significantly increasing the defence layer for banks. However, along with the development of new technologies, the current challenge of cybersecurity at banks lies in scalability, high costs and resources in both money and time for R&D of defence methods along with the threat of high-tech cybercriminals growing and expanding. This report goes from introducing the importance of cybersecurity at banks, analyzing their management, operational and business objectives, evaluating pre-industry 4.0 technologies used for cybersecurity at banks to assessing post-industry 4.0 technologies focusing on Artificial Intelligence and Blockchain, discussing current policies and practices and ending with discussing key advantages and challenges for 4.0 technologies and recommendations for further developing cybersecurity at banks.",
        "field": "Metaverse Banking",
        "link": "http://arxiv.org/abs/2503.00070v1"
    },
    {
        "id": "2503.10644v1",
        "title": "Combined climate stress testing of supply-chain networks and the financial system with nation-wide firm-level emission estimates",
        "authors": [
            "Zlata TabachovÃ¡",
            "Christian Diem",
            "Johannes Stangl",
            "AndrÃ¡s Borsos",
            "Stefan Thurner"
        ],
        "published": "2025-02-25T20:25:47Z",
        "summary": "On the way towards carbon neutrality, climate stress testing provides estimates for the physical and transition risks that climate change poses to the economy and the financial system. Missing firm-level CO2 emissions data severely impedes the assessment of transition risks originating from carbon pricing. Based on the individual emissions of all Hungarian firms (410,523), as estimated from their fossil fuel purchases, we conduct a stress test of both actual and hypothetical carbon pricing policies. Using a simple 1:1 economic ABM and introducing the new carbon-to-profit ratio, we identify firms that become unprofitable and default, and estimate the respective loan write-offs. We find that 45% of all companies are directly exposed to carbon pricing. At a price of 45 EUR/t, direct economic losses of 1.3% of total sales and bank equity losses of 1.2% are expected. Secondary default cascades in supply chain networks could increase these losses by 300% to 4000%, depending on firms' ability to substitute essential inputs. To reduce transition risks, firms should reduce their dependence on essential inputs from supply chains with high CO2 exposure. We discuss the implications of different policy implementations on these transition risks.",
        "field": "Metaverse Banking",
        "link": "http://arxiv.org/abs/2503.10644v1"
    },
    {
        "id": "2502.14766v2",
        "title": "Multi-Layer Deep xVA: Structural Credit Models, Measure Changes and Convergence Analysis",
        "authors": [
            "Kristoffer Andersson",
            "Alessandro Gnoatto"
        ],
        "published": "2025-02-20T17:41:55Z",
        "summary": "We propose a structural default model for portfolio-wide valuation adjustments (xVAs) and represent it as a system of coupled backward stochastic differential equations. The framework is divided into four layers, each capturing a key component: (i) clean values, (ii) initial margin and Collateral Valuation Adjustment (ColVA), (iii) Credit/Debit Valuation Adjustments (CVA/DVA) together with Margin Valuation Adjustment (MVA), and (iv) Funding Valuation Adjustment (FVA). Because these layers depend on one another through collateral and default effects, a naive Monte Carlo approach would require deeply nested simulations, making the problem computationally intractable. To address this challenge, we use an iterative deep BSDE approach, handling each layer sequentially so that earlier outputs serve as inputs to the subsequent layers. Initial margin is computed via deep quantile regression to reflect margin requirements over the Margin Period of Risk. We also adopt a change-of-measure method that highlights rare but significant defaults of the bank or counterparty, ensuring that these events are accurately captured in the training process. We further extend Han and Long's (2020) a posteriori error analysis to BSDEs on bounded domains. Due to the random exit from the domain, we obtain an order of convergence of $\\mathcal{O}(h^{1/4-\\epsilon})$ rather than the usual $\\mathcal{O}(h^{1/2})$. Numerical experiments illustrate that this method drastically reduces computational demands and successfully scales to high-dimensional, non-symmetric portfolios. The results confirm its effectiveness and accuracy, offering a practical alternative to nested Monte Carlo simulations in multi-counterparty xVA analyses.",
        "field": "Metaverse Banking",
        "link": "http://arxiv.org/abs/2502.14766v2"
    },
    {
        "id": "2503.09823v1",
        "title": "Data Traceability for Privacy Alignment",
        "authors": [
            "Kevin Liao",
            "Shreya Thipireddy",
            "Daniel Weitzner"
        ],
        "published": "2025-03-12T20:42:23Z",
        "summary": "This paper offers a new privacy approach for the growing ecosystem of services--ranging from open banking to healthcare--dependent on sensitive personal data sharing between individuals and third-parties. While these services offer significant benefits, individuals want control over their data, transparency regarding how their data is used, and accountability from third-parties for misuse. However, existing legal and technical mechanisms are inadequate for supporting these needs. A comprehensive approach to the modern privacy challenges of accountable third-party data sharing requires a closer alignment of technical system architecture and legal institutional design. In order to achieve this privacy alignment, we extend traditional security threat modeling and analysis to encompass a broader range of privacy notions than has been typically considered. In particular, we introduce the concept of covert-accountability, which addresses adversaries that may act dishonestly but face potential identification and legal consequences. As a concrete instance of this design approach, we present the OTrace protocol, designed to provide traceable, accountable, consumer-control in third-party data sharing ecosystems. OTrace empowers consumers with the knowledge of where their data is, who has it, what it is being used for, and whom it is being shared with. By applying our alignment framework to OTrace, we demonstrate that OTrace's technical affordances can provide more confident, scalable regulatory oversight when combined with complementary legal mechanisms.",
        "field": "Quantum Cryptography in Banking",
        "link": "http://arxiv.org/abs/2503.09823v1"
    },
    {
        "id": "2503.09586v1",
        "title": "Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot",
        "authors": [
            "Andrew Crossman",
            "Andrew R. Plummer",
            "Chandra Sekharudu",
            "Deepak Warrier",
            "Mohammad Yekrangian"
        ],
        "published": "2025-03-12T17:54:18Z",
        "summary": "We present Auspex - a threat modeling system built using a specialized collection of generative artificial intelligence-based methods that capture threat modeling tradecraft. This new approach, called tradecraft prompting, centers on encoding the on-the-ground knowledge of threat modelers within the prompts that drive a generative AI-based threat modeling system. Auspex employs tradecraft prompts in two processing stages. The first stage centers on ingesting and processing system architecture information using prompts that encode threat modeling tradecraft knowledge pertaining to system decomposition and description. The second stage centers on chaining the resulting system analysis through a collection of prompts that encode tradecraft knowledge on threat identification, classification, and mitigation. The two-stage process yields a threat matrix for a system that specifies threat scenarios, threat types, information security categorizations and potential mitigations. Auspex produces formalized threat model output in minutes, relative to the weeks or months a manual process takes. More broadly, the focus on bespoke tradecraft prompting, as opposed to fine-tuning or agent-based add-ons, makes Auspex a lightweight, flexible, modular, and extensible foundational system capable of addressing the complexity, resource, and standardization limitations of both existing manual and automated threat modeling processes. In this connection, we establish the baseline value of Auspex to threat modelers through an evaluation procedure based on feedback collected from cybersecurity subject matter experts measuring the quality and utility of threat models generated by Auspex on real banking systems. We conclude with a discussion of system performance and plans for enhancements to Auspex.",
        "field": "Quantum Cryptography in Banking",
        "link": "http://arxiv.org/abs/2503.09586v1"
    },
    {
        "id": "2503.00070v1",
        "title": "Systematic Review of Cybersecurity in Banking: Evolution from Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain, Policies and Practice",
        "authors": [
            "Tue Nhi Tran"
        ],
        "published": "2025-02-27T14:17:06Z",
        "summary": "Throughout the history from pre-industry 4.0 to post-industry 4.0, cybersecurity at banks has undergone significant changes. Pre-industry 4.0 cyber security at banks relied on individual security methods that were highly manual and had low accuracy. When moving to post-industry 4.0, cybersecurity at banks had a major turning point with security methods that combined different technologies such as Artificial Intelligence (AI), Blockchain, IoT, automating necessary processes and significantly increasing the defence layer for banks. However, along with the development of new technologies, the current challenge of cybersecurity at banks lies in scalability, high costs and resources in both money and time for R&D of defence methods along with the threat of high-tech cybercriminals growing and expanding. This report goes from introducing the importance of cybersecurity at banks, analyzing their management, operational and business objectives, evaluating pre-industry 4.0 technologies used for cybersecurity at banks to assessing post-industry 4.0 technologies focusing on Artificial Intelligence and Blockchain, discussing current policies and practices and ending with discussing key advantages and challenges for 4.0 technologies and recommendations for further developing cybersecurity at banks.",
        "field": "Quantum Cryptography in Banking",
        "link": "http://arxiv.org/abs/2503.00070v1"
    },
    {
        "id": "2503.10644v1",
        "title": "Combined climate stress testing of supply-chain networks and the financial system with nation-wide firm-level emission estimates",
        "authors": [
            "Zlata TabachovÃ¡",
            "Christian Diem",
            "Johannes Stangl",
            "AndrÃ¡s Borsos",
            "Stefan Thurner"
        ],
        "published": "2025-02-25T20:25:47Z",
        "summary": "On the way towards carbon neutrality, climate stress testing provides estimates for the physical and transition risks that climate change poses to the economy and the financial system. Missing firm-level CO2 emissions data severely impedes the assessment of transition risks originating from carbon pricing. Based on the individual emissions of all Hungarian firms (410,523), as estimated from their fossil fuel purchases, we conduct a stress test of both actual and hypothetical carbon pricing policies. Using a simple 1:1 economic ABM and introducing the new carbon-to-profit ratio, we identify firms that become unprofitable and default, and estimate the respective loan write-offs. We find that 45% of all companies are directly exposed to carbon pricing. At a price of 45 EUR/t, direct economic losses of 1.3% of total sales and bank equity losses of 1.2% are expected. Secondary default cascades in supply chain networks could increase these losses by 300% to 4000%, depending on firms' ability to substitute essential inputs. To reduce transition risks, firms should reduce their dependence on essential inputs from supply chains with high CO2 exposure. We discuss the implications of different policy implementations on these transition risks.",
        "field": "Quantum Cryptography in Banking",
        "link": "http://arxiv.org/abs/2503.10644v1"
    },
    {
        "id": "2502.14766v2",
        "title": "Multi-Layer Deep xVA: Structural Credit Models, Measure Changes and Convergence Analysis",
        "authors": [
            "Kristoffer Andersson",
            "Alessandro Gnoatto"
        ],
        "published": "2025-02-20T17:41:55Z",
        "summary": "We propose a structural default model for portfolio-wide valuation adjustments (xVAs) and represent it as a system of coupled backward stochastic differential equations. The framework is divided into four layers, each capturing a key component: (i) clean values, (ii) initial margin and Collateral Valuation Adjustment (ColVA), (iii) Credit/Debit Valuation Adjustments (CVA/DVA) together with Margin Valuation Adjustment (MVA), and (iv) Funding Valuation Adjustment (FVA). Because these layers depend on one another through collateral and default effects, a naive Monte Carlo approach would require deeply nested simulations, making the problem computationally intractable. To address this challenge, we use an iterative deep BSDE approach, handling each layer sequentially so that earlier outputs serve as inputs to the subsequent layers. Initial margin is computed via deep quantile regression to reflect margin requirements over the Margin Period of Risk. We also adopt a change-of-measure method that highlights rare but significant defaults of the bank or counterparty, ensuring that these events are accurately captured in the training process. We further extend Han and Long's (2020) a posteriori error analysis to BSDEs on bounded domains. Due to the random exit from the domain, we obtain an order of convergence of $\\mathcal{O}(h^{1/4-\\epsilon})$ rather than the usual $\\mathcal{O}(h^{1/2})$. Numerical experiments illustrate that this method drastically reduces computational demands and successfully scales to high-dimensional, non-symmetric portfolios. The results confirm its effectiveness and accuracy, offering a practical alternative to nested Monte Carlo simulations in multi-counterparty xVA analyses.",
        "field": "Quantum Cryptography in Banking",
        "link": "http://arxiv.org/abs/2502.14766v2"
    },
    {
        "id": "2503.11216v1",
        "title": "Cross-Platform Benchmarking of the FHE Libraries: Novel Insights into SEAL and Openfhe",
        "authors": [
            "Faneela",
            "Jawad Ahmad",
            "Baraq Ghaleb",
            "Sana Ullah Jan",
            "William J. Buchanan"
        ],
        "published": "2025-03-14T09:08:30Z",
        "summary": "The rapid growth of cloud computing and data-driven applications has amplified privacy concerns, driven by the increasing demand to process sensitive data securely. Homomorphic encryption (HE) has become a vital solution for addressing these concerns by enabling computations on encrypted data without revealing its contents. This paper provides a comprehensive evaluation of two leading HE libraries, SEAL and OpenFHE, examining their performance, usability, and support for prominent HE schemes such as BGV and CKKS. Our analysis highlights computational efficiency, memory usage, and scalability across Linux and Windows platforms, emphasizing their applicability in real-world scenarios. Results reveal that Linux outperforms Windows in computation efficiency, with OpenFHE emerging as the optimal choice across diverse cryptographic settings. This paper provides valuable insights for researchers and practitioners to advance privacy-preserving applications using FHE.",
        "field": "Digital Asset Custody Solutions",
        "link": "http://arxiv.org/abs/2503.11216v1"
    },
    {
        "id": "2503.10058v1",
        "title": "Deep Learning Approaches for Anti-Money Laundering on Mobile Transactions: Review, Framework, and Directions",
        "authors": [
            "Jiani Fan",
            "Lwin Khin Shar",
            "Ruichen Zhang",
            "Ziyao Liu",
            "Wenzhuo Yang",
            "Dusit Niyato",
            "Bomin Mao",
            "Kwok-Yan Lam"
        ],
        "published": "2025-03-13T05:19:44Z",
        "summary": "Money laundering is a financial crime that obscures the origin of illicit funds, necessitating the development and enforcement of anti-money laundering (AML) policies by governments and organizations. The proliferation of mobile payment platforms and smart IoT devices has significantly complicated AML investigations. As payment networks become more interconnected, there is an increasing need for efficient real-time detection to process large volumes of transaction data on heterogeneous payment systems by different operators such as digital currencies, cryptocurrencies and account-based payments. Most of these mobile payment networks are supported by connected devices, many of which are considered loT devices in the FinTech space that constantly generate data. Furthermore, the growing complexity and unpredictability of transaction patterns across these networks contribute to a higher incidence of false positives. While machine learning solutions have the potential to enhance detection efficiency, their application in AML faces unique challenges, such as addressing privacy concerns tied to sensitive financial data and managing the real-world constraint of limited data availability due to data regulations. Existing surveys in the AML literature broadly review machine learning approaches for money laundering detection, but they often lack an in-depth exploration of advanced deep learning techniques - an emerging field with significant potential. To address this gap, this paper conducts a comprehensive review of deep learning solutions and the challenges associated with their use in AML. Additionally, we propose a novel framework that applies the least-privilege principle by integrating machine learning techniques, codifying AML red flags, and employing account profiling to provide context for predictions and enable effective fraud detection under limited data availability....",
        "field": "Digital Asset Custody Solutions",
        "link": "http://arxiv.org/abs/2503.10058v1"
    },
    {
        "id": "2503.09934v1",
        "title": "A Pharmacy Benefit Manager Insurance Business Model",
        "authors": [
            "Lawrence W. Abrams"
        ],
        "published": "2025-03-13T01:13:16Z",
        "summary": "It is time to move on from attempts to make the pharmacy benefit manager (PBM) reseller business model more transparent. Time and time again the Big 3 PBMs have developed opaque alternatives to piece-meal 100% pass-through mandates. Time and time again PBMs have demonstrated expertise in finding loopholes in state government disclosure laws. The purpose of this paper is to provide quantitative estimates of two transparent insurance business models as a solution to the PBM agency issue. The key parameter used is an 8% gross profit margin figure disclosed by the Big 3 PBMs themselves. Based on reported drug trend delivered to plans, we use a $1,200 to $1,500 per member per year (PMPY) as the range for this key performance indicator (KPI). We propose that discussions of PBM insurance business models start with the following figures: (1) a fixed premium model with medical loss ratio ranging from 92% to 85%; (2) a fee-for-service model ranging from $96 to $180 PMPY with risk sharing of deviations from a contracted PMPY delivered drug spend.",
        "field": "Digital Asset Custody Solutions",
        "link": "http://arxiv.org/abs/2503.09934v1"
    },
    {
        "id": "2503.09433v1",
        "title": "CASTLE: Benchmarking Dataset for Static Code Analyzers and LLMs towards CWE Detection",
        "authors": [
            "Richard A. Dubniczky",
            "Krisztofer ZoltÃ¡n HorvÃ¡t",
            "TamÃ¡s Bisztray",
            "Mohamed Amine Ferrag",
            "Lucas C. Cordeiro",
            "Norbert Tihanyi"
        ],
        "published": "2025-03-12T14:30:05Z",
        "summary": "Identifying vulnerabilities in source code is crucial, especially in critical software components. Existing methods such as static analysis, dynamic analysis, formal verification, and recently Large Language Models are widely used to detect security flaws. This paper introduces CASTLE (CWE Automated Security Testing and Low-Level Evaluation), a benchmarking framework for evaluating the vulnerability detection capabilities of different methods. We assess 13 static analysis tools, 10 LLMs, and 2 formal verification tools using a hand-crafted dataset of 250 micro-benchmark programs covering 25 common CWEs. We propose the CASTLE Score, a novel evaluation metric to ensure fair comparison. Our results reveal key differences: ESBMC (a formal verification tool) minimizes false positives but struggles with vulnerabilities beyond model checking, such as weak cryptography or SQL injection. Static analyzers suffer from high false positives, increasing manual validation efforts for developers. LLMs perform exceptionally well in the CASTLE dataset when identifying vulnerabilities in small code snippets. However, their accuracy declines, and hallucinations increase as the code size grows. These results suggest that LLMs could play a pivotal role in future security solutions, particularly within code completion frameworks, where they can provide real-time guidance to prevent vulnerabilities. The dataset is accessible at https://github.com/CASTLE-Benchmark.",
        "field": "Digital Asset Custody Solutions",
        "link": "http://arxiv.org/abs/2503.09433v1"
    },
    {
        "id": "2503.09317v1",
        "title": "RaceTEE: A Practical Privacy-Preserving Off-Chain Smart Contract Execution Architecture",
        "authors": [
            "Keyu Zhang",
            "Andrew Martin"
        ],
        "published": "2025-03-12T12:10:02Z",
        "summary": "Decentralized on-chain smart contracts enable trustless collaboration, yet their inherent data transparency and execution overhead hinder widespread adoption. Existing cryptographic approaches incur high computational costs and lack generality. Meanwhile, prior TEE-based solutions suffer from practical limitations, such as the inability to support inter-contract interactions, reliance on unbreakable TEEs, and compromised usability. We introduce RaceTEE, a practical and privacy-preserving off-chain execution architecture for smart contracts that leverages Trusted Execution Environments (TEEs). RaceTEE decouples transaction ordering (on-chain) from execution (off-chain), with computations performed competitively in TEEs, ensuring confidentiality and minimizing overhead. It further enhances practicality through three key improvements: supporting secure inter-contract interactions, providing a key rotation scheme that enforces forward and backward secrecy even in the event of TEE breaches, and enabling full compatibility with existing blockchains without altering the user interaction model. To validate its feasibility, we prototype RaceTEE using Intel SGX and Ethereum, demonstrating its applicability across various use cases and evaluating its performance.",
        "field": "Digital Asset Custody Solutions",
        "link": "http://arxiv.org/abs/2503.09317v1"
    },
    {
        "id": "2503.09823v1",
        "title": "Data Traceability for Privacy Alignment",
        "authors": [
            "Kevin Liao",
            "Shreya Thipireddy",
            "Daniel Weitzner"
        ],
        "published": "2025-03-12T20:42:23Z",
        "summary": "This paper offers a new privacy approach for the growing ecosystem of services--ranging from open banking to healthcare--dependent on sensitive personal data sharing between individuals and third-parties. While these services offer significant benefits, individuals want control over their data, transparency regarding how their data is used, and accountability from third-parties for misuse. However, existing legal and technical mechanisms are inadequate for supporting these needs. A comprehensive approach to the modern privacy challenges of accountable third-party data sharing requires a closer alignment of technical system architecture and legal institutional design. In order to achieve this privacy alignment, we extend traditional security threat modeling and analysis to encompass a broader range of privacy notions than has been typically considered. In particular, we introduce the concept of covert-accountability, which addresses adversaries that may act dishonestly but face potential identification and legal consequences. As a concrete instance of this design approach, we present the OTrace protocol, designed to provide traceable, accountable, consumer-control in third-party data sharing ecosystems. OTrace empowers consumers with the knowledge of where their data is, who has it, what it is being used for, and whom it is being shared with. By applying our alignment framework to OTrace, we demonstrate that OTrace's technical affordances can provide more confident, scalable regulatory oversight when combined with complementary legal mechanisms.",
        "field": "RegTech in Banking",
        "link": "http://arxiv.org/abs/2503.09823v1"
    },
    {
        "id": "2503.09586v1",
        "title": "Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot",
        "authors": [
            "Andrew Crossman",
            "Andrew R. Plummer",
            "Chandra Sekharudu",
            "Deepak Warrier",
            "Mohammad Yekrangian"
        ],
        "published": "2025-03-12T17:54:18Z",
        "summary": "We present Auspex - a threat modeling system built using a specialized collection of generative artificial intelligence-based methods that capture threat modeling tradecraft. This new approach, called tradecraft prompting, centers on encoding the on-the-ground knowledge of threat modelers within the prompts that drive a generative AI-based threat modeling system. Auspex employs tradecraft prompts in two processing stages. The first stage centers on ingesting and processing system architecture information using prompts that encode threat modeling tradecraft knowledge pertaining to system decomposition and description. The second stage centers on chaining the resulting system analysis through a collection of prompts that encode tradecraft knowledge on threat identification, classification, and mitigation. The two-stage process yields a threat matrix for a system that specifies threat scenarios, threat types, information security categorizations and potential mitigations. Auspex produces formalized threat model output in minutes, relative to the weeks or months a manual process takes. More broadly, the focus on bespoke tradecraft prompting, as opposed to fine-tuning or agent-based add-ons, makes Auspex a lightweight, flexible, modular, and extensible foundational system capable of addressing the complexity, resource, and standardization limitations of both existing manual and automated threat modeling processes. In this connection, we establish the baseline value of Auspex to threat modelers through an evaluation procedure based on feedback collected from cybersecurity subject matter experts measuring the quality and utility of threat models generated by Auspex on real banking systems. We conclude with a discussion of system performance and plans for enhancements to Auspex.",
        "field": "RegTech in Banking",
        "link": "http://arxiv.org/abs/2503.09586v1"
    },
    {
        "id": "2503.00070v1",
        "title": "Systematic Review of Cybersecurity in Banking: Evolution from Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain, Policies and Practice",
        "authors": [
            "Tue Nhi Tran"
        ],
        "published": "2025-02-27T14:17:06Z",
        "summary": "Throughout the history from pre-industry 4.0 to post-industry 4.0, cybersecurity at banks has undergone significant changes. Pre-industry 4.0 cyber security at banks relied on individual security methods that were highly manual and had low accuracy. When moving to post-industry 4.0, cybersecurity at banks had a major turning point with security methods that combined different technologies such as Artificial Intelligence (AI), Blockchain, IoT, automating necessary processes and significantly increasing the defence layer for banks. However, along with the development of new technologies, the current challenge of cybersecurity at banks lies in scalability, high costs and resources in both money and time for R&D of defence methods along with the threat of high-tech cybercriminals growing and expanding. This report goes from introducing the importance of cybersecurity at banks, analyzing their management, operational and business objectives, evaluating pre-industry 4.0 technologies used for cybersecurity at banks to assessing post-industry 4.0 technologies focusing on Artificial Intelligence and Blockchain, discussing current policies and practices and ending with discussing key advantages and challenges for 4.0 technologies and recommendations for further developing cybersecurity at banks.",
        "field": "RegTech in Banking",
        "link": "http://arxiv.org/abs/2503.00070v1"
    },
    {
        "id": "2503.10644v1",
        "title": "Combined climate stress testing of supply-chain networks and the financial system with nation-wide firm-level emission estimates",
        "authors": [
            "Zlata TabachovÃ¡",
            "Christian Diem",
            "Johannes Stangl",
            "AndrÃ¡s Borsos",
            "Stefan Thurner"
        ],
        "published": "2025-02-25T20:25:47Z",
        "summary": "On the way towards carbon neutrality, climate stress testing provides estimates for the physical and transition risks that climate change poses to the economy and the financial system. Missing firm-level CO2 emissions data severely impedes the assessment of transition risks originating from carbon pricing. Based on the individual emissions of all Hungarian firms (410,523), as estimated from their fossil fuel purchases, we conduct a stress test of both actual and hypothetical carbon pricing policies. Using a simple 1:1 economic ABM and introducing the new carbon-to-profit ratio, we identify firms that become unprofitable and default, and estimate the respective loan write-offs. We find that 45% of all companies are directly exposed to carbon pricing. At a price of 45 EUR/t, direct economic losses of 1.3% of total sales and bank equity losses of 1.2% are expected. Secondary default cascades in supply chain networks could increase these losses by 300% to 4000%, depending on firms' ability to substitute essential inputs. To reduce transition risks, firms should reduce their dependence on essential inputs from supply chains with high CO2 exposure. We discuss the implications of different policy implementations on these transition risks.",
        "field": "RegTech in Banking",
        "link": "http://arxiv.org/abs/2503.10644v1"
    },
    {
        "id": "2502.14766v2",
        "title": "Multi-Layer Deep xVA: Structural Credit Models, Measure Changes and Convergence Analysis",
        "authors": [
            "Kristoffer Andersson",
            "Alessandro Gnoatto"
        ],
        "published": "2025-02-20T17:41:55Z",
        "summary": "We propose a structural default model for portfolio-wide valuation adjustments (xVAs) and represent it as a system of coupled backward stochastic differential equations. The framework is divided into four layers, each capturing a key component: (i) clean values, (ii) initial margin and Collateral Valuation Adjustment (ColVA), (iii) Credit/Debit Valuation Adjustments (CVA/DVA) together with Margin Valuation Adjustment (MVA), and (iv) Funding Valuation Adjustment (FVA). Because these layers depend on one another through collateral and default effects, a naive Monte Carlo approach would require deeply nested simulations, making the problem computationally intractable. To address this challenge, we use an iterative deep BSDE approach, handling each layer sequentially so that earlier outputs serve as inputs to the subsequent layers. Initial margin is computed via deep quantile regression to reflect margin requirements over the Margin Period of Risk. We also adopt a change-of-measure method that highlights rare but significant defaults of the bank or counterparty, ensuring that these events are accurately captured in the training process. We further extend Han and Long's (2020) a posteriori error analysis to BSDEs on bounded domains. Due to the random exit from the domain, we obtain an order of convergence of $\\mathcal{O}(h^{1/4-\\epsilon})$ rather than the usual $\\mathcal{O}(h^{1/2})$. Numerical experiments illustrate that this method drastically reduces computational demands and successfully scales to high-dimensional, non-symmetric portfolios. The results confirm its effectiveness and accuracy, offering a practical alternative to nested Monte Carlo simulations in multi-counterparty xVA analyses.",
        "field": "RegTech in Banking",
        "link": "http://arxiv.org/abs/2502.14766v2"
    },
    {
        "id": "2503.09823v1",
        "title": "Data Traceability for Privacy Alignment",
        "authors": [
            "Kevin Liao",
            "Shreya Thipireddy",
            "Daniel Weitzner"
        ],
        "published": "2025-03-12T20:42:23Z",
        "summary": "This paper offers a new privacy approach for the growing ecosystem of services--ranging from open banking to healthcare--dependent on sensitive personal data sharing between individuals and third-parties. While these services offer significant benefits, individuals want control over their data, transparency regarding how their data is used, and accountability from third-parties for misuse. However, existing legal and technical mechanisms are inadequate for supporting these needs. A comprehensive approach to the modern privacy challenges of accountable third-party data sharing requires a closer alignment of technical system architecture and legal institutional design. In order to achieve this privacy alignment, we extend traditional security threat modeling and analysis to encompass a broader range of privacy notions than has been typically considered. In particular, we introduce the concept of covert-accountability, which addresses adversaries that may act dishonestly but face potential identification and legal consequences. As a concrete instance of this design approach, we present the OTrace protocol, designed to provide traceable, accountable, consumer-control in third-party data sharing ecosystems. OTrace empowers consumers with the knowledge of where their data is, who has it, what it is being used for, and whom it is being shared with. By applying our alignment framework to OTrace, we demonstrate that OTrace's technical affordances can provide more confident, scalable regulatory oversight when combined with complementary legal mechanisms.",
        "field": "Voice Banking",
        "link": "http://arxiv.org/abs/2503.09823v1"
    },
    {
        "id": "2503.09586v1",
        "title": "Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot",
        "authors": [
            "Andrew Crossman",
            "Andrew R. Plummer",
            "Chandra Sekharudu",
            "Deepak Warrier",
            "Mohammad Yekrangian"
        ],
        "published": "2025-03-12T17:54:18Z",
        "summary": "We present Auspex - a threat modeling system built using a specialized collection of generative artificial intelligence-based methods that capture threat modeling tradecraft. This new approach, called tradecraft prompting, centers on encoding the on-the-ground knowledge of threat modelers within the prompts that drive a generative AI-based threat modeling system. Auspex employs tradecraft prompts in two processing stages. The first stage centers on ingesting and processing system architecture information using prompts that encode threat modeling tradecraft knowledge pertaining to system decomposition and description. The second stage centers on chaining the resulting system analysis through a collection of prompts that encode tradecraft knowledge on threat identification, classification, and mitigation. The two-stage process yields a threat matrix for a system that specifies threat scenarios, threat types, information security categorizations and potential mitigations. Auspex produces formalized threat model output in minutes, relative to the weeks or months a manual process takes. More broadly, the focus on bespoke tradecraft prompting, as opposed to fine-tuning or agent-based add-ons, makes Auspex a lightweight, flexible, modular, and extensible foundational system capable of addressing the complexity, resource, and standardization limitations of both existing manual and automated threat modeling processes. In this connection, we establish the baseline value of Auspex to threat modelers through an evaluation procedure based on feedback collected from cybersecurity subject matter experts measuring the quality and utility of threat models generated by Auspex on real banking systems. We conclude with a discussion of system performance and plans for enhancements to Auspex.",
        "field": "Voice Banking",
        "link": "http://arxiv.org/abs/2503.09586v1"
    },
    {
        "id": "2503.00070v1",
        "title": "Systematic Review of Cybersecurity in Banking: Evolution from Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain, Policies and Practice",
        "authors": [
            "Tue Nhi Tran"
        ],
        "published": "2025-02-27T14:17:06Z",
        "summary": "Throughout the history from pre-industry 4.0 to post-industry 4.0, cybersecurity at banks has undergone significant changes. Pre-industry 4.0 cyber security at banks relied on individual security methods that were highly manual and had low accuracy. When moving to post-industry 4.0, cybersecurity at banks had a major turning point with security methods that combined different technologies such as Artificial Intelligence (AI), Blockchain, IoT, automating necessary processes and significantly increasing the defence layer for banks. However, along with the development of new technologies, the current challenge of cybersecurity at banks lies in scalability, high costs and resources in both money and time for R&D of defence methods along with the threat of high-tech cybercriminals growing and expanding. This report goes from introducing the importance of cybersecurity at banks, analyzing their management, operational and business objectives, evaluating pre-industry 4.0 technologies used for cybersecurity at banks to assessing post-industry 4.0 technologies focusing on Artificial Intelligence and Blockchain, discussing current policies and practices and ending with discussing key advantages and challenges for 4.0 technologies and recommendations for further developing cybersecurity at banks.",
        "field": "Voice Banking",
        "link": "http://arxiv.org/abs/2503.00070v1"
    },
    {
        "id": "2503.10644v1",
        "title": "Combined climate stress testing of supply-chain networks and the financial system with nation-wide firm-level emission estimates",
        "authors": [
            "Zlata TabachovÃ¡",
            "Christian Diem",
            "Johannes Stangl",
            "AndrÃ¡s Borsos",
            "Stefan Thurner"
        ],
        "published": "2025-02-25T20:25:47Z",
        "summary": "On the way towards carbon neutrality, climate stress testing provides estimates for the physical and transition risks that climate change poses to the economy and the financial system. Missing firm-level CO2 emissions data severely impedes the assessment of transition risks originating from carbon pricing. Based on the individual emissions of all Hungarian firms (410,523), as estimated from their fossil fuel purchases, we conduct a stress test of both actual and hypothetical carbon pricing policies. Using a simple 1:1 economic ABM and introducing the new carbon-to-profit ratio, we identify firms that become unprofitable and default, and estimate the respective loan write-offs. We find that 45% of all companies are directly exposed to carbon pricing. At a price of 45 EUR/t, direct economic losses of 1.3% of total sales and bank equity losses of 1.2% are expected. Secondary default cascades in supply chain networks could increase these losses by 300% to 4000%, depending on firms' ability to substitute essential inputs. To reduce transition risks, firms should reduce their dependence on essential inputs from supply chains with high CO2 exposure. We discuss the implications of different policy implementations on these transition risks.",
        "field": "Voice Banking",
        "link": "http://arxiv.org/abs/2503.10644v1"
    },
    {
        "id": "2502.14766v2",
        "title": "Multi-Layer Deep xVA: Structural Credit Models, Measure Changes and Convergence Analysis",
        "authors": [
            "Kristoffer Andersson",
            "Alessandro Gnoatto"
        ],
        "published": "2025-02-20T17:41:55Z",
        "summary": "We propose a structural default model for portfolio-wide valuation adjustments (xVAs) and represent it as a system of coupled backward stochastic differential equations. The framework is divided into four layers, each capturing a key component: (i) clean values, (ii) initial margin and Collateral Valuation Adjustment (ColVA), (iii) Credit/Debit Valuation Adjustments (CVA/DVA) together with Margin Valuation Adjustment (MVA), and (iv) Funding Valuation Adjustment (FVA). Because these layers depend on one another through collateral and default effects, a naive Monte Carlo approach would require deeply nested simulations, making the problem computationally intractable. To address this challenge, we use an iterative deep BSDE approach, handling each layer sequentially so that earlier outputs serve as inputs to the subsequent layers. Initial margin is computed via deep quantile regression to reflect margin requirements over the Margin Period of Risk. We also adopt a change-of-measure method that highlights rare but significant defaults of the bank or counterparty, ensuring that these events are accurately captured in the training process. We further extend Han and Long's (2020) a posteriori error analysis to BSDEs on bounded domains. Due to the random exit from the domain, we obtain an order of convergence of $\\mathcal{O}(h^{1/4-\\epsilon})$ rather than the usual $\\mathcal{O}(h^{1/2})$. Numerical experiments illustrate that this method drastically reduces computational demands and successfully scales to high-dimensional, non-symmetric portfolios. The results confirm its effectiveness and accuracy, offering a practical alternative to nested Monte Carlo simulations in multi-counterparty xVA analyses.",
        "field": "Voice Banking",
        "link": "http://arxiv.org/abs/2502.14766v2"
    },
    {
        "id": "2503.10207v1",
        "title": "Efficient Implementation of CRYSTALS-KYBER Key Encapsulation Mechanism on ESP32",
        "authors": [
            "Fabian Segatz",
            "Muhammad Ihsan Al Hafiz"
        ],
        "published": "2025-03-13T09:45:31Z",
        "summary": "Kyber, an IND-CCA2-secure lattice-based post-quantum key-encapsulation mechanism, is the winner of the first post-quantum cryptography standardization process of the US National Institute of Standards and Technology. In this work, we provide an efficient implementation of Kyber on ESP32, a very popular microcontroller for Internet of Things applications. We hand-partition the Kyber algorithm to enable utilization of the ESP32 dual-core architecture, which allows us to speed up its execution by 1.21x (keygen), 1.22x (encaps) and 1.20x (decaps). We also explore the possibility of gaining further improvement by utilizing the ESP32 SHA and AES coprocessor and achieve a culminated speed-up of 1.72x (keygen), 1.84x (encaps) and 1.69x (decaps).",
        "field": "the current frontier of banking technology",
        "link": "http://arxiv.org/abs/2503.10207v1"
    },
    {
        "id": "2503.10171v1",
        "title": "Verifiable, Efficient and Confidentiality-Preserving Graph Search with Transparency",
        "authors": [
            "Qiuhao Wang",
            "Xu Yang",
            "Yiwei Liu",
            "Saiyu Qi",
            "Hongguang Zhao",
            "Ke Li",
            "Yong Qi"
        ],
        "published": "2025-03-13T08:53:53Z",
        "summary": "Graph databases have garnered extensive attention and research due to their ability to manage relationships between entities efficiently. Today, many graph search services have been outsourced to a third-party server to facilitate storage and computational support. Nevertheless, the outsourcing paradigm may invade the privacy of graphs. PeGraph is the latest scheme achieving encrypted search over social graphs to address the privacy leakage, which maintains two data structures XSet and TSet motivated by the OXT technology to support encrypted conjunctive search. However, PeGraph still exhibits limitations inherent to the underlying OXT. It does not provide transparent search capabilities, suffers from expensive computation and result pattern leakages, and it fails to support search over dynamic encrypted graph database and results verification. In this paper, we propose SecGraph to address the first two limitations, which adopts a novel system architecture that leverages an SGX-enabled cloud server to provide users with secure and transparent search services since the secret key protection and computational overhead have been offloaded to the cloud server. Besides, we design an LDCF-encoded XSet based on the Logarithmic Dynamic Cuckoo Filter to facilitate efficient plaintext computation in trusted memory, effectively mitigating the risks of result pattern leakage and performance degradation due to exceeding the limited trusted memory capacity. Finally, we design a new dynamic version of TSet named Twin-TSet to enable conjunctive search over dynamic encrypted graph database. In order to support verifiable search, we further propose VSecGraph, which utilizes a procedure-oriented verification method to verify all data structures loaded into the trusted memory, thus bypassing the computational overhead associated with the client's local verification.",
        "field": "the current frontier of banking technology",
        "link": "http://arxiv.org/abs/2503.10171v1"
    },
    {
        "id": "2503.09666v1",
        "title": "Blockchain-Enabled Management Framework for Federated Coalition Networks",
        "authors": [
            "Jorge Ãlvaro GonzÃ¡lez",
            "Ana MarÃ­a Saiz GarcÃ­a",
            "Victor Monzon Baeza"
        ],
        "published": "2025-03-12T16:59:23Z",
        "summary": "In a globalized and interconnected world, interoperability has become a key concept for advancing tactical scenarios. Federated Coalition Networks (FCN) enable cooperation between entities from multiple nations while allowing each to maintain control over their systems. However, this interoperability necessitates the sharing of increasing amounts of information between different tactical assets, raising the need for higher security measures. Emerging technologies like blockchain drive a revolution in secure communications, paving the way for new tactical scenarios. In this work, we propose a blockchain-based framework to enhance the resilience and security of the management of these networks. We offer a guide to FCN design to help a broad audience understand the military networks in international missions by a use case and key functions applied to a proposed architecture. We evaluate its effectiveness and performance in information encryption to validate this framework.",
        "field": "the current frontier of banking technology",
        "link": "http://arxiv.org/abs/2503.09666v1"
    },
    {
        "id": "2503.09375v1",
        "title": "Quantum Computing and Cybersecurity Education: A Novel Curriculum for Enhancing Graduate STEM Learning",
        "authors": [
            "Suryansh Upadhyay",
            "Koustubh Phalak",
            "Jungeun Lee",
            "Kathleen Mitchell Hill",
            "Swaroop Ghosh"
        ],
        "published": "2025-03-12T13:26:54Z",
        "summary": "Quantum computing is an emerging paradigm with the potential to transform numerous application areas by addressing problems considered intractable in the classical domain. However, its integration into cyberspace introduces significant security and privacy challenges. The exponential rise in cyber attacks, further complicated by quantum capabilities, poses serious risks to financial systems and national security. The scope of quantum threats extends beyond traditional software, operating system, and network vulnerabilities, necessitating a shift in cybersecurity education. Traditional cybersecurity education, often reliant on didactic methods, lacks hands on, student centered learning experiences necessary to prepare students for these evolving challenges. There is an urgent need for curricula that address both classical and quantum security threats through experiential learning. In this work, we present the design and evaluation of EE 597: Introduction to Hardware Security, a graduate level course integrating hands-on quantum security learning with classical security concepts through simulations and cloud-based quantum hardware. Unlike conventional courses focused on quantum threats to cryptographic systems, EE 597 explores security challenges specific to quantum computing itself. We employ a mixed-methods evaluation using pre and post surveys to assess student learning outcomes and engagement. Results indicate significant improvements in students' understanding of quantum and hardware security, with strong positive feedback on course structure and remote instruction (mean scores: 3.33 to 3.83 on a 4 point scale).",
        "field": "the current frontier of banking technology",
        "link": "http://arxiv.org/abs/2503.09375v1"
    },
    {
        "id": "2503.09327v1",
        "title": "Heuristic-Based Address Clustering in Cardano Blockchain",
        "authors": [
            "Mostafa Chegenizadeh",
            "Sina Rafati Niya",
            "Claudio J. Tessone"
        ],
        "published": "2025-03-12T12:22:26Z",
        "summary": "Blockchain technology has recently gained widespread popularity as a practical method of storing immutable data while preserving the privacy of users by anonymizing their real identities. This anonymization approach, however, significantly complicates the analysis of blockchain data. To address this problem, heuristic-based clustering algorithms as an effective way of linking all addresses controlled by the same entity have been presented in the literature. In this paper, considering the particular features of the Extended Unspent Transaction Outputs accounting model introduced by the Cardano blockchain, two new clustering heuristics are proposed for clustering the Cardano payment addresses. Applying these heuristics and employing the UnionFind algorithm, we efficiently cluster all the addresses that have appeared on the Cardano blockchain from September 2017 to January 2023, where each cluster represents a distinct entity. The results show that each medium-sized entity in the Cardano network owns and controls 9.67 payment addresses on average. The results also confirm that a power law distribution is fitted to the distribution of entity sizes recognized using our proposed heuristics.",
        "field": "the current frontier of banking technology",
        "link": "http://arxiv.org/abs/2503.09327v1"
    },
    {
        "id": "2503.10207v1",
        "title": "Efficient Implementation of CRYSTALS-KYBER Key Encapsulation Mechanism on ESP32",
        "authors": [
            "Fabian Segatz",
            "Muhammad Ihsan Al Hafiz"
        ],
        "published": "2025-03-13T09:45:31Z",
        "summary": "Kyber, an IND-CCA2-secure lattice-based post-quantum key-encapsulation mechanism, is the winner of the first post-quantum cryptography standardization process of the US National Institute of Standards and Technology. In this work, we provide an efficient implementation of Kyber on ESP32, a very popular microcontroller for Internet of Things applications. We hand-partition the Kyber algorithm to enable utilization of the ESP32 dual-core architecture, which allows us to speed up its execution by 1.21x (keygen), 1.22x (encaps) and 1.20x (decaps). We also explore the possibility of gaining further improvement by utilizing the ESP32 SHA and AES coprocessor and achieve a culminated speed-up of 1.72x (keygen), 1.84x (encaps) and 1.69x (decaps).",
        "field": "Blockchain Technology",
        "link": "http://arxiv.org/abs/2503.10207v1"
    },
    {
        "id": "2503.10171v1",
        "title": "Verifiable, Efficient and Confidentiality-Preserving Graph Search with Transparency",
        "authors": [
            "Qiuhao Wang",
            "Xu Yang",
            "Yiwei Liu",
            "Saiyu Qi",
            "Hongguang Zhao",
            "Ke Li",
            "Yong Qi"
        ],
        "published": "2025-03-13T08:53:53Z",
        "summary": "Graph databases have garnered extensive attention and research due to their ability to manage relationships between entities efficiently. Today, many graph search services have been outsourced to a third-party server to facilitate storage and computational support. Nevertheless, the outsourcing paradigm may invade the privacy of graphs. PeGraph is the latest scheme achieving encrypted search over social graphs to address the privacy leakage, which maintains two data structures XSet and TSet motivated by the OXT technology to support encrypted conjunctive search. However, PeGraph still exhibits limitations inherent to the underlying OXT. It does not provide transparent search capabilities, suffers from expensive computation and result pattern leakages, and it fails to support search over dynamic encrypted graph database and results verification. In this paper, we propose SecGraph to address the first two limitations, which adopts a novel system architecture that leverages an SGX-enabled cloud server to provide users with secure and transparent search services since the secret key protection and computational overhead have been offloaded to the cloud server. Besides, we design an LDCF-encoded XSet based on the Logarithmic Dynamic Cuckoo Filter to facilitate efficient plaintext computation in trusted memory, effectively mitigating the risks of result pattern leakage and performance degradation due to exceeding the limited trusted memory capacity. Finally, we design a new dynamic version of TSet named Twin-TSet to enable conjunctive search over dynamic encrypted graph database. In order to support verifiable search, we further propose VSecGraph, which utilizes a procedure-oriented verification method to verify all data structures loaded into the trusted memory, thus bypassing the computational overhead associated with the client's local verification.",
        "field": "Blockchain Technology",
        "link": "http://arxiv.org/abs/2503.10171v1"
    },
    {
        "id": "2503.09666v1",
        "title": "Blockchain-Enabled Management Framework for Federated Coalition Networks",
        "authors": [
            "Jorge Ãlvaro GonzÃ¡lez",
            "Ana MarÃ­a Saiz GarcÃ­a",
            "Victor Monzon Baeza"
        ],
        "published": "2025-03-12T16:59:23Z",
        "summary": "In a globalized and interconnected world, interoperability has become a key concept for advancing tactical scenarios. Federated Coalition Networks (FCN) enable cooperation between entities from multiple nations while allowing each to maintain control over their systems. However, this interoperability necessitates the sharing of increasing amounts of information between different tactical assets, raising the need for higher security measures. Emerging technologies like blockchain drive a revolution in secure communications, paving the way for new tactical scenarios. In this work, we propose a blockchain-based framework to enhance the resilience and security of the management of these networks. We offer a guide to FCN design to help a broad audience understand the military networks in international missions by a use case and key functions applied to a proposed architecture. We evaluate its effectiveness and performance in information encryption to validate this framework.",
        "field": "Blockchain Technology",
        "link": "http://arxiv.org/abs/2503.09666v1"
    },
    {
        "id": "2503.09375v1",
        "title": "Quantum Computing and Cybersecurity Education: A Novel Curriculum for Enhancing Graduate STEM Learning",
        "authors": [
            "Suryansh Upadhyay",
            "Koustubh Phalak",
            "Jungeun Lee",
            "Kathleen Mitchell Hill",
            "Swaroop Ghosh"
        ],
        "published": "2025-03-12T13:26:54Z",
        "summary": "Quantum computing is an emerging paradigm with the potential to transform numerous application areas by addressing problems considered intractable in the classical domain. However, its integration into cyberspace introduces significant security and privacy challenges. The exponential rise in cyber attacks, further complicated by quantum capabilities, poses serious risks to financial systems and national security. The scope of quantum threats extends beyond traditional software, operating system, and network vulnerabilities, necessitating a shift in cybersecurity education. Traditional cybersecurity education, often reliant on didactic methods, lacks hands on, student centered learning experiences necessary to prepare students for these evolving challenges. There is an urgent need for curricula that address both classical and quantum security threats through experiential learning. In this work, we present the design and evaluation of EE 597: Introduction to Hardware Security, a graduate level course integrating hands-on quantum security learning with classical security concepts through simulations and cloud-based quantum hardware. Unlike conventional courses focused on quantum threats to cryptographic systems, EE 597 explores security challenges specific to quantum computing itself. We employ a mixed-methods evaluation using pre and post surveys to assess student learning outcomes and engagement. Results indicate significant improvements in students' understanding of quantum and hardware security, with strong positive feedback on course structure and remote instruction (mean scores: 3.33 to 3.83 on a 4 point scale).",
        "field": "Blockchain Technology",
        "link": "http://arxiv.org/abs/2503.09375v1"
    },
    {
        "id": "2503.09327v1",
        "title": "Heuristic-Based Address Clustering in Cardano Blockchain",
        "authors": [
            "Mostafa Chegenizadeh",
            "Sina Rafati Niya",
            "Claudio J. Tessone"
        ],
        "published": "2025-03-12T12:22:26Z",
        "summary": "Blockchain technology has recently gained widespread popularity as a practical method of storing immutable data while preserving the privacy of users by anonymizing their real identities. This anonymization approach, however, significantly complicates the analysis of blockchain data. To address this problem, heuristic-based clustering algorithms as an effective way of linking all addresses controlled by the same entity have been presented in the literature. In this paper, considering the particular features of the Extended Unspent Transaction Outputs accounting model introduced by the Cardano blockchain, two new clustering heuristics are proposed for clustering the Cardano payment addresses. Applying these heuristics and employing the UnionFind algorithm, we efficiently cluster all the addresses that have appeared on the Cardano blockchain from September 2017 to January 2023, where each cluster represents a distinct entity. The results show that each medium-sized entity in the Cardano network owns and controls 9.67 payment addresses on average. The results also confirm that a power law distribution is fitted to the distribution of entity sizes recognized using our proposed heuristics.",
        "field": "Blockchain Technology",
        "link": "http://arxiv.org/abs/2503.09327v1"
    },
    {
        "id": "2503.11619v1",
        "title": "Tit-for-Tat: Safeguarding Large Vision-Language Models Against Jailbreak Attacks via Adversarial Defense",
        "authors": [
            "Shuyang Hao",
            "Yiwei Wang",
            "Bryan Hooi",
            "Ming-Hsuan Yang",
            "Jun Liu",
            "Chengcheng Tang",
            "Zi Huang",
            "Yujun Cai"
        ],
        "published": "2025-03-14T17:39:45Z",
        "summary": "Deploying large vision-language models (LVLMs) introduces a unique vulnerability: susceptibility to malicious attacks via visual inputs. However, existing defense methods suffer from two key limitations: (1) They solely focus on textual defenses, fail to directly address threats in the visual domain where attacks originate, and (2) the additional processing steps often incur significant computational overhead or compromise model performance on benign tasks. Building on these insights, we propose ESIII (Embedding Security Instructions Into Images), a novel methodology for transforming the visual space from a source of vulnerability into an active defense mechanism. Initially, we embed security instructions into defensive images through gradient-based optimization, obtaining security instructions in the visual dimension. Subsequently, we integrate security instructions from visual and textual dimensions with the input query. The collaboration between security instructions from different dimensions ensures comprehensive security protection. Extensive experiments demonstrate that our approach effectively fortifies the robustness of LVLMs against such attacks while preserving their performance on standard benign tasks and incurring an imperceptible increase in time costs.",
        "field": "Edge Computing",
        "link": "http://arxiv.org/abs/2503.11619v1"
    },
    {
        "id": "2503.11573v1",
        "title": "Synthesizing Access Control Policies using Large Language Models",
        "authors": [
            "Adarsh Vatsa",
            "Pratyush Patel",
            "William Eiers"
        ],
        "published": "2025-03-14T16:40:25Z",
        "summary": "Cloud compute systems allow administrators to write access control policies that govern access to private data. While policies are written in convenient languages, such as AWS Identity and Access Management Policy Language, manually written policies often become complex and error prone. In this paper, we investigate whether and how well Large Language Models (LLMs) can be used to synthesize access control policies. Our investigation focuses on the task of taking an access control request specification and zero-shot prompting LLMs to synthesize a well-formed access control policy which correctly adheres to the request specification. We consider two scenarios, one which the request specification is given as a concrete list of requests to be allowed or denied, and another in which a natural language description is used to specify sets of requests to be allowed or denied. We then argue that for zero-shot prompting, more precise and structured prompts using a syntax based approach are necessary and experimentally show preliminary results validating our approach.",
        "field": "Edge Computing",
        "link": "http://arxiv.org/abs/2503.11573v1"
    },
    {
        "id": "2503.11404v1",
        "title": "Towards A Correct Usage of Cryptography in Semantic Watermarks for Diffusion Models",
        "authors": [
            "Jonas Thietke",
            "Andreas MÃ¼ller",
            "Denis Lukovnikov",
            "Asja Fischer",
            "Erwin Quiring"
        ],
        "published": "2025-03-14T13:45:46Z",
        "summary": "Semantic watermarking methods enable the direct integration of watermarks into the generation process of latent diffusion models by only modifying the initial latent noise. One line of approaches building on Gaussian Shading relies on cryptographic primitives to steer the sampling process of the latent noise. However, we identify several issues in the usage of cryptographic techniques in Gaussian Shading, particularly in its proof of lossless performance and key management, causing ambiguity in follow-up works, too. In this work, we therefore revisit the cryptographic primitives for semantic watermarking. We introduce a novel, general proof of lossless performance based on IND\\$-CPA security for semantic watermarks. We then discuss the configuration of the cryptographic primitives in semantic watermarks with respect to security, efficiency, and generation quality.",
        "field": "Edge Computing",
        "link": "http://arxiv.org/abs/2503.11404v1"
    },
    {
        "id": "2503.11216v1",
        "title": "Cross-Platform Benchmarking of the FHE Libraries: Novel Insights into SEAL and Openfhe",
        "authors": [
            "Faneela",
            "Jawad Ahmad",
            "Baraq Ghaleb",
            "Sana Ullah Jan",
            "William J. Buchanan"
        ],
        "published": "2025-03-14T09:08:30Z",
        "summary": "The rapid growth of cloud computing and data-driven applications has amplified privacy concerns, driven by the increasing demand to process sensitive data securely. Homomorphic encryption (HE) has become a vital solution for addressing these concerns by enabling computations on encrypted data without revealing its contents. This paper provides a comprehensive evaluation of two leading HE libraries, SEAL and OpenFHE, examining their performance, usability, and support for prominent HE schemes such as BGV and CKKS. Our analysis highlights computational efficiency, memory usage, and scalability across Linux and Windows platforms, emphasizing their applicability in real-world scenarios. Results reveal that Linux outperforms Windows in computation efficiency, with OpenFHE emerging as the optimal choice across diverse cryptographic settings. This paper provides valuable insights for researchers and practitioners to advance privacy-preserving applications using FHE.",
        "field": "Edge Computing",
        "link": "http://arxiv.org/abs/2503.11216v1"
    },
    {
        "id": "2503.11185v1",
        "title": "Align in Depth: Defending Jailbreak Attacks via Progressive Answer Detoxification",
        "authors": [
            "Yingjie Zhang",
            "Tong Liu",
            "Zhe Zhao",
            "Guozhu Meng",
            "Kai Chen"
        ],
        "published": "2025-03-14T08:32:12Z",
        "summary": "Large Language Models (LLMs) are vulnerable to jailbreak attacks, which use crafted prompts to elicit toxic responses. These attacks exploit LLMs' difficulty in dynamically detecting harmful intents during the generation process. Traditional safety alignment methods, often relying on the initial few generation steps, are ineffective due to limited computational budget. This paper proposes DEEPALIGN, a robust defense framework that fine-tunes LLMs to progressively detoxify generated content, significantly improving both the computational budget and effectiveness of mitigating harmful generation. Our approach uses a hybrid loss function operating on hidden states to directly improve LLMs' inherent awareness of toxity during generation. Furthermore, we redefine safe responses by generating semantically relevant answers to harmful queries, thereby increasing robustness against representation-mutation attacks. Evaluations across multiple LLMs demonstrate state-of-the-art defense performance against six different attack types, reducing Attack Success Rates by up to two orders of magnitude compared to previous state-of-the-art defense while preserving utility. This work advances LLM safety by addressing limitations of conventional alignment through dynamic, context-aware mitigation.",
        "field": "Edge Computing",
        "link": "http://arxiv.org/abs/2503.11185v1"
    },
    {
        "id": "2503.10207v1",
        "title": "Efficient Implementation of CRYSTALS-KYBER Key Encapsulation Mechanism on ESP32",
        "authors": [
            "Fabian Segatz",
            "Muhammad Ihsan Al Hafiz"
        ],
        "published": "2025-03-13T09:45:31Z",
        "summary": "Kyber, an IND-CCA2-secure lattice-based post-quantum key-encapsulation mechanism, is the winner of the first post-quantum cryptography standardization process of the US National Institute of Standards and Technology. In this work, we provide an efficient implementation of Kyber on ESP32, a very popular microcontroller for Internet of Things applications. We hand-partition the Kyber algorithm to enable utilization of the ESP32 dual-core architecture, which allows us to speed up its execution by 1.21x (keygen), 1.22x (encaps) and 1.20x (decaps). We also explore the possibility of gaining further improvement by utilizing the ESP32 SHA and AES coprocessor and achieve a culminated speed-up of 1.72x (keygen), 1.84x (encaps) and 1.69x (decaps).",
        "field": "5G Technology",
        "link": "http://arxiv.org/abs/2503.10207v1"
    },
    {
        "id": "2503.10171v1",
        "title": "Verifiable, Efficient and Confidentiality-Preserving Graph Search with Transparency",
        "authors": [
            "Qiuhao Wang",
            "Xu Yang",
            "Yiwei Liu",
            "Saiyu Qi",
            "Hongguang Zhao",
            "Ke Li",
            "Yong Qi"
        ],
        "published": "2025-03-13T08:53:53Z",
        "summary": "Graph databases have garnered extensive attention and research due to their ability to manage relationships between entities efficiently. Today, many graph search services have been outsourced to a third-party server to facilitate storage and computational support. Nevertheless, the outsourcing paradigm may invade the privacy of graphs. PeGraph is the latest scheme achieving encrypted search over social graphs to address the privacy leakage, which maintains two data structures XSet and TSet motivated by the OXT technology to support encrypted conjunctive search. However, PeGraph still exhibits limitations inherent to the underlying OXT. It does not provide transparent search capabilities, suffers from expensive computation and result pattern leakages, and it fails to support search over dynamic encrypted graph database and results verification. In this paper, we propose SecGraph to address the first two limitations, which adopts a novel system architecture that leverages an SGX-enabled cloud server to provide users with secure and transparent search services since the secret key protection and computational overhead have been offloaded to the cloud server. Besides, we design an LDCF-encoded XSet based on the Logarithmic Dynamic Cuckoo Filter to facilitate efficient plaintext computation in trusted memory, effectively mitigating the risks of result pattern leakage and performance degradation due to exceeding the limited trusted memory capacity. Finally, we design a new dynamic version of TSet named Twin-TSet to enable conjunctive search over dynamic encrypted graph database. In order to support verifiable search, we further propose VSecGraph, which utilizes a procedure-oriented verification method to verify all data structures loaded into the trusted memory, thus bypassing the computational overhead associated with the client's local verification.",
        "field": "5G Technology",
        "link": "http://arxiv.org/abs/2503.10171v1"
    },
    {
        "id": "2503.09666v1",
        "title": "Blockchain-Enabled Management Framework for Federated Coalition Networks",
        "authors": [
            "Jorge Ãlvaro GonzÃ¡lez",
            "Ana MarÃ­a Saiz GarcÃ­a",
            "Victor Monzon Baeza"
        ],
        "published": "2025-03-12T16:59:23Z",
        "summary": "In a globalized and interconnected world, interoperability has become a key concept for advancing tactical scenarios. Federated Coalition Networks (FCN) enable cooperation between entities from multiple nations while allowing each to maintain control over their systems. However, this interoperability necessitates the sharing of increasing amounts of information between different tactical assets, raising the need for higher security measures. Emerging technologies like blockchain drive a revolution in secure communications, paving the way for new tactical scenarios. In this work, we propose a blockchain-based framework to enhance the resilience and security of the management of these networks. We offer a guide to FCN design to help a broad audience understand the military networks in international missions by a use case and key functions applied to a proposed architecture. We evaluate its effectiveness and performance in information encryption to validate this framework.",
        "field": "5G Technology",
        "link": "http://arxiv.org/abs/2503.09666v1"
    },
    {
        "id": "2503.09375v1",
        "title": "Quantum Computing and Cybersecurity Education: A Novel Curriculum for Enhancing Graduate STEM Learning",
        "authors": [
            "Suryansh Upadhyay",
            "Koustubh Phalak",
            "Jungeun Lee",
            "Kathleen Mitchell Hill",
            "Swaroop Ghosh"
        ],
        "published": "2025-03-12T13:26:54Z",
        "summary": "Quantum computing is an emerging paradigm with the potential to transform numerous application areas by addressing problems considered intractable in the classical domain. However, its integration into cyberspace introduces significant security and privacy challenges. The exponential rise in cyber attacks, further complicated by quantum capabilities, poses serious risks to financial systems and national security. The scope of quantum threats extends beyond traditional software, operating system, and network vulnerabilities, necessitating a shift in cybersecurity education. Traditional cybersecurity education, often reliant on didactic methods, lacks hands on, student centered learning experiences necessary to prepare students for these evolving challenges. There is an urgent need for curricula that address both classical and quantum security threats through experiential learning. In this work, we present the design and evaluation of EE 597: Introduction to Hardware Security, a graduate level course integrating hands-on quantum security learning with classical security concepts through simulations and cloud-based quantum hardware. Unlike conventional courses focused on quantum threats to cryptographic systems, EE 597 explores security challenges specific to quantum computing itself. We employ a mixed-methods evaluation using pre and post surveys to assess student learning outcomes and engagement. Results indicate significant improvements in students' understanding of quantum and hardware security, with strong positive feedback on course structure and remote instruction (mean scores: 3.33 to 3.83 on a 4 point scale).",
        "field": "5G Technology",
        "link": "http://arxiv.org/abs/2503.09375v1"
    },
    {
        "id": "2503.09327v1",
        "title": "Heuristic-Based Address Clustering in Cardano Blockchain",
        "authors": [
            "Mostafa Chegenizadeh",
            "Sina Rafati Niya",
            "Claudio J. Tessone"
        ],
        "published": "2025-03-12T12:22:26Z",
        "summary": "Blockchain technology has recently gained widespread popularity as a practical method of storing immutable data while preserving the privacy of users by anonymizing their real identities. This anonymization approach, however, significantly complicates the analysis of blockchain data. To address this problem, heuristic-based clustering algorithms as an effective way of linking all addresses controlled by the same entity have been presented in the literature. In this paper, considering the particular features of the Extended Unspent Transaction Outputs accounting model introduced by the Cardano blockchain, two new clustering heuristics are proposed for clustering the Cardano payment addresses. Applying these heuristics and employing the UnionFind algorithm, we efficiently cluster all the addresses that have appeared on the Cardano blockchain from September 2017 to January 2023, where each cluster represents a distinct entity. The results show that each medium-sized entity in the Cardano network owns and controls 9.67 payment addresses on average. The results also confirm that a power law distribution is fitted to the distribution of entity sizes recognized using our proposed heuristics.",
        "field": "5G Technology",
        "link": "http://arxiv.org/abs/2503.09327v1"
    },
    {
        "id": "2503.10058v1",
        "title": "Deep Learning Approaches for Anti-Money Laundering on Mobile Transactions: Review, Framework, and Directions",
        "authors": [
            "Jiani Fan",
            "Lwin Khin Shar",
            "Ruichen Zhang",
            "Ziyao Liu",
            "Wenzhuo Yang",
            "Dusit Niyato",
            "Bomin Mao",
            "Kwok-Yan Lam"
        ],
        "published": "2025-03-13T05:19:44Z",
        "summary": "Money laundering is a financial crime that obscures the origin of illicit funds, necessitating the development and enforcement of anti-money laundering (AML) policies by governments and organizations. The proliferation of mobile payment platforms and smart IoT devices has significantly complicated AML investigations. As payment networks become more interconnected, there is an increasing need for efficient real-time detection to process large volumes of transaction data on heterogeneous payment systems by different operators such as digital currencies, cryptocurrencies and account-based payments. Most of these mobile payment networks are supported by connected devices, many of which are considered loT devices in the FinTech space that constantly generate data. Furthermore, the growing complexity and unpredictability of transaction patterns across these networks contribute to a higher incidence of false positives. While machine learning solutions have the potential to enhance detection efficiency, their application in AML faces unique challenges, such as addressing privacy concerns tied to sensitive financial data and managing the real-world constraint of limited data availability due to data regulations. Existing surveys in the AML literature broadly review machine learning approaches for money laundering detection, but they often lack an in-depth exploration of advanced deep learning techniques - an emerging field with significant potential. To address this gap, this paper conducts a comprehensive review of deep learning solutions and the challenges associated with their use in AML. Additionally, we propose a novel framework that applies the least-privilege principle by integrating machine learning techniques, codifying AML red flags, and employing account profiling to provide context for predictions and enable effective fraud detection under limited data availability....",
        "field": "Internet of Things (IoT)",
        "link": "http://arxiv.org/abs/2503.10058v1"
    },
    {
        "id": "2503.09513v1",
        "title": "RESTRAIN: Reinforcement Learning-Based Secure Framework for Trigger-Action IoT Environment",
        "authors": [
            "Md Morshed Alam",
            "Lokesh Chandra Das",
            "Sandip Roy",
            "Sachin Shetty",
            "Weichao Wang"
        ],
        "published": "2025-03-12T16:23:14Z",
        "summary": "Internet of Things (IoT) platforms with trigger-action capability allow event conditions to trigger actions in IoT devices autonomously by creating a chain of interactions. Adversaries exploit this chain of interactions to maliciously inject fake event conditions into IoT hubs, triggering unauthorized actions on target IoT devices to implement remote injection attacks. Existing defense mechanisms focus mainly on the verification of event transactions using physical event fingerprints to enforce the security policies to block unsafe event transactions. These approaches are designed to provide offline defense against injection attacks. The state-of-the-art online defense mechanisms offer real-time defense, but extensive reliability on the inference of attack impacts on the IoT network limits the generalization capability of these approaches. In this paper, we propose a platform-independent multi-agent online defense system, namely RESTRAIN, to counter remote injection attacks at runtime. RESTRAIN allows the defense agent to profile attack actions at runtime and leverages reinforcement learning to optimize a defense policy that complies with the security requirements of the IoT network. The experimental results show that the defense agent effectively takes real-time defense actions against complex and dynamic remote injection attacks and maximizes the security gain with minimal computational overhead.",
        "field": "Internet of Things (IoT)",
        "link": "http://arxiv.org/abs/2503.09513v1"
    },
    {
        "id": "2503.09038v1",
        "title": "Image Encryption Using DNA Encoding, Snake Permutation and Chaotic Substitution Techniques",
        "authors": [
            "Waleed Ahmed Farooqui",
            "Jawad Ahmad",
            "Nadeem Kureshi",
            "Fawad Ahmed",
            "Aizaz Ahmad Khattak",
            "Muhammad Shahbaz Khan"
        ],
        "published": "2025-03-12T03:54:37Z",
        "summary": "Securing image data in IoT networks and other insecure information channels is a matter of critical concern. This paper presents a new image encryption scheme using DNA encoding, snake permutation and chaotic substitution techniques that ensures robust security of the image data with reduced computational overhead. The DNA encoding and snake permutation modules ensure effective scrambling of the pixels and result in efficient diffusion in the plaintext image. For the confusion part, the chaotic substitution technique is implemented, which substitutes the pixel values chosen randomly from 3 S-boxes. Extensive security analysis validate the efficacy of the image encryption algorithm proposed in this paper and results demonstrate that the encrypted images have an ideal information entropy of 7.9895 and an almost zero correlation coefficient of -0.001660. These results indicate a high degree of randomness and no correlation in the encrypted image.",
        "field": "Internet of Things (IoT)",
        "link": "http://arxiv.org/abs/2503.09038v1"
    },
    {
        "id": "2503.08607v1",
        "title": "A Fair and Lightweight Consensus Algorithm for IoT",
        "authors": [
            "Sokratis Vavilis",
            "Harris Niavis",
            "Konstantinos Loupos"
        ],
        "published": "2025-03-11T16:45:51Z",
        "summary": "As hyperconnected devices and decentralized data architectures expand, securing IoT transactions becomes increasingly challenging. Blockchain offers a promising solution, but its effectiveness relies on the underlying consensus algorithm. Traditional mechanisms like PoW and PoS are often impractical for resource-constrained IoT environments. To address these limitations, this work introduces a fair and lightweight hybrid consensus algorithm tailored for IoT. The proposed approach minimizes resource demands on the nodes while ensuring a secure and fair agreement process. Specifically, it leverages a distributed lottery mechanism to fairly propose blocks without requiring specialized hardware. In addition, a reputation-based block voting mechanism is incorporated to enhance trust and establish finality. Finally, experimental evaluation was conducted to validate the key features of the consensus algorithm.",
        "field": "Internet of Things (IoT)",
        "link": "http://arxiv.org/abs/2503.08607v1"
    },
    {
        "id": "2503.08293v1",
        "title": "A systematic literature review of unsupervised learning algorithms for anomalous traffic detection based on flows",
        "authors": [
            "Alberto Miguel-Diez",
            "AdriÃ¡n Campazas-Vega",
            "Claudia Ãlvarez-Aparicio",
            "Gonzalo Esteban-Costales",
            "Ãngel Manuel Guerrero-Higueras"
        ],
        "published": "2025-03-11T11:06:00Z",
        "summary": "The constant increase of devices connected to the Internet, and therefore of cyber-attacks, makes it necessary to analyze network traffic in order to recognize malicious activity. Traditional packet-based analysis methods are insufficient because in large networks the amount of traffic is so high that it is unfeasible to review all communications. For this reason, flows is a suitable approach for this situation, which in future 5G networks will have to be used, as the number of packets will increase dramatically. If this is also combined with unsupervised learning models, it can detect new threats for which it has not been trained. This paper presents a systematic review of the literature on unsupervised learning algorithms for detecting anomalies in network flows, following the PRISMA guideline. A total of 63 scientific articles have been reviewed, analyzing 13 of them in depth. The results obtained show that autoencoder is the most used option, followed by SVM, ALAD, or SOM. On the other hand, all the datasets used for anomaly detection have been collected, including some specialised in IoT or with real data collected from honeypots.",
        "field": "Internet of Things (IoT)",
        "link": "http://arxiv.org/abs/2503.08293v1"
    },
    {
        "id": "2502.20359v1",
        "title": "Evaluating the long-term viability of eye-tracking for continuous authentication in virtual reality",
        "authors": [
            "Sai Ganesh Grandhi",
            "Saeed Samet"
        ],
        "published": "2025-02-27T18:32:13Z",
        "summary": "Traditional authentication methods, such as passwords and biometrics, verify a user's identity only at the start of a session, leaving systems vulnerable to session hijacking. Continuous authentication, however, ensures ongoing verification by monitoring user behavior. This study investigates the long-term feasibility of eye-tracking as a behavioral biometric for continuous authentication in virtual reality (VR) environments, using data from the GazebaseVR dataset. Our approach evaluates three architectures, Transformer Encoder, DenseNet, and XGBoost, on short and long-term data to determine their efficacy in user identification tasks. Initial results indicate that both Transformer Encoder and DenseNet models achieve high accuracy rates of up to 97% in short-term settings, effectively capturing unique gaze patterns. However, when tested on data collected 26 months later, model accuracy declined significantly, with rates as low as 1.78% for some tasks. To address this, we propose periodic model updates incorporating recent data, restoring accuracy to over 95%. These findings highlight the adaptability required for gaze-based continuous authentication systems and underscore the need for model retraining to manage evolving user behavior. Our study provides insights into the efficacy and limitations of eye-tracking as a biometric for VR authentication, paving the way for adaptive, secure VR user experiences.",
        "field": "Augmented Reality (AR) and Virtual Reality (VR)",
        "link": "http://arxiv.org/abs/2502.20359v1"
    },
    {
        "id": "2501.19223v1",
        "title": "Through the Looking Glass: LLM-Based Analysis of AR/VR Android Applications Privacy Policies",
        "authors": [
            "Abdulaziz Alghamdi",
            "David Mohaisen"
        ],
        "published": "2025-01-31T15:30:14Z",
        "summary": "\\begin{abstract} This paper comprehensively analyzes privacy policies in AR/VR applications, leveraging BERT, a state-of-the-art text classification model, to evaluate the clarity and thoroughness of these policies. By comparing the privacy policies of AR/VR applications with those of free and premium websites, this study provides a broad perspective on the current state of privacy practices within the AR/VR industry. Our findings indicate that AR/VR applications generally offer a higher percentage of positive segments than free content but lower than premium websites. The analysis of highlighted segments and words revealed that AR/VR applications strategically emphasize critical privacy practices and key terms. This enhances privacy policies' clarity and effectiveness.",
        "field": "Augmented Reality (AR) and Virtual Reality (VR)",
        "link": "http://arxiv.org/abs/2501.19223v1"
    },
    {
        "id": "2501.15313v1",
        "title": "I Know What You Did Last Summer: Identifying VR User Activity Through VR Network Traffic",
        "authors": [
            "Sheikh Samit Muhaimin",
            "Spyridon Mastorakis"
        ],
        "published": "2025-01-25T19:58:29Z",
        "summary": "Virtual Reality (VR) technology has gained substantial traction and has the potential to transform a number of industries, including education, entertainment, and professional sectors. Nevertheless, concerns have arisen about the security and privacy implications of VR applications and the impact that they might have on users. In this paper, we investigate the following overarching research question: can VR applications and VR user activities in the context of such applications (e.g., manipulating virtual objects, walking, talking, flying) be identified based on the (potentially encrypted) network traffic that is generated by VR headsets during the operation of VR applications? To answer this question, we collect network traffic data from 25 VR applications running on the Meta Quest Pro headset and identify characteristics of the generated network traffic, which we subsequently use to train off-the-shelf Machine Learning (ML) models. Our results indicate that through the use of ML models, we can identify the VR applications being used with an accuracy of 92.4F% and the VR user activities performed with an accuracy of 91%. Furthermore, our results demonstrate that an attacker does not need to collect large amounts of network traffic data for each VR application to carry out such an attack. Specifically, an attacker only needs to collect less than 10 minutes of network traffic data for each VR application in order to identify applications with an accuracy higher than 90% and VR user activities with an accuracy higher than 88%.",
        "field": "Augmented Reality (AR) and Virtual Reality (VR)",
        "link": "http://arxiv.org/abs/2501.15313v1"
    },
    {
        "id": "2412.14815v1",
        "title": "Non-intrusive and Unconstrained Keystroke Inference in VR Platforms via Infrared Side Channel",
        "authors": [
            "Tao Ni",
            "Yuefeng Du",
            "Qingchuan Zhao",
            "Cong Wang"
        ],
        "published": "2024-12-19T13:09:46Z",
        "summary": "Virtual Reality (VR) technologies are increasingly employed in numerous applications across various areas. Therefore, it is essential to ensure the security of interactions between users and VR devices. In this paper, we disclose a new side-channel leakage in the constellation tracking system of mainstream VR platforms, where the infrared (IR) signals emitted from the VR controllers for controller-headset interactions can be maliciously exploited to reconstruct unconstrained input keystrokes on the virtual keyboard non-intrusively. We propose a novel keystroke inference attack named VRecKey to demonstrate the feasibility and practicality of this novel infrared side channel. Specifically, VRecKey leverages a customized 2D IR sensor array to intercept ambient IR signals emitted from VR controllers and subsequently infers (i) character-level key presses on the virtual keyboard and (ii) word-level keystrokes along with their typing trajectories. We extensively evaluate the effectiveness of VRecKey with two commercial VR devices, and the results indicate that it can achieve over 94.2% and 90.5% top-3 accuracy in inferring character-level and word-level keystrokes with varying lengths, respectively. In addition, empirical results show that VRecKey is resilient to several practical impact factors and presents effectiveness in various real-world scenarios, which provides a complementary and orthogonal attack surface for the exploration of keystroke inference attacks in VR platforms.",
        "field": "Augmented Reality (AR) and Virtual Reality (VR)",
        "link": "http://arxiv.org/abs/2412.14815v1"
    },
    {
        "id": "2412.06759v2",
        "title": "XRZoo: A Large-Scale and Versatile Dataset of Extended Reality (XR) Applications",
        "authors": [
            "Shuqing Li",
            "Chenran Zhang",
            "Cuiyun Gao",
            "Michael R. Lyu"
        ],
        "published": "2024-12-09T18:49:27Z",
        "summary": "The rapid advancement of Extended Reality (XR, encompassing AR, MR, and VR) and spatial computing technologies forms a foundational layer for the emerging Metaverse, enabling innovative applications across healthcare, education, manufacturing, and entertainment. However, research in this area is often limited by the lack of large, representative, and highquality application datasets that can support empirical studies and the development of new approaches benefiting XR software processes. In this paper, we introduce XRZoo, a comprehensive and curated dataset of XR applications designed to bridge this gap. XRZoo contains 12,528 free XR applications, spanning nine app stores, across all XR techniques (i.e., AR, MR, and VR) and use cases, with detailed metadata on key aspects such as application descriptions, application categories, release dates, user review numbers, and hardware specifications, etc. By making XRZoo publicly available, we aim to foster reproducible XR software engineering and security research, enable cross-disciplinary investigations, and also support the development of advanced XR systems by providing examples to developers. Our dataset serves as a valuable resource for researchers and practitioners interested in improving the scalability, usability, and effectiveness of XR applications. XRZoo will be released and actively maintained.",
        "field": "Augmented Reality (AR) and Virtual Reality (VR)",
        "link": "http://arxiv.org/abs/2412.06759v2"
    },
    {
        "id": "2503.09823v1",
        "title": "Data Traceability for Privacy Alignment",
        "authors": [
            "Kevin Liao",
            "Shreya Thipireddy",
            "Daniel Weitzner"
        ],
        "published": "2025-03-12T20:42:23Z",
        "summary": "This paper offers a new privacy approach for the growing ecosystem of services--ranging from open banking to healthcare--dependent on sensitive personal data sharing between individuals and third-parties. While these services offer significant benefits, individuals want control over their data, transparency regarding how their data is used, and accountability from third-parties for misuse. However, existing legal and technical mechanisms are inadequate for supporting these needs. A comprehensive approach to the modern privacy challenges of accountable third-party data sharing requires a closer alignment of technical system architecture and legal institutional design. In order to achieve this privacy alignment, we extend traditional security threat modeling and analysis to encompass a broader range of privacy notions than has been typically considered. In particular, we introduce the concept of covert-accountability, which addresses adversaries that may act dishonestly but face potential identification and legal consequences. As a concrete instance of this design approach, we present the OTrace protocol, designed to provide traceable, accountable, consumer-control in third-party data sharing ecosystems. OTrace empowers consumers with the knowledge of where their data is, who has it, what it is being used for, and whom it is being shared with. By applying our alignment framework to OTrace, we demonstrate that OTrace's technical affordances can provide more confident, scalable regulatory oversight when combined with complementary legal mechanisms.",
        "field": "Natural Language Processing (NLP) in Banking",
        "link": "http://arxiv.org/abs/2503.09823v1"
    },
    {
        "id": "2503.09586v1",
        "title": "Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot",
        "authors": [
            "Andrew Crossman",
            "Andrew R. Plummer",
            "Chandra Sekharudu",
            "Deepak Warrier",
            "Mohammad Yekrangian"
        ],
        "published": "2025-03-12T17:54:18Z",
        "summary": "We present Auspex - a threat modeling system built using a specialized collection of generative artificial intelligence-based methods that capture threat modeling tradecraft. This new approach, called tradecraft prompting, centers on encoding the on-the-ground knowledge of threat modelers within the prompts that drive a generative AI-based threat modeling system. Auspex employs tradecraft prompts in two processing stages. The first stage centers on ingesting and processing system architecture information using prompts that encode threat modeling tradecraft knowledge pertaining to system decomposition and description. The second stage centers on chaining the resulting system analysis through a collection of prompts that encode tradecraft knowledge on threat identification, classification, and mitigation. The two-stage process yields a threat matrix for a system that specifies threat scenarios, threat types, information security categorizations and potential mitigations. Auspex produces formalized threat model output in minutes, relative to the weeks or months a manual process takes. More broadly, the focus on bespoke tradecraft prompting, as opposed to fine-tuning or agent-based add-ons, makes Auspex a lightweight, flexible, modular, and extensible foundational system capable of addressing the complexity, resource, and standardization limitations of both existing manual and automated threat modeling processes. In this connection, we establish the baseline value of Auspex to threat modelers through an evaluation procedure based on feedback collected from cybersecurity subject matter experts measuring the quality and utility of threat models generated by Auspex on real banking systems. We conclude with a discussion of system performance and plans for enhancements to Auspex.",
        "field": "Natural Language Processing (NLP) in Banking",
        "link": "http://arxiv.org/abs/2503.09586v1"
    },
    {
        "id": "2503.00070v1",
        "title": "Systematic Review of Cybersecurity in Banking: Evolution from Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain, Policies and Practice",
        "authors": [
            "Tue Nhi Tran"
        ],
        "published": "2025-02-27T14:17:06Z",
        "summary": "Throughout the history from pre-industry 4.0 to post-industry 4.0, cybersecurity at banks has undergone significant changes. Pre-industry 4.0 cyber security at banks relied on individual security methods that were highly manual and had low accuracy. When moving to post-industry 4.0, cybersecurity at banks had a major turning point with security methods that combined different technologies such as Artificial Intelligence (AI), Blockchain, IoT, automating necessary processes and significantly increasing the defence layer for banks. However, along with the development of new technologies, the current challenge of cybersecurity at banks lies in scalability, high costs and resources in both money and time for R&D of defence methods along with the threat of high-tech cybercriminals growing and expanding. This report goes from introducing the importance of cybersecurity at banks, analyzing their management, operational and business objectives, evaluating pre-industry 4.0 technologies used for cybersecurity at banks to assessing post-industry 4.0 technologies focusing on Artificial Intelligence and Blockchain, discussing current policies and practices and ending with discussing key advantages and challenges for 4.0 technologies and recommendations for further developing cybersecurity at banks.",
        "field": "Natural Language Processing (NLP) in Banking",
        "link": "http://arxiv.org/abs/2503.00070v1"
    },
    {
        "id": "2503.10644v1",
        "title": "Combined climate stress testing of supply-chain networks and the financial system with nation-wide firm-level emission estimates",
        "authors": [
            "Zlata TabachovÃ¡",
            "Christian Diem",
            "Johannes Stangl",
            "AndrÃ¡s Borsos",
            "Stefan Thurner"
        ],
        "published": "2025-02-25T20:25:47Z",
        "summary": "On the way towards carbon neutrality, climate stress testing provides estimates for the physical and transition risks that climate change poses to the economy and the financial system. Missing firm-level CO2 emissions data severely impedes the assessment of transition risks originating from carbon pricing. Based on the individual emissions of all Hungarian firms (410,523), as estimated from their fossil fuel purchases, we conduct a stress test of both actual and hypothetical carbon pricing policies. Using a simple 1:1 economic ABM and introducing the new carbon-to-profit ratio, we identify firms that become unprofitable and default, and estimate the respective loan write-offs. We find that 45% of all companies are directly exposed to carbon pricing. At a price of 45 EUR/t, direct economic losses of 1.3% of total sales and bank equity losses of 1.2% are expected. Secondary default cascades in supply chain networks could increase these losses by 300% to 4000%, depending on firms' ability to substitute essential inputs. To reduce transition risks, firms should reduce their dependence on essential inputs from supply chains with high CO2 exposure. We discuss the implications of different policy implementations on these transition risks.",
        "field": "Natural Language Processing (NLP) in Banking",
        "link": "http://arxiv.org/abs/2503.10644v1"
    },
    {
        "id": "2502.14766v2",
        "title": "Multi-Layer Deep xVA: Structural Credit Models, Measure Changes and Convergence Analysis",
        "authors": [
            "Kristoffer Andersson",
            "Alessandro Gnoatto"
        ],
        "published": "2025-02-20T17:41:55Z",
        "summary": "We propose a structural default model for portfolio-wide valuation adjustments (xVAs) and represent it as a system of coupled backward stochastic differential equations. The framework is divided into four layers, each capturing a key component: (i) clean values, (ii) initial margin and Collateral Valuation Adjustment (ColVA), (iii) Credit/Debit Valuation Adjustments (CVA/DVA) together with Margin Valuation Adjustment (MVA), and (iv) Funding Valuation Adjustment (FVA). Because these layers depend on one another through collateral and default effects, a naive Monte Carlo approach would require deeply nested simulations, making the problem computationally intractable. To address this challenge, we use an iterative deep BSDE approach, handling each layer sequentially so that earlier outputs serve as inputs to the subsequent layers. Initial margin is computed via deep quantile regression to reflect margin requirements over the Margin Period of Risk. We also adopt a change-of-measure method that highlights rare but significant defaults of the bank or counterparty, ensuring that these events are accurately captured in the training process. We further extend Han and Long's (2020) a posteriori error analysis to BSDEs on bounded domains. Due to the random exit from the domain, we obtain an order of convergence of $\\mathcal{O}(h^{1/4-\\epsilon})$ rather than the usual $\\mathcal{O}(h^{1/2})$. Numerical experiments illustrate that this method drastically reduces computational demands and successfully scales to high-dimensional, non-symmetric portfolios. The results confirm its effectiveness and accuracy, offering a practical alternative to nested Monte Carlo simulations in multi-counterparty xVA analyses.",
        "field": "Natural Language Processing (NLP) in Banking",
        "link": "http://arxiv.org/abs/2502.14766v2"
    },
    {
        "id": "2503.10207v1",
        "title": "Efficient Implementation of CRYSTALS-KYBER Key Encapsulation Mechanism on ESP32",
        "authors": [
            "Fabian Segatz",
            "Muhammad Ihsan Al Hafiz"
        ],
        "published": "2025-03-13T09:45:31Z",
        "summary": "Kyber, an IND-CCA2-secure lattice-based post-quantum key-encapsulation mechanism, is the winner of the first post-quantum cryptography standardization process of the US National Institute of Standards and Technology. In this work, we provide an efficient implementation of Kyber on ESP32, a very popular microcontroller for Internet of Things applications. We hand-partition the Kyber algorithm to enable utilization of the ESP32 dual-core architecture, which allows us to speed up its execution by 1.21x (keygen), 1.22x (encaps) and 1.20x (decaps). We also explore the possibility of gaining further improvement by utilizing the ESP32 SHA and AES coprocessor and achieve a culminated speed-up of 1.72x (keygen), 1.84x (encaps) and 1.69x (decaps).",
        "field": "Digital Twin Technology",
        "link": "http://arxiv.org/abs/2503.10207v1"
    },
    {
        "id": "2503.10171v1",
        "title": "Verifiable, Efficient and Confidentiality-Preserving Graph Search with Transparency",
        "authors": [
            "Qiuhao Wang",
            "Xu Yang",
            "Yiwei Liu",
            "Saiyu Qi",
            "Hongguang Zhao",
            "Ke Li",
            "Yong Qi"
        ],
        "published": "2025-03-13T08:53:53Z",
        "summary": "Graph databases have garnered extensive attention and research due to their ability to manage relationships between entities efficiently. Today, many graph search services have been outsourced to a third-party server to facilitate storage and computational support. Nevertheless, the outsourcing paradigm may invade the privacy of graphs. PeGraph is the latest scheme achieving encrypted search over social graphs to address the privacy leakage, which maintains two data structures XSet and TSet motivated by the OXT technology to support encrypted conjunctive search. However, PeGraph still exhibits limitations inherent to the underlying OXT. It does not provide transparent search capabilities, suffers from expensive computation and result pattern leakages, and it fails to support search over dynamic encrypted graph database and results verification. In this paper, we propose SecGraph to address the first two limitations, which adopts a novel system architecture that leverages an SGX-enabled cloud server to provide users with secure and transparent search services since the secret key protection and computational overhead have been offloaded to the cloud server. Besides, we design an LDCF-encoded XSet based on the Logarithmic Dynamic Cuckoo Filter to facilitate efficient plaintext computation in trusted memory, effectively mitigating the risks of result pattern leakage and performance degradation due to exceeding the limited trusted memory capacity. Finally, we design a new dynamic version of TSet named Twin-TSet to enable conjunctive search over dynamic encrypted graph database. In order to support verifiable search, we further propose VSecGraph, which utilizes a procedure-oriented verification method to verify all data structures loaded into the trusted memory, thus bypassing the computational overhead associated with the client's local verification.",
        "field": "Digital Twin Technology",
        "link": "http://arxiv.org/abs/2503.10171v1"
    },
    {
        "id": "2503.09666v1",
        "title": "Blockchain-Enabled Management Framework for Federated Coalition Networks",
        "authors": [
            "Jorge Ãlvaro GonzÃ¡lez",
            "Ana MarÃ­a Saiz GarcÃ­a",
            "Victor Monzon Baeza"
        ],
        "published": "2025-03-12T16:59:23Z",
        "summary": "In a globalized and interconnected world, interoperability has become a key concept for advancing tactical scenarios. Federated Coalition Networks (FCN) enable cooperation between entities from multiple nations while allowing each to maintain control over their systems. However, this interoperability necessitates the sharing of increasing amounts of information between different tactical assets, raising the need for higher security measures. Emerging technologies like blockchain drive a revolution in secure communications, paving the way for new tactical scenarios. In this work, we propose a blockchain-based framework to enhance the resilience and security of the management of these networks. We offer a guide to FCN design to help a broad audience understand the military networks in international missions by a use case and key functions applied to a proposed architecture. We evaluate its effectiveness and performance in information encryption to validate this framework.",
        "field": "Digital Twin Technology",
        "link": "http://arxiv.org/abs/2503.09666v1"
    },
    {
        "id": "2503.09375v1",
        "title": "Quantum Computing and Cybersecurity Education: A Novel Curriculum for Enhancing Graduate STEM Learning",
        "authors": [
            "Suryansh Upadhyay",
            "Koustubh Phalak",
            "Jungeun Lee",
            "Kathleen Mitchell Hill",
            "Swaroop Ghosh"
        ],
        "published": "2025-03-12T13:26:54Z",
        "summary": "Quantum computing is an emerging paradigm with the potential to transform numerous application areas by addressing problems considered intractable in the classical domain. However, its integration into cyberspace introduces significant security and privacy challenges. The exponential rise in cyber attacks, further complicated by quantum capabilities, poses serious risks to financial systems and national security. The scope of quantum threats extends beyond traditional software, operating system, and network vulnerabilities, necessitating a shift in cybersecurity education. Traditional cybersecurity education, often reliant on didactic methods, lacks hands on, student centered learning experiences necessary to prepare students for these evolving challenges. There is an urgent need for curricula that address both classical and quantum security threats through experiential learning. In this work, we present the design and evaluation of EE 597: Introduction to Hardware Security, a graduate level course integrating hands-on quantum security learning with classical security concepts through simulations and cloud-based quantum hardware. Unlike conventional courses focused on quantum threats to cryptographic systems, EE 597 explores security challenges specific to quantum computing itself. We employ a mixed-methods evaluation using pre and post surveys to assess student learning outcomes and engagement. Results indicate significant improvements in students' understanding of quantum and hardware security, with strong positive feedback on course structure and remote instruction (mean scores: 3.33 to 3.83 on a 4 point scale).",
        "field": "Digital Twin Technology",
        "link": "http://arxiv.org/abs/2503.09375v1"
    },
    {
        "id": "2503.09327v1",
        "title": "Heuristic-Based Address Clustering in Cardano Blockchain",
        "authors": [
            "Mostafa Chegenizadeh",
            "Sina Rafati Niya",
            "Claudio J. Tessone"
        ],
        "published": "2025-03-12T12:22:26Z",
        "summary": "Blockchain technology has recently gained widespread popularity as a practical method of storing immutable data while preserving the privacy of users by anonymizing their real identities. This anonymization approach, however, significantly complicates the analysis of blockchain data. To address this problem, heuristic-based clustering algorithms as an effective way of linking all addresses controlled by the same entity have been presented in the literature. In this paper, considering the particular features of the Extended Unspent Transaction Outputs accounting model introduced by the Cardano blockchain, two new clustering heuristics are proposed for clustering the Cardano payment addresses. Applying these heuristics and employing the UnionFind algorithm, we efficiently cluster all the addresses that have appeared on the Cardano blockchain from September 2017 to January 2023, where each cluster represents a distinct entity. The results show that each medium-sized entity in the Cardano network owns and controls 9.67 payment addresses on average. The results also confirm that a power law distribution is fitted to the distribution of entity sizes recognized using our proposed heuristics.",
        "field": "Digital Twin Technology",
        "link": "http://arxiv.org/abs/2503.09327v1"
    },
    {
        "id": "2503.11508v1",
        "title": "Leveraging Angle of Arrival Estimation against Impersonation Attacks in Physical Layer Authentication",
        "authors": [
            "Thuy M. Pham",
            "Linda Senigagliesi",
            "Marco Baldi",
            "Rafael F. Schaefer",
            "Gerhard P. Fettweis",
            "Arsenia Chorti"
        ],
        "published": "2025-03-14T15:29:55Z",
        "summary": "In this paper, we investigate the utilization of the angle of arrival (AoA) as a feature for robust physical layer authentication (PLA). While most of the existing approaches to PLA focus on common features of the physical layer of communication channels, such as channel frequency response, channel impulse response or received signal strength, the use of AoA in this domain has not yet been studied in depth, particularly regarding the ability to thwart impersonation attacks. In this work, we demonstrate that an impersonation attack targeting AoA based PLA is only feasible under strict conditions on the attacker's location and hardware capabilities, which highlights the AoA's potential as a strong feature for PLA. We extend previous works considering a single-antenna attacker to the case of a multiple-antenna attacker, and we develop a theoretical characterization of the conditions in which a successful impersonation attack can be mounted. Furthermore, we leverage extensive simulations in support of theoretical analyses, to validate the robustness of AoA-based PLA.",
        "field": "Biometric Authentication",
        "link": "http://arxiv.org/abs/2503.11508v1"
    },
    {
        "id": "2503.08632v1",
        "title": "Secret-Key Generation from Private Identifiers under Channel Uncertainty",
        "authors": [
            "Vamoua Yachongka",
            "RÃ©mi A. Chou"
        ],
        "published": "2025-03-11T17:20:48Z",
        "summary": "This study investigates secret-key generation for device authentication using physical identifiers, such as responses from physical unclonable functions (PUFs). The system includes two legitimate terminals (encoder and decoder) and an eavesdropper (Eve), each with access to different measurements of the identifier. From the device identifier, the encoder generates a secret key, which is securely stored in a private database, along with helper data that is saved in a public database accessible by the decoder for key reconstruction. Eve, who also has access to the public database, may use both her own measurements and the helper data to attempt to estimate the secret key and identifier. Our setup focuses on authentication scenarios where channel statistics are uncertain, with the involved parties employing multiple antennas to enhance signal reception. Our contributions include deriving inner and outer bounds on the optimal trade-off among secret-key, storage, and privacy-leakage rates for general discrete sources, and showing that these bounds are tight for Gaussian sources.",
        "field": "Biometric Authentication",
        "link": "http://arxiv.org/abs/2503.08632v1"
    },
    {
        "id": "2503.08256v1",
        "title": "SoK: A cloudy view on trust relationships of CVMs -- How Confidential Virtual Machines are falling short in Public Cloud",
        "authors": [
            "Jana Eisoldt",
            "Anna Galanou",
            "Andrey Ruzhanskiy",
            "Nils KÃ¼chenmeister",
            "Yewgenij Baburkin",
            "Tianxiang Dai",
            "Ivan Gudymenko",
            "Stefan KÃ¶psell",
            "RÃ¼diger Kapitza"
        ],
        "published": "2025-03-11T10:21:29Z",
        "summary": "Confidential computing in the public cloud intends to safeguard workload privacy while outsourcing infrastructure management to a cloud provider. This is achieved by executing customer workloads within so called Trusted Execution Environments (TEEs), such as Confidential Virtual Machines (CVMs), which protect them from unauthorized access by cloud administrators and privileged system software. At the core of confidential computing lies remote attestation -- a mechanism that enables workload owners to verify the initial state of their workload and furthermore authenticate the underlying hardware. hile this represents a significant advancement in cloud security, this SoK critically examines the confidential computing offerings of market-leading cloud providers to assess whether they genuinely adhere to its core principles. We develop a taxonomy based on carefully selected criteria to systematically evaluate these offerings, enabling us to analyse the components responsible for remote attestation, the evidence provided at each stage, the extent of cloud provider influence and whether this undermines the threat model of confidential computing. Specifically, we investigate how CVMs are deployed in the public cloud infrastructures, the extent to which customers can request and verify attestation evidence, and their ability to define and enforce configuration and attestation requirements. This analysis provides insight into whether confidential computing guarantees -- namely confidentiality and integrity -- are genuinely upheld. Our findings reveal that all major cloud providers retain control over critical parts of the trusted software stack and, in some cases, intervene in the standard remote attestation process. This directly contradicts their claims of delivering confidential computing, as the model fundamentally excludes the cloud provider from the set of trusted entities.",
        "field": "Biometric Authentication",
        "link": "http://arxiv.org/abs/2503.08256v1"
    },
    {
        "id": "2503.07857v1",
        "title": "Efficient Resource Management for Secure and Low-Latency O-RAN Communication",
        "authors": [
            "Zaineh Abughazzah",
            "Emna Baccour",
            "Ahmed Refaey",
            "Amr Mohamed",
            "Mounir Hamdi"
        ],
        "published": "2025-03-10T21:03:48Z",
        "summary": "Open Radio Access Networks (O-RAN) are transforming telecommunications by shifting from centralized to distributed architectures, promoting flexibility, interoperability, and innovation through open interfaces and multi-vendor environments. However, O-RAN's reliance on cloud-based architecture and enhanced observability introduces significant security and resource management challenges. Efficient resource management is crucial for secure and reliable communication in O-RAN, within the resource-constrained environment and heterogeneity of requirements, where multiple User Equipment (UE) and O-RAN Radio Units (O-RUs) coexist. This paper develops a framework to manage these aspects, ensuring each O-RU is associated with UEs based on their communication channel qualities and computational resources, and selecting appropriate encryption algorithms to safeguard data confidentiality, integrity, and authentication. A Multi-objective Optimization Problem (MOP) is formulated to minimize latency and maximize security within resource constraints. Different approaches are proposed to relax the complexity of the problem and achieve near-optimal performance, facilitating trade-offs between latency, security, and solution complexity. Simulation results demonstrate that the proposed approaches are close enough to the optimal solution, proving that our approach is both effective and efficient.",
        "field": "Biometric Authentication",
        "link": "http://arxiv.org/abs/2503.07857v1"
    },
    {
        "id": "2503.05839v1",
        "title": "Enhancing AUTOSAR-Based Firmware Over-the-Air Updates in the Automotive Industry with a Practical Implementation on a Steering System",
        "authors": [
            "Mostafa Ahmed Mostafa Ahmed",
            "Mohamed Khaled Mohamed Elsayed",
            "Radwa Waheed Ezzat Abdelmohsen"
        ],
        "published": "2025-03-06T23:54:40Z",
        "summary": "The automotive industry is increasingly reliant on software to manage complex vehicle functionalities, making efficient and secure firmware updates essential. Traditional firmware update methods, requiring physical connections through On-Board Diagnostics (OBD) ports, are inconvenient, costly, and time-consuming. Firmware Over-the-Air (FOTA) technology offers a revolutionary solution by enabling wireless updates, reducing operational costs, and enhancing the user experience. This project aims to design and implement an advanced FOTA system tailored for modern vehicles, incorporating the AUTOSAR architecture for scalability and standardization, and utilizing delta updating to minimize firmware update sizes, thereby improving bandwidth efficiency and reducing flashing times. To ensure security, the system integrates the UDS 0x27 protocol for authentication and data integrity during the update process. Communication between Electronic Control Units (ECUs) is achieved using the CAN protocol, while the ESP8266 module and the master ECU communicate via SPI for data transfer. The system's architecture includes key components such as a bootloader, boot manager, and bootloader updater to facilitate seamless firmware updates. The functionality of the system is demonstrated through two applications: a blinking LED and a Lane Keeping Assist (LKA) system, showcasing its versatility in handling critical automotive features. This project represents a significant step forward in automotive technology, offering a user-centric, efficient, and secure solution for automotive firmware management.",
        "field": "Biometric Authentication",
        "link": "http://arxiv.org/abs/2503.05839v1"
    },
    {
        "id": "2503.10945v1",
        "title": "$(\\varepsilon, Î´)$ Considered Harmful: Best Practices for Reporting Differential Privacy Guarantees",
        "authors": [
            "Juan Felipe Gomez",
            "Bogdan Kulynych",
            "Georgios Kaissis",
            "Jamie Hayes",
            "Borja Balle",
            "Antti Honkela"
        ],
        "published": "2025-03-13T23:06:30Z",
        "summary": "Current practices for reporting the level of differential privacy (DP) guarantees for machine learning (ML) algorithms provide an incomplete and potentially misleading picture of the guarantees and make it difficult to compare privacy levels across different settings. We argue for using Gaussian differential privacy (GDP) as the primary means of communicating DP guarantees in ML, with the full privacy profile as a secondary option in case GDP is too inaccurate. Unlike other widely used alternatives, GDP has only one parameter, which ensures easy comparability of guarantees, and it can accurately capture the full privacy profile of many important ML applications. To support our claims, we investigate the privacy profiles of state-of-the-art DP large-scale image classification, and the TopDown algorithm for the U.S. Decennial Census, observing that GDP fits the profiles remarkably well in all three cases. Although GDP is ideal for reporting the final guarantees, other formalisms (e.g., privacy loss random variables) are needed for accurate privacy accounting. We show that such intermediate representations can be efficiently converted to GDP with minimal loss in tightness.",
        "field": "Federated Learning",
        "link": "http://arxiv.org/abs/2503.10945v1"
    },
    {
        "id": "2503.10944v1",
        "title": "Phishsense-1B: A Technical Perspective on an AI-Powered Phishing Detection Model",
        "authors": [
            "SE Blake"
        ],
        "published": "2025-03-13T23:03:09Z",
        "summary": "Phishing is a persistent cybersecurity threat in today's digital landscape. This paper introduces Phishsense-1B, a refined version of the Llama-Guard-3-1B model, specifically tailored for phishing detection and reasoning. This adaptation utilizes Low-Rank Adaptation (LoRA) and the GuardReasoner finetuning methodology. We outline our LoRA-based fine-tuning process, describe the balanced dataset comprising phishing and benign emails, and highlight significant performance improvements over the original model. Our findings indicate that Phishsense-1B achieves an impressive 97.5% accuracy on a custom dataset and maintains strong performance with 70% accuracy on a challenging real-world dataset. This performance notably surpasses both unadapted models and BERT-based detectors. Additionally, we examine current state-of-the-art detection methods, compare prompt-engineering with fine-tuning strategies, and explore potential deployment scenarios.",
        "field": "Federated Learning",
        "link": "http://arxiv.org/abs/2503.10944v1"
    },
    {
        "id": "2503.10809v1",
        "title": "Attacking Multimodal OS Agents with Malicious Image Patches",
        "authors": [
            "Lukas Aichberger",
            "Alasdair Paren",
            "Yarin Gal",
            "Philip Torr",
            "Adel Bibi"
        ],
        "published": "2025-03-13T18:59:12Z",
        "summary": "Recent advances in operating system (OS) agents enable vision-language models to interact directly with the graphical user interface of an OS. These multimodal OS agents autonomously perform computer-based tasks in response to a single prompt via application programming interfaces (APIs). Such APIs typically support low-level operations, including mouse clicks, keyboard inputs, and screenshot captures. We introduce a novel attack vector: malicious image patches (MIPs) that have been adversarially perturbed so that, when captured in a screenshot, they cause an OS agent to perform harmful actions by exploiting specific APIs. For instance, MIPs embedded in desktop backgrounds or shared on social media can redirect an agent to a malicious website, enabling further exploitation. These MIPs generalise across different user requests and screen layouts, and remain effective for multiple OS agents. The existence of such attacks highlights critical security vulnerabilities in OS agents, which should be carefully addressed before their widespread adoption.",
        "field": "Federated Learning",
        "link": "http://arxiv.org/abs/2503.10809v1"
    },
    {
        "id": "2503.10269v1",
        "title": "Targeted Data Poisoning for Black-Box Audio Datasets Ownership Verification",
        "authors": [
            "Wassim Bouaziz",
            "El-Mahdi El-Mhamdi",
            "Nicolas Usunier"
        ],
        "published": "2025-03-13T11:25:25Z",
        "summary": "Protecting the use of audio datasets is a major concern for data owners, particularly with the recent rise of audio deep learning models. While watermarks can be used to protect the data itself, they do not allow to identify a deep learning model trained on a protected dataset. In this paper, we adapt to audio data the recently introduced data taggants approach. Data taggants is a method to verify if a neural network was trained on a protected image dataset with top-$k$ predictions access to the model only. This method relies on a targeted data poisoning scheme by discreetly altering a small fraction (1%) of the dataset as to induce a harmless behavior on out-of-distribution data called keys. We evaluate our method on the Speechcommands and the ESC50 datasets and state of the art transformer models, and show that we can detect the use of the dataset with high confidence without loss of performance. We also show the robustness of our method against common data augmentation techniques, making it a practical method to protect audio datasets.",
        "field": "Federated Learning",
        "link": "http://arxiv.org/abs/2503.10269v1"
    },
    {
        "id": "2503.10218v1",
        "title": "Moss: Proxy Model-based Full-Weight Aggregation in Federated Learning with Heterogeneous Models",
        "authors": [
            "Yifeng Cai",
            "Ziqi Zhang",
            "Ding Li",
            "Yao Guo",
            "Xiangqun Chen"
        ],
        "published": "2025-03-13T10:00:58Z",
        "summary": "Modern Federated Learning (FL) has become increasingly essential for handling highly heterogeneous mobile devices. Current approaches adopt a partial model aggregation paradigm that leads to sub-optimal model accuracy and higher training overhead. In this paper, we challenge the prevailing notion of partial-model aggregation and propose a novel \"full-weight aggregation\" method named Moss, which aggregates all weights within heterogeneous models to preserve comprehensive knowledge. Evaluation across various applications demonstrates that Moss significantly accelerates training, reduces on-device training time and energy consumption, enhances accuracy, and minimizes network bandwidth utilization when compared to state-of-the-art baselines.",
        "field": "Federated Learning",
        "link": "http://arxiv.org/abs/2503.10218v1"
    },
    {
        "id": "2502.13513v1",
        "title": "Phantom Events: Demystifying the Issues of Log Forgery in Blockchain",
        "authors": [
            "Yixuan Liu",
            "Yuxin Dong",
            "Ye Liu",
            "Xiapu Luo",
            "Yi Li"
        ],
        "published": "2025-02-19T08:07:26Z",
        "summary": "With the rapid development of blockchain technology, transaction logs play a central role in various applications, including decentralized exchanges, wallets, cross-chain bridges, and other third-party services. However, these logs, particularly those based on smart contract events, are highly susceptible to manipulation and forgery, creating substantial security risks across the ecosystem. To address this issue, we present the first in-depth security analysis of transaction log forgery in EVM-based blockchains, a phenomenon we term Phantom Events. We systematically model five types of attacks and propose a tool designed to detect event forgery vulnerabilities in smart contracts. Our evaluation demonstrates that our approach outperforms existing tools in identifying potential phantom events. Furthermore, we have successfully identified real-world instances for all five types of attacks across multiple decentralized applications. Finally, we call on community developers to take proactive steps to address these critical security vulnerabilities.",
        "field": "Digital Wallets",
        "link": "http://arxiv.org/abs/2502.13513v1"
    },
    {
        "id": "2502.03247v1",
        "title": "Thetacrypt: A Distributed Service for Threshold Cryptography",
        "authors": [
            "Mariarosaria Barbaraci",
            "Noah Schmid",
            "Orestis Alpos",
            "Michael Senn",
            "Christian Cachin"
        ],
        "published": "2025-02-05T15:03:59Z",
        "summary": "Threshold cryptography is a powerful and well-known technique with many applications to systems relying on distributed trust. It has recently emerged also as a solution to challenges in blockchain: frontrunning prevention, managing wallet keys, and generating randomness. This work presents Thetacrypt, a versatile library for integrating many threshold schemes into one codebase. It offers a way to easily build distributed systems using threshold cryptography and is agnostic to their implementation language. The architecture of Thetacrypt supports diverse protocols uniformly. The library currently includes six cryptographic schemes that span ciphers, signatures, and randomness generation. The library additionally contains a flexible adapter to an underlying networking layer that provides peer-to-peer communication and a total-order broadcast channel; the latter can be implemented by distributed ledgers, for instance. Thetacrypt serves as a controlled testbed for evaluating the performance of multiple threshold-cryptographic schemes under consistent conditions, showing how the traditional micro benchmarking approach neglects the distributed nature of the protocols and its relevance when considering system performance.",
        "field": "Digital Wallets",
        "link": "http://arxiv.org/abs/2502.03247v1"
    },
    {
        "id": "2501.17089v1",
        "title": "CRSet: Non-Interactive Verifiable Credential Revocation with Metadata Privacy for Issuers and Everyone Else",
        "authors": [
            "Felix Hoops",
            "Jonas Gebele",
            "Florian Matthes"
        ],
        "published": "2025-01-28T17:23:45Z",
        "summary": "Like any digital certificate, Verifiable Credentials (VCs) require a way to revoke them in case of an error or key compromise. Existing solutions for VC revocation, most prominently Bitstring Status List, are not viable for many use cases since they leak the issuer's behavior, which in turn leaks internal business metrics. For instance, exact staff fluctuation through issuance and revocation of employee IDs. We introduce CRSet, a revocation mechanism that allows an issuer to encode revocation information for years worth of VCs as a Bloom filter cascade. Padding is used to provide deniability for issuer metrics. Issuers periodically publish this filter cascade on a decentralized storage system. Relying Parties (RPs) can download it to perform any number of revocation checks locally. Compared to existing solutions, CRSet protects the metadata of subject, RPs, and issuer equally. At the same time, it is non-interactive, making it work with wallet devices having limited hardware power and drop-in compatible with existing VC exchange protocols and wallet applications. We present a prototype using the Ethereum blockchain as decentralized storage. The recently introduced blob-carrying transactions, enabling cheaper data writes, allow us to write each CRSet directly to the chain. We built software for issuers and RPs that we successfully tested end-to-end with an existing publicly available wallet agents and the OpenID for Verifiable Credentials protocols. Storage and bandwidth costs paid by issuers and RP are higher than for Bitstring Status List, but still manageable at around 1 MB for an issuer issuing hundreds of thousands of VCs annually and covering decades.",
        "field": "Digital Wallets",
        "link": "http://arxiv.org/abs/2501.17089v1"
    },
    {
        "id": "2501.16681v1",
        "title": "Blockchain Address Poisoning",
        "authors": [
            "Taro Tsuchiya",
            "Jin-Dong Dong",
            "Kyle Soska",
            "Nicolas Christin"
        ],
        "published": "2025-01-28T03:34:59Z",
        "summary": "In many blockchains, e.g., Ethereum, Binance Smart Chain (BSC), the primary representation used for wallet addresses is a hardly memorable 40-digit hexadecimal string. As a result, users often select addresses from their recent transaction history, which enables blockchain address poisoning. The adversary first generates lookalike addresses similar to one with which the victim has previously interacted, and then engages with the victim to ``poison'' their transaction history. The goal is to have the victim mistakenly send tokens to the lookalike address, as opposed to the intended recipient. Compared to contemporary studies, this paper provides four notable contributions. First, we develop a detection system and perform measurements over two years on Ethereum and BSC. We identify 13 times the number of attack attempts reported previously -- totaling 270M on-chain attacks targeting 17M victims. 6,633 incidents have caused at least 83.8M USD in losses, which makes blockchain address poisoning one of the largest cryptocurrency phishing schemes observed in the wild. Second, we analyze a few large attack entities using improved clustering techniques, and model attacker profitability and competition. Third, we reveal attack strategies -- targeted populations, success conditions (address similarity, timing), and cross-chain attacks. Fourth, we mathematically define and simulate the lookalike address-generation process across various software- and hardware-based implementations, and identify a large-scale attacker group that appears to use GPUs. We also discuss defensive countermeasures.",
        "field": "Digital Wallets",
        "link": "http://arxiv.org/abs/2501.16681v1"
    },
    {
        "id": "2501.11798v1",
        "title": "Blockchain Security Risk Assessment in Quantum Era, Migration Strategies and Proactive Defense",
        "authors": [
            "Yaser Baseri",
            "Abdelhakim Hafid",
            "Yahya Shahsavari",
            "Dimitrios Makrakis",
            "Hassan Khodaiemehr"
        ],
        "published": "2025-01-21T00:27:41Z",
        "summary": "The emergence of quantum computing presents a formidable challenge to the security of blockchain systems. Traditional cryptographic algorithms, foundational to digital signatures, message encryption, and hashing functions, become vulnerable to the immense computational power of quantum computers. This paper conducts a thorough risk assessment of transitioning to quantum-resistant blockchains, comprehensively analyzing potential threats targeting vital blockchain components: the network, mining pools, transaction verification mechanisms, smart contracts, and user wallets. By elucidating the intricate challenges and strategic considerations inherent in transitioning to quantum-resistant algorithms, the paper evaluates risks and highlights obstacles in securing blockchain components with quantum-resistant cryptography. It offers a hybrid migration strategy to facilitate a smooth transition from classical to quantum-resistant cryptography. The analysis extends to prominent blockchains such as Bitcoin, Ethereum, Ripple, Litecoin, and Zcash, assessing vulnerable components, potential impacts, and associated STRIDE threats, thereby identifying areas susceptible to quantum attacks. Beyond analysis, the paper provides actionable guidance for designing secure and resilient blockchain ecosystems in the quantum computing era. Recognizing the looming threat of quantum computers, this research advocates for a proactive transition to quantum-resistant blockchain networks. It proposes a tailored security blueprint that strategically fortifies each component against the evolving landscape of quantum-induced cyber threats. Emphasizing the critical need for blockchain stakeholders to adopt proactive measures and implement quantum-resistant solutions, the paper underscores the importance of embracing these insights to navigate the complexities of the quantum era with resilience and confidence.",
        "field": "Digital Wallets",
        "link": "http://arxiv.org/abs/2501.11798v1"
    },
    {
        "id": "2503.10207v1",
        "title": "Efficient Implementation of CRYSTALS-KYBER Key Encapsulation Mechanism on ESP32",
        "authors": [
            "Fabian Segatz",
            "Muhammad Ihsan Al Hafiz"
        ],
        "published": "2025-03-13T09:45:31Z",
        "summary": "Kyber, an IND-CCA2-secure lattice-based post-quantum key-encapsulation mechanism, is the winner of the first post-quantum cryptography standardization process of the US National Institute of Standards and Technology. In this work, we provide an efficient implementation of Kyber on ESP32, a very popular microcontroller for Internet of Things applications. We hand-partition the Kyber algorithm to enable utilization of the ESP32 dual-core architecture, which allows us to speed up its execution by 1.21x (keygen), 1.22x (encaps) and 1.20x (decaps). We also explore the possibility of gaining further improvement by utilizing the ESP32 SHA and AES coprocessor and achieve a culminated speed-up of 1.72x (keygen), 1.84x (encaps) and 1.69x (decaps).",
        "field": "RegTech (Regulatory Technology)",
        "link": "http://arxiv.org/abs/2503.10207v1"
    },
    {
        "id": "2503.10171v1",
        "title": "Verifiable, Efficient and Confidentiality-Preserving Graph Search with Transparency",
        "authors": [
            "Qiuhao Wang",
            "Xu Yang",
            "Yiwei Liu",
            "Saiyu Qi",
            "Hongguang Zhao",
            "Ke Li",
            "Yong Qi"
        ],
        "published": "2025-03-13T08:53:53Z",
        "summary": "Graph databases have garnered extensive attention and research due to their ability to manage relationships between entities efficiently. Today, many graph search services have been outsourced to a third-party server to facilitate storage and computational support. Nevertheless, the outsourcing paradigm may invade the privacy of graphs. PeGraph is the latest scheme achieving encrypted search over social graphs to address the privacy leakage, which maintains two data structures XSet and TSet motivated by the OXT technology to support encrypted conjunctive search. However, PeGraph still exhibits limitations inherent to the underlying OXT. It does not provide transparent search capabilities, suffers from expensive computation and result pattern leakages, and it fails to support search over dynamic encrypted graph database and results verification. In this paper, we propose SecGraph to address the first two limitations, which adopts a novel system architecture that leverages an SGX-enabled cloud server to provide users with secure and transparent search services since the secret key protection and computational overhead have been offloaded to the cloud server. Besides, we design an LDCF-encoded XSet based on the Logarithmic Dynamic Cuckoo Filter to facilitate efficient plaintext computation in trusted memory, effectively mitigating the risks of result pattern leakage and performance degradation due to exceeding the limited trusted memory capacity. Finally, we design a new dynamic version of TSet named Twin-TSet to enable conjunctive search over dynamic encrypted graph database. In order to support verifiable search, we further propose VSecGraph, which utilizes a procedure-oriented verification method to verify all data structures loaded into the trusted memory, thus bypassing the computational overhead associated with the client's local verification.",
        "field": "RegTech (Regulatory Technology)",
        "link": "http://arxiv.org/abs/2503.10171v1"
    },
    {
        "id": "2503.09823v1",
        "title": "Data Traceability for Privacy Alignment",
        "authors": [
            "Kevin Liao",
            "Shreya Thipireddy",
            "Daniel Weitzner"
        ],
        "published": "2025-03-12T20:42:23Z",
        "summary": "This paper offers a new privacy approach for the growing ecosystem of services--ranging from open banking to healthcare--dependent on sensitive personal data sharing between individuals and third-parties. While these services offer significant benefits, individuals want control over their data, transparency regarding how their data is used, and accountability from third-parties for misuse. However, existing legal and technical mechanisms are inadequate for supporting these needs. A comprehensive approach to the modern privacy challenges of accountable third-party data sharing requires a closer alignment of technical system architecture and legal institutional design. In order to achieve this privacy alignment, we extend traditional security threat modeling and analysis to encompass a broader range of privacy notions than has been typically considered. In particular, we introduce the concept of covert-accountability, which addresses adversaries that may act dishonestly but face potential identification and legal consequences. As a concrete instance of this design approach, we present the OTrace protocol, designed to provide traceable, accountable, consumer-control in third-party data sharing ecosystems. OTrace empowers consumers with the knowledge of where their data is, who has it, what it is being used for, and whom it is being shared with. By applying our alignment framework to OTrace, we demonstrate that OTrace's technical affordances can provide more confident, scalable regulatory oversight when combined with complementary legal mechanisms.",
        "field": "RegTech (Regulatory Technology)",
        "link": "http://arxiv.org/abs/2503.09823v1"
    },
    {
        "id": "2503.09666v1",
        "title": "Blockchain-Enabled Management Framework for Federated Coalition Networks",
        "authors": [
            "Jorge Ãlvaro GonzÃ¡lez",
            "Ana MarÃ­a Saiz GarcÃ­a",
            "Victor Monzon Baeza"
        ],
        "published": "2025-03-12T16:59:23Z",
        "summary": "In a globalized and interconnected world, interoperability has become a key concept for advancing tactical scenarios. Federated Coalition Networks (FCN) enable cooperation between entities from multiple nations while allowing each to maintain control over their systems. However, this interoperability necessitates the sharing of increasing amounts of information between different tactical assets, raising the need for higher security measures. Emerging technologies like blockchain drive a revolution in secure communications, paving the way for new tactical scenarios. In this work, we propose a blockchain-based framework to enhance the resilience and security of the management of these networks. We offer a guide to FCN design to help a broad audience understand the military networks in international missions by a use case and key functions applied to a proposed architecture. We evaluate its effectiveness and performance in information encryption to validate this framework.",
        "field": "RegTech (Regulatory Technology)",
        "link": "http://arxiv.org/abs/2503.09666v1"
    },
    {
        "id": "2503.09375v1",
        "title": "Quantum Computing and Cybersecurity Education: A Novel Curriculum for Enhancing Graduate STEM Learning",
        "authors": [
            "Suryansh Upadhyay",
            "Koustubh Phalak",
            "Jungeun Lee",
            "Kathleen Mitchell Hill",
            "Swaroop Ghosh"
        ],
        "published": "2025-03-12T13:26:54Z",
        "summary": "Quantum computing is an emerging paradigm with the potential to transform numerous application areas by addressing problems considered intractable in the classical domain. However, its integration into cyberspace introduces significant security and privacy challenges. The exponential rise in cyber attacks, further complicated by quantum capabilities, poses serious risks to financial systems and national security. The scope of quantum threats extends beyond traditional software, operating system, and network vulnerabilities, necessitating a shift in cybersecurity education. Traditional cybersecurity education, often reliant on didactic methods, lacks hands on, student centered learning experiences necessary to prepare students for these evolving challenges. There is an urgent need for curricula that address both classical and quantum security threats through experiential learning. In this work, we present the design and evaluation of EE 597: Introduction to Hardware Security, a graduate level course integrating hands-on quantum security learning with classical security concepts through simulations and cloud-based quantum hardware. Unlike conventional courses focused on quantum threats to cryptographic systems, EE 597 explores security challenges specific to quantum computing itself. We employ a mixed-methods evaluation using pre and post surveys to assess student learning outcomes and engagement. Results indicate significant improvements in students' understanding of quantum and hardware security, with strong positive feedback on course structure and remote instruction (mean scores: 3.33 to 3.83 on a 4 point scale).",
        "field": "RegTech (Regulatory Technology)",
        "link": "http://arxiv.org/abs/2503.09375v1"
    },
    {
        "id": "2503.11634v1",
        "title": "Translating Between the Common Haar Random State Model and the Unitary Model",
        "authors": [
            "Eli Goldin",
            "Mark Zhandry"
        ],
        "published": "2025-03-14T17:52:48Z",
        "summary": "Black-box separations are a cornerstone of cryptography, indicating barriers to various goals. A recent line of work has explored black-box separations for quantum cryptographic primitives. Namely, a number of separations are known in the Common Haar Random State (CHRS) model, though this model is not considered a complete separation, but rather a starting point. A few very recent works have attempted to lift these separations to a unitary separation, which are considered complete separations. Unfortunately, we find significant errors in some of these lifting results. We prove general conditions under which CHRS separations can be generically lifted, thereby giving simple, modular, and bug-free proofs of complete unitary separations between various quantum primitives. Our techniques allow for simpler proofs of existing separations as well as new separations that were previously only known in the CHRS model.",
        "field": "Synthetic Data Generation",
        "link": "http://arxiv.org/abs/2503.11634v1"
    },
    {
        "id": "2503.09823v1",
        "title": "Data Traceability for Privacy Alignment",
        "authors": [
            "Kevin Liao",
            "Shreya Thipireddy",
            "Daniel Weitzner"
        ],
        "published": "2025-03-12T20:42:23Z",
        "summary": "This paper offers a new privacy approach for the growing ecosystem of services--ranging from open banking to healthcare--dependent on sensitive personal data sharing between individuals and third-parties. While these services offer significant benefits, individuals want control over their data, transparency regarding how their data is used, and accountability from third-parties for misuse. However, existing legal and technical mechanisms are inadequate for supporting these needs. A comprehensive approach to the modern privacy challenges of accountable third-party data sharing requires a closer alignment of technical system architecture and legal institutional design. In order to achieve this privacy alignment, we extend traditional security threat modeling and analysis to encompass a broader range of privacy notions than has been typically considered. In particular, we introduce the concept of covert-accountability, which addresses adversaries that may act dishonestly but face potential identification and legal consequences. As a concrete instance of this design approach, we present the OTrace protocol, designed to provide traceable, accountable, consumer-control in third-party data sharing ecosystems. OTrace empowers consumers with the knowledge of where their data is, who has it, what it is being used for, and whom it is being shared with. By applying our alignment framework to OTrace, we demonstrate that OTrace's technical affordances can provide more confident, scalable regulatory oversight when combined with complementary legal mechanisms.",
        "field": "Explainable AI (XAI) in Banking",
        "link": "http://arxiv.org/abs/2503.09823v1"
    },
    {
        "id": "2503.09586v1",
        "title": "Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot",
        "authors": [
            "Andrew Crossman",
            "Andrew R. Plummer",
            "Chandra Sekharudu",
            "Deepak Warrier",
            "Mohammad Yekrangian"
        ],
        "published": "2025-03-12T17:54:18Z",
        "summary": "We present Auspex - a threat modeling system built using a specialized collection of generative artificial intelligence-based methods that capture threat modeling tradecraft. This new approach, called tradecraft prompting, centers on encoding the on-the-ground knowledge of threat modelers within the prompts that drive a generative AI-based threat modeling system. Auspex employs tradecraft prompts in two processing stages. The first stage centers on ingesting and processing system architecture information using prompts that encode threat modeling tradecraft knowledge pertaining to system decomposition and description. The second stage centers on chaining the resulting system analysis through a collection of prompts that encode tradecraft knowledge on threat identification, classification, and mitigation. The two-stage process yields a threat matrix for a system that specifies threat scenarios, threat types, information security categorizations and potential mitigations. Auspex produces formalized threat model output in minutes, relative to the weeks or months a manual process takes. More broadly, the focus on bespoke tradecraft prompting, as opposed to fine-tuning or agent-based add-ons, makes Auspex a lightweight, flexible, modular, and extensible foundational system capable of addressing the complexity, resource, and standardization limitations of both existing manual and automated threat modeling processes. In this connection, we establish the baseline value of Auspex to threat modelers through an evaluation procedure based on feedback collected from cybersecurity subject matter experts measuring the quality and utility of threat models generated by Auspex on real banking systems. We conclude with a discussion of system performance and plans for enhancements to Auspex.",
        "field": "Explainable AI (XAI) in Banking",
        "link": "http://arxiv.org/abs/2503.09586v1"
    },
    {
        "id": "2503.00070v1",
        "title": "Systematic Review of Cybersecurity in Banking: Evolution from Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain, Policies and Practice",
        "authors": [
            "Tue Nhi Tran"
        ],
        "published": "2025-02-27T14:17:06Z",
        "summary": "Throughout the history from pre-industry 4.0 to post-industry 4.0, cybersecurity at banks has undergone significant changes. Pre-industry 4.0 cyber security at banks relied on individual security methods that were highly manual and had low accuracy. When moving to post-industry 4.0, cybersecurity at banks had a major turning point with security methods that combined different technologies such as Artificial Intelligence (AI), Blockchain, IoT, automating necessary processes and significantly increasing the defence layer for banks. However, along with the development of new technologies, the current challenge of cybersecurity at banks lies in scalability, high costs and resources in both money and time for R&D of defence methods along with the threat of high-tech cybercriminals growing and expanding. This report goes from introducing the importance of cybersecurity at banks, analyzing their management, operational and business objectives, evaluating pre-industry 4.0 technologies used for cybersecurity at banks to assessing post-industry 4.0 technologies focusing on Artificial Intelligence and Blockchain, discussing current policies and practices and ending with discussing key advantages and challenges for 4.0 technologies and recommendations for further developing cybersecurity at banks.",
        "field": "Explainable AI (XAI) in Banking",
        "link": "http://arxiv.org/abs/2503.00070v1"
    },
    {
        "id": "2503.10644v1",
        "title": "Combined climate stress testing of supply-chain networks and the financial system with nation-wide firm-level emission estimates",
        "authors": [
            "Zlata TabachovÃ¡",
            "Christian Diem",
            "Johannes Stangl",
            "AndrÃ¡s Borsos",
            "Stefan Thurner"
        ],
        "published": "2025-02-25T20:25:47Z",
        "summary": "On the way towards carbon neutrality, climate stress testing provides estimates for the physical and transition risks that climate change poses to the economy and the financial system. Missing firm-level CO2 emissions data severely impedes the assessment of transition risks originating from carbon pricing. Based on the individual emissions of all Hungarian firms (410,523), as estimated from their fossil fuel purchases, we conduct a stress test of both actual and hypothetical carbon pricing policies. Using a simple 1:1 economic ABM and introducing the new carbon-to-profit ratio, we identify firms that become unprofitable and default, and estimate the respective loan write-offs. We find that 45% of all companies are directly exposed to carbon pricing. At a price of 45 EUR/t, direct economic losses of 1.3% of total sales and bank equity losses of 1.2% are expected. Secondary default cascades in supply chain networks could increase these losses by 300% to 4000%, depending on firms' ability to substitute essential inputs. To reduce transition risks, firms should reduce their dependence on essential inputs from supply chains with high CO2 exposure. We discuss the implications of different policy implementations on these transition risks.",
        "field": "Explainable AI (XAI) in Banking",
        "link": "http://arxiv.org/abs/2503.10644v1"
    },
    {
        "id": "2502.14766v2",
        "title": "Multi-Layer Deep xVA: Structural Credit Models, Measure Changes and Convergence Analysis",
        "authors": [
            "Kristoffer Andersson",
            "Alessandro Gnoatto"
        ],
        "published": "2025-02-20T17:41:55Z",
        "summary": "We propose a structural default model for portfolio-wide valuation adjustments (xVAs) and represent it as a system of coupled backward stochastic differential equations. The framework is divided into four layers, each capturing a key component: (i) clean values, (ii) initial margin and Collateral Valuation Adjustment (ColVA), (iii) Credit/Debit Valuation Adjustments (CVA/DVA) together with Margin Valuation Adjustment (MVA), and (iv) Funding Valuation Adjustment (FVA). Because these layers depend on one another through collateral and default effects, a naive Monte Carlo approach would require deeply nested simulations, making the problem computationally intractable. To address this challenge, we use an iterative deep BSDE approach, handling each layer sequentially so that earlier outputs serve as inputs to the subsequent layers. Initial margin is computed via deep quantile regression to reflect margin requirements over the Margin Period of Risk. We also adopt a change-of-measure method that highlights rare but significant defaults of the bank or counterparty, ensuring that these events are accurately captured in the training process. We further extend Han and Long's (2020) a posteriori error analysis to BSDEs on bounded domains. Due to the random exit from the domain, we obtain an order of convergence of $\\mathcal{O}(h^{1/4-\\epsilon})$ rather than the usual $\\mathcal{O}(h^{1/2})$. Numerical experiments illustrate that this method drastically reduces computational demands and successfully scales to high-dimensional, non-symmetric portfolios. The results confirm its effectiveness and accuracy, offering a practical alternative to nested Monte Carlo simulations in multi-counterparty xVA analyses.",
        "field": "Explainable AI (XAI) in Banking",
        "link": "http://arxiv.org/abs/2502.14766v2"
    },
    {
        "id": "2503.11619v1",
        "title": "Tit-for-Tat: Safeguarding Large Vision-Language Models Against Jailbreak Attacks via Adversarial Defense",
        "authors": [
            "Shuyang Hao",
            "Yiwei Wang",
            "Bryan Hooi",
            "Ming-Hsuan Yang",
            "Jun Liu",
            "Chengcheng Tang",
            "Zi Huang",
            "Yujun Cai"
        ],
        "published": "2025-03-14T17:39:45Z",
        "summary": "Deploying large vision-language models (LVLMs) introduces a unique vulnerability: susceptibility to malicious attacks via visual inputs. However, existing defense methods suffer from two key limitations: (1) They solely focus on textual defenses, fail to directly address threats in the visual domain where attacks originate, and (2) the additional processing steps often incur significant computational overhead or compromise model performance on benign tasks. Building on these insights, we propose ESIII (Embedding Security Instructions Into Images), a novel methodology for transforming the visual space from a source of vulnerability into an active defense mechanism. Initially, we embed security instructions into defensive images through gradient-based optimization, obtaining security instructions in the visual dimension. Subsequently, we integrate security instructions from visual and textual dimensions with the input query. The collaboration between security instructions from different dimensions ensures comprehensive security protection. Extensive experiments demonstrate that our approach effectively fortifies the robustness of LVLMs against such attacks while preserving their performance on standard benign tasks and incurring an imperceptible increase in time costs.",
        "field": "Privacy-Enhancing Computation",
        "link": "http://arxiv.org/abs/2503.11619v1"
    },
    {
        "id": "2503.11573v1",
        "title": "Synthesizing Access Control Policies using Large Language Models",
        "authors": [
            "Adarsh Vatsa",
            "Pratyush Patel",
            "William Eiers"
        ],
        "published": "2025-03-14T16:40:25Z",
        "summary": "Cloud compute systems allow administrators to write access control policies that govern access to private data. While policies are written in convenient languages, such as AWS Identity and Access Management Policy Language, manually written policies often become complex and error prone. In this paper, we investigate whether and how well Large Language Models (LLMs) can be used to synthesize access control policies. Our investigation focuses on the task of taking an access control request specification and zero-shot prompting LLMs to synthesize a well-formed access control policy which correctly adheres to the request specification. We consider two scenarios, one which the request specification is given as a concrete list of requests to be allowed or denied, and another in which a natural language description is used to specify sets of requests to be allowed or denied. We then argue that for zero-shot prompting, more precise and structured prompts using a syntax based approach are necessary and experimentally show preliminary results validating our approach.",
        "field": "Privacy-Enhancing Computation",
        "link": "http://arxiv.org/abs/2503.11573v1"
    },
    {
        "id": "2503.11404v1",
        "title": "Towards A Correct Usage of Cryptography in Semantic Watermarks for Diffusion Models",
        "authors": [
            "Jonas Thietke",
            "Andreas MÃ¼ller",
            "Denis Lukovnikov",
            "Asja Fischer",
            "Erwin Quiring"
        ],
        "published": "2025-03-14T13:45:46Z",
        "summary": "Semantic watermarking methods enable the direct integration of watermarks into the generation process of latent diffusion models by only modifying the initial latent noise. One line of approaches building on Gaussian Shading relies on cryptographic primitives to steer the sampling process of the latent noise. However, we identify several issues in the usage of cryptographic techniques in Gaussian Shading, particularly in its proof of lossless performance and key management, causing ambiguity in follow-up works, too. In this work, we therefore revisit the cryptographic primitives for semantic watermarking. We introduce a novel, general proof of lossless performance based on IND\\$-CPA security for semantic watermarks. We then discuss the configuration of the cryptographic primitives in semantic watermarks with respect to security, efficiency, and generation quality.",
        "field": "Privacy-Enhancing Computation",
        "link": "http://arxiv.org/abs/2503.11404v1"
    },
    {
        "id": "2503.11216v1",
        "title": "Cross-Platform Benchmarking of the FHE Libraries: Novel Insights into SEAL and Openfhe",
        "authors": [
            "Faneela",
            "Jawad Ahmad",
            "Baraq Ghaleb",
            "Sana Ullah Jan",
            "William J. Buchanan"
        ],
        "published": "2025-03-14T09:08:30Z",
        "summary": "The rapid growth of cloud computing and data-driven applications has amplified privacy concerns, driven by the increasing demand to process sensitive data securely. Homomorphic encryption (HE) has become a vital solution for addressing these concerns by enabling computations on encrypted data without revealing its contents. This paper provides a comprehensive evaluation of two leading HE libraries, SEAL and OpenFHE, examining their performance, usability, and support for prominent HE schemes such as BGV and CKKS. Our analysis highlights computational efficiency, memory usage, and scalability across Linux and Windows platforms, emphasizing their applicability in real-world scenarios. Results reveal that Linux outperforms Windows in computation efficiency, with OpenFHE emerging as the optimal choice across diverse cryptographic settings. This paper provides valuable insights for researchers and practitioners to advance privacy-preserving applications using FHE.",
        "field": "Privacy-Enhancing Computation",
        "link": "http://arxiv.org/abs/2503.11216v1"
    },
    {
        "id": "2503.11185v1",
        "title": "Align in Depth: Defending Jailbreak Attacks via Progressive Answer Detoxification",
        "authors": [
            "Yingjie Zhang",
            "Tong Liu",
            "Zhe Zhao",
            "Guozhu Meng",
            "Kai Chen"
        ],
        "published": "2025-03-14T08:32:12Z",
        "summary": "Large Language Models (LLMs) are vulnerable to jailbreak attacks, which use crafted prompts to elicit toxic responses. These attacks exploit LLMs' difficulty in dynamically detecting harmful intents during the generation process. Traditional safety alignment methods, often relying on the initial few generation steps, are ineffective due to limited computational budget. This paper proposes DEEPALIGN, a robust defense framework that fine-tunes LLMs to progressively detoxify generated content, significantly improving both the computational budget and effectiveness of mitigating harmful generation. Our approach uses a hybrid loss function operating on hidden states to directly improve LLMs' inherent awareness of toxity during generation. Furthermore, we redefine safe responses by generating semantically relevant answers to harmful queries, thereby increasing robustness against representation-mutation attacks. Evaluations across multiple LLMs demonstrate state-of-the-art defense performance against six different attack types, reducing Attack Success Rates by up to two orders of magnitude compared to previous state-of-the-art defense while preserving utility. This work advances LLM safety by addressing limitations of conventional alignment through dynamic, context-aware mitigation.",
        "field": "Privacy-Enhancing Computation",
        "link": "http://arxiv.org/abs/2503.11185v1"
    },
    {
        "id": "2503.09823v1",
        "title": "Data Traceability for Privacy Alignment",
        "authors": [
            "Kevin Liao",
            "Shreya Thipireddy",
            "Daniel Weitzner"
        ],
        "published": "2025-03-12T20:42:23Z",
        "summary": "This paper offers a new privacy approach for the growing ecosystem of services--ranging from open banking to healthcare--dependent on sensitive personal data sharing between individuals and third-parties. While these services offer significant benefits, individuals want control over their data, transparency regarding how their data is used, and accountability from third-parties for misuse. However, existing legal and technical mechanisms are inadequate for supporting these needs. A comprehensive approach to the modern privacy challenges of accountable third-party data sharing requires a closer alignment of technical system architecture and legal institutional design. In order to achieve this privacy alignment, we extend traditional security threat modeling and analysis to encompass a broader range of privacy notions than has been typically considered. In particular, we introduce the concept of covert-accountability, which addresses adversaries that may act dishonestly but face potential identification and legal consequences. As a concrete instance of this design approach, we present the OTrace protocol, designed to provide traceable, accountable, consumer-control in third-party data sharing ecosystems. OTrace empowers consumers with the knowledge of where their data is, who has it, what it is being used for, and whom it is being shared with. By applying our alignment framework to OTrace, we demonstrate that OTrace's technical affordances can provide more confident, scalable regulatory oversight when combined with complementary legal mechanisms.",
        "field": "Generative AI in Banking",
        "link": "http://arxiv.org/abs/2503.09823v1"
    },
    {
        "id": "2503.09586v1",
        "title": "Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot",
        "authors": [
            "Andrew Crossman",
            "Andrew R. Plummer",
            "Chandra Sekharudu",
            "Deepak Warrier",
            "Mohammad Yekrangian"
        ],
        "published": "2025-03-12T17:54:18Z",
        "summary": "We present Auspex - a threat modeling system built using a specialized collection of generative artificial intelligence-based methods that capture threat modeling tradecraft. This new approach, called tradecraft prompting, centers on encoding the on-the-ground knowledge of threat modelers within the prompts that drive a generative AI-based threat modeling system. Auspex employs tradecraft prompts in two processing stages. The first stage centers on ingesting and processing system architecture information using prompts that encode threat modeling tradecraft knowledge pertaining to system decomposition and description. The second stage centers on chaining the resulting system analysis through a collection of prompts that encode tradecraft knowledge on threat identification, classification, and mitigation. The two-stage process yields a threat matrix for a system that specifies threat scenarios, threat types, information security categorizations and potential mitigations. Auspex produces formalized threat model output in minutes, relative to the weeks or months a manual process takes. More broadly, the focus on bespoke tradecraft prompting, as opposed to fine-tuning or agent-based add-ons, makes Auspex a lightweight, flexible, modular, and extensible foundational system capable of addressing the complexity, resource, and standardization limitations of both existing manual and automated threat modeling processes. In this connection, we establish the baseline value of Auspex to threat modelers through an evaluation procedure based on feedback collected from cybersecurity subject matter experts measuring the quality and utility of threat models generated by Auspex on real banking systems. We conclude with a discussion of system performance and plans for enhancements to Auspex.",
        "field": "Generative AI in Banking",
        "link": "http://arxiv.org/abs/2503.09586v1"
    },
    {
        "id": "2503.00070v1",
        "title": "Systematic Review of Cybersecurity in Banking: Evolution from Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain, Policies and Practice",
        "authors": [
            "Tue Nhi Tran"
        ],
        "published": "2025-02-27T14:17:06Z",
        "summary": "Throughout the history from pre-industry 4.0 to post-industry 4.0, cybersecurity at banks has undergone significant changes. Pre-industry 4.0 cyber security at banks relied on individual security methods that were highly manual and had low accuracy. When moving to post-industry 4.0, cybersecurity at banks had a major turning point with security methods that combined different technologies such as Artificial Intelligence (AI), Blockchain, IoT, automating necessary processes and significantly increasing the defence layer for banks. However, along with the development of new technologies, the current challenge of cybersecurity at banks lies in scalability, high costs and resources in both money and time for R&D of defence methods along with the threat of high-tech cybercriminals growing and expanding. This report goes from introducing the importance of cybersecurity at banks, analyzing their management, operational and business objectives, evaluating pre-industry 4.0 technologies used for cybersecurity at banks to assessing post-industry 4.0 technologies focusing on Artificial Intelligence and Blockchain, discussing current policies and practices and ending with discussing key advantages and challenges for 4.0 technologies and recommendations for further developing cybersecurity at banks.",
        "field": "Generative AI in Banking",
        "link": "http://arxiv.org/abs/2503.00070v1"
    },
    {
        "id": "2503.10644v1",
        "title": "Combined climate stress testing of supply-chain networks and the financial system with nation-wide firm-level emission estimates",
        "authors": [
            "Zlata TabachovÃ¡",
            "Christian Diem",
            "Johannes Stangl",
            "AndrÃ¡s Borsos",
            "Stefan Thurner"
        ],
        "published": "2025-02-25T20:25:47Z",
        "summary": "On the way towards carbon neutrality, climate stress testing provides estimates for the physical and transition risks that climate change poses to the economy and the financial system. Missing firm-level CO2 emissions data severely impedes the assessment of transition risks originating from carbon pricing. Based on the individual emissions of all Hungarian firms (410,523), as estimated from their fossil fuel purchases, we conduct a stress test of both actual and hypothetical carbon pricing policies. Using a simple 1:1 economic ABM and introducing the new carbon-to-profit ratio, we identify firms that become unprofitable and default, and estimate the respective loan write-offs. We find that 45% of all companies are directly exposed to carbon pricing. At a price of 45 EUR/t, direct economic losses of 1.3% of total sales and bank equity losses of 1.2% are expected. Secondary default cascades in supply chain networks could increase these losses by 300% to 4000%, depending on firms' ability to substitute essential inputs. To reduce transition risks, firms should reduce their dependence on essential inputs from supply chains with high CO2 exposure. We discuss the implications of different policy implementations on these transition risks.",
        "field": "Generative AI in Banking",
        "link": "http://arxiv.org/abs/2503.10644v1"
    },
    {
        "id": "2502.14766v2",
        "title": "Multi-Layer Deep xVA: Structural Credit Models, Measure Changes and Convergence Analysis",
        "authors": [
            "Kristoffer Andersson",
            "Alessandro Gnoatto"
        ],
        "published": "2025-02-20T17:41:55Z",
        "summary": "We propose a structural default model for portfolio-wide valuation adjustments (xVAs) and represent it as a system of coupled backward stochastic differential equations. The framework is divided into four layers, each capturing a key component: (i) clean values, (ii) initial margin and Collateral Valuation Adjustment (ColVA), (iii) Credit/Debit Valuation Adjustments (CVA/DVA) together with Margin Valuation Adjustment (MVA), and (iv) Funding Valuation Adjustment (FVA). Because these layers depend on one another through collateral and default effects, a naive Monte Carlo approach would require deeply nested simulations, making the problem computationally intractable. To address this challenge, we use an iterative deep BSDE approach, handling each layer sequentially so that earlier outputs serve as inputs to the subsequent layers. Initial margin is computed via deep quantile regression to reflect margin requirements over the Margin Period of Risk. We also adopt a change-of-measure method that highlights rare but significant defaults of the bank or counterparty, ensuring that these events are accurately captured in the training process. We further extend Han and Long's (2020) a posteriori error analysis to BSDEs on bounded domains. Due to the random exit from the domain, we obtain an order of convergence of $\\mathcal{O}(h^{1/4-\\epsilon})$ rather than the usual $\\mathcal{O}(h^{1/2})$. Numerical experiments illustrate that this method drastically reduces computational demands and successfully scales to high-dimensional, non-symmetric portfolios. The results confirm its effectiveness and accuracy, offering a practical alternative to nested Monte Carlo simulations in multi-counterparty xVA analyses.",
        "field": "Generative AI in Banking",
        "link": "http://arxiv.org/abs/2502.14766v2"
    },
    {
        "id": "2503.11573v1",
        "title": "Synthesizing Access Control Policies using Large Language Models",
        "authors": [
            "Adarsh Vatsa",
            "Pratyush Patel",
            "William Eiers"
        ],
        "published": "2025-03-14T16:40:25Z",
        "summary": "Cloud compute systems allow administrators to write access control policies that govern access to private data. While policies are written in convenient languages, such as AWS Identity and Access Management Policy Language, manually written policies often become complex and error prone. In this paper, we investigate whether and how well Large Language Models (LLMs) can be used to synthesize access control policies. Our investigation focuses on the task of taking an access control request specification and zero-shot prompting LLMs to synthesize a well-formed access control policy which correctly adheres to the request specification. We consider two scenarios, one which the request specification is given as a concrete list of requests to be allowed or denied, and another in which a natural language description is used to specify sets of requests to be allowed or denied. We then argue that for zero-shot prompting, more precise and structured prompts using a syntax based approach are necessary and experimentally show preliminary results validating our approach.",
        "field": "Autonomous Banking Systems",
        "link": "http://arxiv.org/abs/2503.11573v1"
    },
    {
        "id": "2503.10809v1",
        "title": "Attacking Multimodal OS Agents with Malicious Image Patches",
        "authors": [
            "Lukas Aichberger",
            "Alasdair Paren",
            "Yarin Gal",
            "Philip Torr",
            "Adel Bibi"
        ],
        "published": "2025-03-13T18:59:12Z",
        "summary": "Recent advances in operating system (OS) agents enable vision-language models to interact directly with the graphical user interface of an OS. These multimodal OS agents autonomously perform computer-based tasks in response to a single prompt via application programming interfaces (APIs). Such APIs typically support low-level operations, including mouse clicks, keyboard inputs, and screenshot captures. We introduce a novel attack vector: malicious image patches (MIPs) that have been adversarially perturbed so that, when captured in a screenshot, they cause an OS agent to perform harmful actions by exploiting specific APIs. For instance, MIPs embedded in desktop backgrounds or shared on social media can redirect an agent to a malicious website, enabling further exploitation. These MIPs generalise across different user requests and screen layouts, and remain effective for multiple OS agents. The existence of such attacks highlights critical security vulnerabilities in OS agents, which should be carefully addressed before their widespread adoption.",
        "field": "Autonomous Banking Systems",
        "link": "http://arxiv.org/abs/2503.10809v1"
    },
    {
        "id": "2503.10793v1",
        "title": "HALURust: Exploiting Hallucinations of Large Language Models to Detect Vulnerabilities in Rust",
        "authors": [
            "Yu Luo",
            "Han Zhou",
            "Mengtao Zhang",
            "Dylan De La Rosa",
            "Hafsa Ahmed",
            "Weifeng Xu",
            "Dianxiang Xu"
        ],
        "published": "2025-03-13T18:38:34Z",
        "summary": "As an emerging programming language, Rust has rapidly gained popularity and recognition among developers due to its strong emphasis on safety. It employs a unique ownership system and safe concurrency practices to ensure robust safety. Despite these safeguards, security in Rust still presents challenges. Since 2018, 442 Rust-related vulnerabilities have been reported in real-world applications. The limited availability of data has resulted in existing vulnerability detection tools performing poorly in real-world scenarios, often failing to adapt to new and complex vulnerabilities. This paper introduces HALURust, a novel framework that leverages hallucinations of large language models (LLMs) to detect vulnerabilities in real-world Rust scenarios. HALURust leverages LLMs' strength in natural language generation by transforming code into detailed vulnerability analysis reports. The key innovation lies in prompting the LLM to always assume the presence of a vulnerability. If the code sample is vulnerable, the LLM provides an accurate analysis; if not, it generates a hallucinated report. By fine-tuning LLMs on these hallucinations, HALURust can effectively distinguish between vulnerable and non-vulnerable code samples. HALURust was evaluated on a dataset of 81 real-world vulnerabilities, covering 447 functions and 18,691 lines of code across 54 applications. It outperformed existing methods, achieving an F1 score of 77.3%, with over 10% improvement. The hallucinated report-based fine-tuning improved detection by 20\\% compared to traditional code-based fine-tuning. Additionally, HALURust effectively adapted to unseen vulnerabilities and other programming languages, demonstrating strong generalization capabilities.",
        "field": "Autonomous Banking Systems",
        "link": "http://arxiv.org/abs/2503.10793v1"
    },
    {
        "id": "2503.10411v1",
        "title": "Public Channel-Based Fair Exchange Protocols with Advertising",
        "authors": [
            "Pierpaolo Della Monica",
            "Ivan Visconti",
            "Andrea Vitaletti",
            "Marco Zecchini"
        ],
        "published": "2025-03-13T14:35:32Z",
        "summary": "Before a fair exchange takes place, there is typically an advertisement phase with the goal of increasing the appeal of possessing a digital asset while keeping it sufficiently hidden. In this work, we give a definition that explicitly combines a fair-exchange protocol with a prior advertising phase. Then, we construct such a fair exchange protocol with aids using zk-SNARKs and relying on mainstream decentralized platforms (i.e., a blockchain with smart contracts like Ethereum and a decentralized storage system like IPFS). Experimental results confirm the practical relevance of our decentralized approach, paving the road towards building decentralized marketplaces where users can, even anonymously, and without direct off-chain communications, effectively advertise and exchange their digital assets as part of a system of enhanced NFTs.",
        "field": "Autonomous Banking Systems",
        "link": "http://arxiv.org/abs/2503.10411v1"
    },
    {
        "id": "2503.10320v1",
        "title": "Combinatorial Designs and Cellular Automata: A Survey",
        "authors": [
            "Luca Manzoni",
            "Luca Mariot",
            "Giuliamaria Menara"
        ],
        "published": "2025-03-13T12:54:49Z",
        "summary": "Cellular Automata (CA) are commonly investigated as a particular type of dynamical systems, defined by shift-invariant local rules. In this paper, we consider instead CA as algebraic systems, focusing on the combinatorial designs induced by their short-term behavior. Specifically, we review the main results published in the literature concerning the construction of mutually orthogonal Latin squares via bipermutive CA, considering both the linear and nonlinear cases. We then survey some significant applications of these results to cryptography, and conclude with a discussion of open problems to be addressed in future research on CA-based combinatorial designs.",
        "field": "Autonomous Banking Systems",
        "link": "http://arxiv.org/abs/2503.10320v1"
    },
    {
        "id": "2503.09823v1",
        "title": "Data Traceability for Privacy Alignment",
        "authors": [
            "Kevin Liao",
            "Shreya Thipireddy",
            "Daniel Weitzner"
        ],
        "published": "2025-03-12T20:42:23Z",
        "summary": "This paper offers a new privacy approach for the growing ecosystem of services--ranging from open banking to healthcare--dependent on sensitive personal data sharing between individuals and third-parties. While these services offer significant benefits, individuals want control over their data, transparency regarding how their data is used, and accountability from third-parties for misuse. However, existing legal and technical mechanisms are inadequate for supporting these needs. A comprehensive approach to the modern privacy challenges of accountable third-party data sharing requires a closer alignment of technical system architecture and legal institutional design. In order to achieve this privacy alignment, we extend traditional security threat modeling and analysis to encompass a broader range of privacy notions than has been typically considered. In particular, we introduce the concept of covert-accountability, which addresses adversaries that may act dishonestly but face potential identification and legal consequences. As a concrete instance of this design approach, we present the OTrace protocol, designed to provide traceable, accountable, consumer-control in third-party data sharing ecosystems. OTrace empowers consumers with the knowledge of where their data is, who has it, what it is being used for, and whom it is being shared with. By applying our alignment framework to OTrace, we demonstrate that OTrace's technical affordances can provide more confident, scalable regulatory oversight when combined with complementary legal mechanisms.",
        "field": "Human-Centric AI in Banking",
        "link": "http://arxiv.org/abs/2503.09823v1"
    },
    {
        "id": "2503.09586v1",
        "title": "Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot",
        "authors": [
            "Andrew Crossman",
            "Andrew R. Plummer",
            "Chandra Sekharudu",
            "Deepak Warrier",
            "Mohammad Yekrangian"
        ],
        "published": "2025-03-12T17:54:18Z",
        "summary": "We present Auspex - a threat modeling system built using a specialized collection of generative artificial intelligence-based methods that capture threat modeling tradecraft. This new approach, called tradecraft prompting, centers on encoding the on-the-ground knowledge of threat modelers within the prompts that drive a generative AI-based threat modeling system. Auspex employs tradecraft prompts in two processing stages. The first stage centers on ingesting and processing system architecture information using prompts that encode threat modeling tradecraft knowledge pertaining to system decomposition and description. The second stage centers on chaining the resulting system analysis through a collection of prompts that encode tradecraft knowledge on threat identification, classification, and mitigation. The two-stage process yields a threat matrix for a system that specifies threat scenarios, threat types, information security categorizations and potential mitigations. Auspex produces formalized threat model output in minutes, relative to the weeks or months a manual process takes. More broadly, the focus on bespoke tradecraft prompting, as opposed to fine-tuning or agent-based add-ons, makes Auspex a lightweight, flexible, modular, and extensible foundational system capable of addressing the complexity, resource, and standardization limitations of both existing manual and automated threat modeling processes. In this connection, we establish the baseline value of Auspex to threat modelers through an evaluation procedure based on feedback collected from cybersecurity subject matter experts measuring the quality and utility of threat models generated by Auspex on real banking systems. We conclude with a discussion of system performance and plans for enhancements to Auspex.",
        "field": "Human-Centric AI in Banking",
        "link": "http://arxiv.org/abs/2503.09586v1"
    },
    {
        "id": "2503.00070v1",
        "title": "Systematic Review of Cybersecurity in Banking: Evolution from Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain, Policies and Practice",
        "authors": [
            "Tue Nhi Tran"
        ],
        "published": "2025-02-27T14:17:06Z",
        "summary": "Throughout the history from pre-industry 4.0 to post-industry 4.0, cybersecurity at banks has undergone significant changes. Pre-industry 4.0 cyber security at banks relied on individual security methods that were highly manual and had low accuracy. When moving to post-industry 4.0, cybersecurity at banks had a major turning point with security methods that combined different technologies such as Artificial Intelligence (AI), Blockchain, IoT, automating necessary processes and significantly increasing the defence layer for banks. However, along with the development of new technologies, the current challenge of cybersecurity at banks lies in scalability, high costs and resources in both money and time for R&D of defence methods along with the threat of high-tech cybercriminals growing and expanding. This report goes from introducing the importance of cybersecurity at banks, analyzing their management, operational and business objectives, evaluating pre-industry 4.0 technologies used for cybersecurity at banks to assessing post-industry 4.0 technologies focusing on Artificial Intelligence and Blockchain, discussing current policies and practices and ending with discussing key advantages and challenges for 4.0 technologies and recommendations for further developing cybersecurity at banks.",
        "field": "Human-Centric AI in Banking",
        "link": "http://arxiv.org/abs/2503.00070v1"
    },
    {
        "id": "2503.10644v1",
        "title": "Combined climate stress testing of supply-chain networks and the financial system with nation-wide firm-level emission estimates",
        "authors": [
            "Zlata TabachovÃ¡",
            "Christian Diem",
            "Johannes Stangl",
            "AndrÃ¡s Borsos",
            "Stefan Thurner"
        ],
        "published": "2025-02-25T20:25:47Z",
        "summary": "On the way towards carbon neutrality, climate stress testing provides estimates for the physical and transition risks that climate change poses to the economy and the financial system. Missing firm-level CO2 emissions data severely impedes the assessment of transition risks originating from carbon pricing. Based on the individual emissions of all Hungarian firms (410,523), as estimated from their fossil fuel purchases, we conduct a stress test of both actual and hypothetical carbon pricing policies. Using a simple 1:1 economic ABM and introducing the new carbon-to-profit ratio, we identify firms that become unprofitable and default, and estimate the respective loan write-offs. We find that 45% of all companies are directly exposed to carbon pricing. At a price of 45 EUR/t, direct economic losses of 1.3% of total sales and bank equity losses of 1.2% are expected. Secondary default cascades in supply chain networks could increase these losses by 300% to 4000%, depending on firms' ability to substitute essential inputs. To reduce transition risks, firms should reduce their dependence on essential inputs from supply chains with high CO2 exposure. We discuss the implications of different policy implementations on these transition risks.",
        "field": "Human-Centric AI in Banking",
        "link": "http://arxiv.org/abs/2503.10644v1"
    },
    {
        "id": "2502.14766v2",
        "title": "Multi-Layer Deep xVA: Structural Credit Models, Measure Changes and Convergence Analysis",
        "authors": [
            "Kristoffer Andersson",
            "Alessandro Gnoatto"
        ],
        "published": "2025-02-20T17:41:55Z",
        "summary": "We propose a structural default model for portfolio-wide valuation adjustments (xVAs) and represent it as a system of coupled backward stochastic differential equations. The framework is divided into four layers, each capturing a key component: (i) clean values, (ii) initial margin and Collateral Valuation Adjustment (ColVA), (iii) Credit/Debit Valuation Adjustments (CVA/DVA) together with Margin Valuation Adjustment (MVA), and (iv) Funding Valuation Adjustment (FVA). Because these layers depend on one another through collateral and default effects, a naive Monte Carlo approach would require deeply nested simulations, making the problem computationally intractable. To address this challenge, we use an iterative deep BSDE approach, handling each layer sequentially so that earlier outputs serve as inputs to the subsequent layers. Initial margin is computed via deep quantile regression to reflect margin requirements over the Margin Period of Risk. We also adopt a change-of-measure method that highlights rare but significant defaults of the bank or counterparty, ensuring that these events are accurately captured in the training process. We further extend Han and Long's (2020) a posteriori error analysis to BSDEs on bounded domains. Due to the random exit from the domain, we obtain an order of convergence of $\\mathcal{O}(h^{1/4-\\epsilon})$ rather than the usual $\\mathcal{O}(h^{1/2})$. Numerical experiments illustrate that this method drastically reduces computational demands and successfully scales to high-dimensional, non-symmetric portfolios. The results confirm its effectiveness and accuracy, offering a practical alternative to nested Monte Carlo simulations in multi-counterparty xVA analyses.",
        "field": "Human-Centric AI in Banking",
        "link": "http://arxiv.org/abs/2502.14766v2"
    },
    {
        "id": "2503.09823v1",
        "title": "Data Traceability for Privacy Alignment",
        "authors": [
            "Kevin Liao",
            "Shreya Thipireddy",
            "Daniel Weitzner"
        ],
        "published": "2025-03-12T20:42:23Z",
        "summary": "This paper offers a new privacy approach for the growing ecosystem of services--ranging from open banking to healthcare--dependent on sensitive personal data sharing between individuals and third-parties. While these services offer significant benefits, individuals want control over their data, transparency regarding how their data is used, and accountability from third-parties for misuse. However, existing legal and technical mechanisms are inadequate for supporting these needs. A comprehensive approach to the modern privacy challenges of accountable third-party data sharing requires a closer alignment of technical system architecture and legal institutional design. In order to achieve this privacy alignment, we extend traditional security threat modeling and analysis to encompass a broader range of privacy notions than has been typically considered. In particular, we introduce the concept of covert-accountability, which addresses adversaries that may act dishonestly but face potential identification and legal consequences. As a concrete instance of this design approach, we present the OTrace protocol, designed to provide traceable, accountable, consumer-control in third-party data sharing ecosystems. OTrace empowers consumers with the knowledge of where their data is, who has it, what it is being used for, and whom it is being shared with. By applying our alignment framework to OTrace, we demonstrate that OTrace's technical affordances can provide more confident, scalable regulatory oversight when combined with complementary legal mechanisms.",
        "field": "Digital Biometrics in Banking",
        "link": "http://arxiv.org/abs/2503.09823v1"
    },
    {
        "id": "2503.09586v1",
        "title": "Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot",
        "authors": [
            "Andrew Crossman",
            "Andrew R. Plummer",
            "Chandra Sekharudu",
            "Deepak Warrier",
            "Mohammad Yekrangian"
        ],
        "published": "2025-03-12T17:54:18Z",
        "summary": "We present Auspex - a threat modeling system built using a specialized collection of generative artificial intelligence-based methods that capture threat modeling tradecraft. This new approach, called tradecraft prompting, centers on encoding the on-the-ground knowledge of threat modelers within the prompts that drive a generative AI-based threat modeling system. Auspex employs tradecraft prompts in two processing stages. The first stage centers on ingesting and processing system architecture information using prompts that encode threat modeling tradecraft knowledge pertaining to system decomposition and description. The second stage centers on chaining the resulting system analysis through a collection of prompts that encode tradecraft knowledge on threat identification, classification, and mitigation. The two-stage process yields a threat matrix for a system that specifies threat scenarios, threat types, information security categorizations and potential mitigations. Auspex produces formalized threat model output in minutes, relative to the weeks or months a manual process takes. More broadly, the focus on bespoke tradecraft prompting, as opposed to fine-tuning or agent-based add-ons, makes Auspex a lightweight, flexible, modular, and extensible foundational system capable of addressing the complexity, resource, and standardization limitations of both existing manual and automated threat modeling processes. In this connection, we establish the baseline value of Auspex to threat modelers through an evaluation procedure based on feedback collected from cybersecurity subject matter experts measuring the quality and utility of threat models generated by Auspex on real banking systems. We conclude with a discussion of system performance and plans for enhancements to Auspex.",
        "field": "Digital Biometrics in Banking",
        "link": "http://arxiv.org/abs/2503.09586v1"
    },
    {
        "id": "2503.00070v1",
        "title": "Systematic Review of Cybersecurity in Banking: Evolution from Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain, Policies and Practice",
        "authors": [
            "Tue Nhi Tran"
        ],
        "published": "2025-02-27T14:17:06Z",
        "summary": "Throughout the history from pre-industry 4.0 to post-industry 4.0, cybersecurity at banks has undergone significant changes. Pre-industry 4.0 cyber security at banks relied on individual security methods that were highly manual and had low accuracy. When moving to post-industry 4.0, cybersecurity at banks had a major turning point with security methods that combined different technologies such as Artificial Intelligence (AI), Blockchain, IoT, automating necessary processes and significantly increasing the defence layer for banks. However, along with the development of new technologies, the current challenge of cybersecurity at banks lies in scalability, high costs and resources in both money and time for R&D of defence methods along with the threat of high-tech cybercriminals growing and expanding. This report goes from introducing the importance of cybersecurity at banks, analyzing their management, operational and business objectives, evaluating pre-industry 4.0 technologies used for cybersecurity at banks to assessing post-industry 4.0 technologies focusing on Artificial Intelligence and Blockchain, discussing current policies and practices and ending with discussing key advantages and challenges for 4.0 technologies and recommendations for further developing cybersecurity at banks.",
        "field": "Digital Biometrics in Banking",
        "link": "http://arxiv.org/abs/2503.00070v1"
    },
    {
        "id": "2503.10644v1",
        "title": "Combined climate stress testing of supply-chain networks and the financial system with nation-wide firm-level emission estimates",
        "authors": [
            "Zlata TabachovÃ¡",
            "Christian Diem",
            "Johannes Stangl",
            "AndrÃ¡s Borsos",
            "Stefan Thurner"
        ],
        "published": "2025-02-25T20:25:47Z",
        "summary": "On the way towards carbon neutrality, climate stress testing provides estimates for the physical and transition risks that climate change poses to the economy and the financial system. Missing firm-level CO2 emissions data severely impedes the assessment of transition risks originating from carbon pricing. Based on the individual emissions of all Hungarian firms (410,523), as estimated from their fossil fuel purchases, we conduct a stress test of both actual and hypothetical carbon pricing policies. Using a simple 1:1 economic ABM and introducing the new carbon-to-profit ratio, we identify firms that become unprofitable and default, and estimate the respective loan write-offs. We find that 45% of all companies are directly exposed to carbon pricing. At a price of 45 EUR/t, direct economic losses of 1.3% of total sales and bank equity losses of 1.2% are expected. Secondary default cascades in supply chain networks could increase these losses by 300% to 4000%, depending on firms' ability to substitute essential inputs. To reduce transition risks, firms should reduce their dependence on essential inputs from supply chains with high CO2 exposure. We discuss the implications of different policy implementations on these transition risks.",
        "field": "Digital Biometrics in Banking",
        "link": "http://arxiv.org/abs/2503.10644v1"
    },
    {
        "id": "2502.14766v2",
        "title": "Multi-Layer Deep xVA: Structural Credit Models, Measure Changes and Convergence Analysis",
        "authors": [
            "Kristoffer Andersson",
            "Alessandro Gnoatto"
        ],
        "published": "2025-02-20T17:41:55Z",
        "summary": "We propose a structural default model for portfolio-wide valuation adjustments (xVAs) and represent it as a system of coupled backward stochastic differential equations. The framework is divided into four layers, each capturing a key component: (i) clean values, (ii) initial margin and Collateral Valuation Adjustment (ColVA), (iii) Credit/Debit Valuation Adjustments (CVA/DVA) together with Margin Valuation Adjustment (MVA), and (iv) Funding Valuation Adjustment (FVA). Because these layers depend on one another through collateral and default effects, a naive Monte Carlo approach would require deeply nested simulations, making the problem computationally intractable. To address this challenge, we use an iterative deep BSDE approach, handling each layer sequentially so that earlier outputs serve as inputs to the subsequent layers. Initial margin is computed via deep quantile regression to reflect margin requirements over the Margin Period of Risk. We also adopt a change-of-measure method that highlights rare but significant defaults of the bank or counterparty, ensuring that these events are accurately captured in the training process. We further extend Han and Long's (2020) a posteriori error analysis to BSDEs on bounded domains. Due to the random exit from the domain, we obtain an order of convergence of $\\mathcal{O}(h^{1/4-\\epsilon})$ rather than the usual $\\mathcal{O}(h^{1/2})$. Numerical experiments illustrate that this method drastically reduces computational demands and successfully scales to high-dimensional, non-symmetric portfolios. The results confirm its effectiveness and accuracy, offering a practical alternative to nested Monte Carlo simulations in multi-counterparty xVA analyses.",
        "field": "Digital Biometrics in Banking",
        "link": "http://arxiv.org/abs/2502.14766v2"
    },
    {
        "id": "2503.10269v1",
        "title": "Targeted Data Poisoning for Black-Box Audio Datasets Ownership Verification",
        "authors": [
            "Wassim Bouaziz",
            "El-Mahdi El-Mhamdi",
            "Nicolas Usunier"
        ],
        "published": "2025-03-13T11:25:25Z",
        "summary": "Protecting the use of audio datasets is a major concern for data owners, particularly with the recent rise of audio deep learning models. While watermarks can be used to protect the data itself, they do not allow to identify a deep learning model trained on a protected dataset. In this paper, we adapt to audio data the recently introduced data taggants approach. Data taggants is a method to verify if a neural network was trained on a protected image dataset with top-$k$ predictions access to the model only. This method relies on a targeted data poisoning scheme by discreetly altering a small fraction (1%) of the dataset as to induce a harmless behavior on out-of-distribution data called keys. We evaluate our method on the Speechcommands and the ESC50 datasets and state of the art transformer models, and show that we can detect the use of the dataset with high confidence without loss of performance. We also show the robustness of our method against common data augmentation techniques, making it a practical method to protect audio datasets.",
        "field": "Quantum Networking",
        "link": "http://arxiv.org/abs/2503.10269v1"
    },
    {
        "id": "2503.10238v1",
        "title": "Post Quantum Migration of Tor",
        "authors": [
            "Denis Berger",
            "Mouad Lemoudden",
            "William J Buchanan"
        ],
        "published": "2025-03-13T10:28:03Z",
        "summary": "Shor's and Grover's algorithms' efficiency and the advancement of quantum computers imply that the cryptography used until now to protect one's privacy is potentially vulnerable to retrospective decryption, also known as \\emph{harvest now, decrypt later} attack in the near future. This dissertation proposes an overview of the cryptographic schemes used by Tor, highlighting the non-quantum-resistant ones and introducing theoretical performance assessment methods of a local Tor network. The measurement is divided into three phases. We will start with benchmarking a local Tor network simulation on constrained devices to isolate the time taken by classical cryptography processes. Secondly, the analysis incorporates existing benchmarks of quantum-secure algorithms and compares these performances on the devices. Lastly, the estimation of overhead is calculated by replacing the measured times of traditional cryptography with the times recorded for Post Quantum Cryptography (PQC) execution within the specified Tor environment. By focusing on the replaceable cryptographic components, using theoretical estimations, and leveraging existing benchmarks, valuable insights into the potential impact of PQC can be obtained without needing to implement it fully.",
        "field": "Quantum Networking",
        "link": "http://arxiv.org/abs/2503.10238v1"
    },
    {
        "id": "2503.10218v1",
        "title": "Moss: Proxy Model-based Full-Weight Aggregation in Federated Learning with Heterogeneous Models",
        "authors": [
            "Yifeng Cai",
            "Ziqi Zhang",
            "Ding Li",
            "Yao Guo",
            "Xiangqun Chen"
        ],
        "published": "2025-03-13T10:00:58Z",
        "summary": "Modern Federated Learning (FL) has become increasingly essential for handling highly heterogeneous mobile devices. Current approaches adopt a partial model aggregation paradigm that leads to sub-optimal model accuracy and higher training overhead. In this paper, we challenge the prevailing notion of partial-model aggregation and propose a novel \"full-weight aggregation\" method named Moss, which aggregates all weights within heterogeneous models to preserve comprehensive knowledge. Evaluation across various applications demonstrates that Moss significantly accelerates training, reduces on-device training time and energy consumption, enhances accuracy, and minimizes network bandwidth utilization when compared to state-of-the-art baselines.",
        "field": "Quantum Networking",
        "link": "http://arxiv.org/abs/2503.10218v1"
    },
    {
        "id": "2503.10058v1",
        "title": "Deep Learning Approaches for Anti-Money Laundering on Mobile Transactions: Review, Framework, and Directions",
        "authors": [
            "Jiani Fan",
            "Lwin Khin Shar",
            "Ruichen Zhang",
            "Ziyao Liu",
            "Wenzhuo Yang",
            "Dusit Niyato",
            "Bomin Mao",
            "Kwok-Yan Lam"
        ],
        "published": "2025-03-13T05:19:44Z",
        "summary": "Money laundering is a financial crime that obscures the origin of illicit funds, necessitating the development and enforcement of anti-money laundering (AML) policies by governments and organizations. The proliferation of mobile payment platforms and smart IoT devices has significantly complicated AML investigations. As payment networks become more interconnected, there is an increasing need for efficient real-time detection to process large volumes of transaction data on heterogeneous payment systems by different operators such as digital currencies, cryptocurrencies and account-based payments. Most of these mobile payment networks are supported by connected devices, many of which are considered loT devices in the FinTech space that constantly generate data. Furthermore, the growing complexity and unpredictability of transaction patterns across these networks contribute to a higher incidence of false positives. While machine learning solutions have the potential to enhance detection efficiency, their application in AML faces unique challenges, such as addressing privacy concerns tied to sensitive financial data and managing the real-world constraint of limited data availability due to data regulations. Existing surveys in the AML literature broadly review machine learning approaches for money laundering detection, but they often lack an in-depth exploration of advanced deep learning techniques - an emerging field with significant potential. To address this gap, this paper conducts a comprehensive review of deep learning solutions and the challenges associated with their use in AML. Additionally, we propose a novel framework that applies the least-privilege principle by integrating machine learning techniques, codifying AML red flags, and employing account profiling to provide context for predictions and enable effective fraud detection under limited data availability....",
        "field": "Quantum Networking",
        "link": "http://arxiv.org/abs/2503.10058v1"
    },
    {
        "id": "2503.09726v1",
        "title": "How Feasible is Augmenting Fake Nodes with Learnable Features as a Counter-strategy against Link Stealing Attacks?",
        "authors": [
            "Mir Imtiaz Mostafiz",
            "Imtiaz Karim",
            "Elisa Bertino"
        ],
        "published": "2025-03-12T18:16:37Z",
        "summary": "Graph Neural Networks (GNNs) are widely used and deployed for graph-based prediction tasks. However, as good as GNNs are for learning graph data, they also come with the risk of privacy leakage. For instance, an attacker can run carefully crafted queries on the GNNs and, from the responses, can infer the existence of an edge between a pair of nodes. This attack, dubbed as a \"link-stealing\" attack, can jeopardize the user's privacy by leaking potentially sensitive information. To protect against this attack, we propose an approach called \"$(N)$ode $(A)$ugmentation for $(R)$estricting $(G)$raphs from $(I)$nsinuating their $(S)$tructure\" ($NARGIS$) and study its feasibility. $NARGIS$ is focused on reshaping the graph embedding space so that the posterior from the GNN model will still provide utility for the prediction task but will introduce ambiguity for the link-stealing attackers. To this end, $NARGIS$ applies spectral clustering on the given graph to facilitate it being augmented with new nodes -- that have learned features instead of fixed ones. It utilizes tri-level optimization for learning parameters for the GNN model, surrogate attacker model, and our defense model (i.e. learnable node features). We extensively evaluate $NARGIS$ on three benchmark citation datasets over eight knowledge availability settings for the attackers. We also evaluate the model fidelity and defense performance on influence-based link inference attacks. Through our studies, we have figured out the best feature of $NARGIS$ -- its superior fidelity-privacy performance trade-off in a significant number of cases. We also have discovered in which cases the model needs to be improved, and proposed ways to integrate different schemes to make the model more robust against link stealing attacks.",
        "field": "Quantum Networking",
        "link": "http://arxiv.org/abs/2503.09726v1"
    },
    {
        "id": "2503.11573v1",
        "title": "Synthesizing Access Control Policies using Large Language Models",
        "authors": [
            "Adarsh Vatsa",
            "Pratyush Patel",
            "William Eiers"
        ],
        "published": "2025-03-14T16:40:25Z",
        "summary": "Cloud compute systems allow administrators to write access control policies that govern access to private data. While policies are written in convenient languages, such as AWS Identity and Access Management Policy Language, manually written policies often become complex and error prone. In this paper, we investigate whether and how well Large Language Models (LLMs) can be used to synthesize access control policies. Our investigation focuses on the task of taking an access control request specification and zero-shot prompting LLMs to synthesize a well-formed access control policy which correctly adheres to the request specification. We consider two scenarios, one which the request specification is given as a concrete list of requests to be allowed or denied, and another in which a natural language description is used to specify sets of requests to be allowed or denied. We then argue that for zero-shot prompting, more precise and structured prompts using a syntax based approach are necessary and experimentally show preliminary results validating our approach.",
        "field": "AI-Driven Personal Finance Management",
        "link": "http://arxiv.org/abs/2503.11573v1"
    },
    {
        "id": "2503.11404v1",
        "title": "Towards A Correct Usage of Cryptography in Semantic Watermarks for Diffusion Models",
        "authors": [
            "Jonas Thietke",
            "Andreas MÃ¼ller",
            "Denis Lukovnikov",
            "Asja Fischer",
            "Erwin Quiring"
        ],
        "published": "2025-03-14T13:45:46Z",
        "summary": "Semantic watermarking methods enable the direct integration of watermarks into the generation process of latent diffusion models by only modifying the initial latent noise. One line of approaches building on Gaussian Shading relies on cryptographic primitives to steer the sampling process of the latent noise. However, we identify several issues in the usage of cryptographic techniques in Gaussian Shading, particularly in its proof of lossless performance and key management, causing ambiguity in follow-up works, too. In this work, we therefore revisit the cryptographic primitives for semantic watermarking. We introduce a novel, general proof of lossless performance based on IND\\$-CPA security for semantic watermarks. We then discuss the configuration of the cryptographic primitives in semantic watermarks with respect to security, efficiency, and generation quality.",
        "field": "AI-Driven Personal Finance Management",
        "link": "http://arxiv.org/abs/2503.11404v1"
    },
    {
        "id": "2503.10171v1",
        "title": "Verifiable, Efficient and Confidentiality-Preserving Graph Search with Transparency",
        "authors": [
            "Qiuhao Wang",
            "Xu Yang",
            "Yiwei Liu",
            "Saiyu Qi",
            "Hongguang Zhao",
            "Ke Li",
            "Yong Qi"
        ],
        "published": "2025-03-13T08:53:53Z",
        "summary": "Graph databases have garnered extensive attention and research due to their ability to manage relationships between entities efficiently. Today, many graph search services have been outsourced to a third-party server to facilitate storage and computational support. Nevertheless, the outsourcing paradigm may invade the privacy of graphs. PeGraph is the latest scheme achieving encrypted search over social graphs to address the privacy leakage, which maintains two data structures XSet and TSet motivated by the OXT technology to support encrypted conjunctive search. However, PeGraph still exhibits limitations inherent to the underlying OXT. It does not provide transparent search capabilities, suffers from expensive computation and result pattern leakages, and it fails to support search over dynamic encrypted graph database and results verification. In this paper, we propose SecGraph to address the first two limitations, which adopts a novel system architecture that leverages an SGX-enabled cloud server to provide users with secure and transparent search services since the secret key protection and computational overhead have been offloaded to the cloud server. Besides, we design an LDCF-encoded XSet based on the Logarithmic Dynamic Cuckoo Filter to facilitate efficient plaintext computation in trusted memory, effectively mitigating the risks of result pattern leakage and performance degradation due to exceeding the limited trusted memory capacity. Finally, we design a new dynamic version of TSet named Twin-TSet to enable conjunctive search over dynamic encrypted graph database. In order to support verifiable search, we further propose VSecGraph, which utilizes a procedure-oriented verification method to verify all data structures loaded into the trusted memory, thus bypassing the computational overhead associated with the client's local verification.",
        "field": "AI-Driven Personal Finance Management",
        "link": "http://arxiv.org/abs/2503.10171v1"
    },
    {
        "id": "2503.10058v1",
        "title": "Deep Learning Approaches for Anti-Money Laundering on Mobile Transactions: Review, Framework, and Directions",
        "authors": [
            "Jiani Fan",
            "Lwin Khin Shar",
            "Ruichen Zhang",
            "Ziyao Liu",
            "Wenzhuo Yang",
            "Dusit Niyato",
            "Bomin Mao",
            "Kwok-Yan Lam"
        ],
        "published": "2025-03-13T05:19:44Z",
        "summary": "Money laundering is a financial crime that obscures the origin of illicit funds, necessitating the development and enforcement of anti-money laundering (AML) policies by governments and organizations. The proliferation of mobile payment platforms and smart IoT devices has significantly complicated AML investigations. As payment networks become more interconnected, there is an increasing need for efficient real-time detection to process large volumes of transaction data on heterogeneous payment systems by different operators such as digital currencies, cryptocurrencies and account-based payments. Most of these mobile payment networks are supported by connected devices, many of which are considered loT devices in the FinTech space that constantly generate data. Furthermore, the growing complexity and unpredictability of transaction patterns across these networks contribute to a higher incidence of false positives. While machine learning solutions have the potential to enhance detection efficiency, their application in AML faces unique challenges, such as addressing privacy concerns tied to sensitive financial data and managing the real-world constraint of limited data availability due to data regulations. Existing surveys in the AML literature broadly review machine learning approaches for money laundering detection, but they often lack an in-depth exploration of advanced deep learning techniques - an emerging field with significant potential. To address this gap, this paper conducts a comprehensive review of deep learning solutions and the challenges associated with their use in AML. Additionally, we propose a novel framework that applies the least-privilege principle by integrating machine learning techniques, codifying AML red flags, and employing account profiling to provide context for predictions and enable effective fraud detection under limited data availability....",
        "field": "AI-Driven Personal Finance Management",
        "link": "http://arxiv.org/abs/2503.10058v1"
    },
    {
        "id": "2503.09934v1",
        "title": "A Pharmacy Benefit Manager Insurance Business Model",
        "authors": [
            "Lawrence W. Abrams"
        ],
        "published": "2025-03-13T01:13:16Z",
        "summary": "It is time to move on from attempts to make the pharmacy benefit manager (PBM) reseller business model more transparent. Time and time again the Big 3 PBMs have developed opaque alternatives to piece-meal 100% pass-through mandates. Time and time again PBMs have demonstrated expertise in finding loopholes in state government disclosure laws. The purpose of this paper is to provide quantitative estimates of two transparent insurance business models as a solution to the PBM agency issue. The key parameter used is an 8% gross profit margin figure disclosed by the Big 3 PBMs themselves. Based on reported drug trend delivered to plans, we use a $1,200 to $1,500 per member per year (PMPY) as the range for this key performance indicator (KPI). We propose that discussions of PBM insurance business models start with the following figures: (1) a fixed premium model with medical loss ratio ranging from 92% to 85%; (2) a fee-for-service model ranging from $96 to $180 PMPY with risk sharing of deviations from a contracted PMPY delivered drug spend.",
        "field": "AI-Driven Personal Finance Management",
        "link": "http://arxiv.org/abs/2503.09934v1"
    },
    {
        "id": "2503.09823v1",
        "title": "Data Traceability for Privacy Alignment",
        "authors": [
            "Kevin Liao",
            "Shreya Thipireddy",
            "Daniel Weitzner"
        ],
        "published": "2025-03-12T20:42:23Z",
        "summary": "This paper offers a new privacy approach for the growing ecosystem of services--ranging from open banking to healthcare--dependent on sensitive personal data sharing between individuals and third-parties. While these services offer significant benefits, individuals want control over their data, transparency regarding how their data is used, and accountability from third-parties for misuse. However, existing legal and technical mechanisms are inadequate for supporting these needs. A comprehensive approach to the modern privacy challenges of accountable third-party data sharing requires a closer alignment of technical system architecture and legal institutional design. In order to achieve this privacy alignment, we extend traditional security threat modeling and analysis to encompass a broader range of privacy notions than has been typically considered. In particular, we introduce the concept of covert-accountability, which addresses adversaries that may act dishonestly but face potential identification and legal consequences. As a concrete instance of this design approach, we present the OTrace protocol, designed to provide traceable, accountable, consumer-control in third-party data sharing ecosystems. OTrace empowers consumers with the knowledge of where their data is, who has it, what it is being used for, and whom it is being shared with. By applying our alignment framework to OTrace, we demonstrate that OTrace's technical affordances can provide more confident, scalable regulatory oversight when combined with complementary legal mechanisms.",
        "field": "Zero-Knowledge Proofs in Banking",
        "link": "http://arxiv.org/abs/2503.09823v1"
    },
    {
        "id": "2503.09586v1",
        "title": "Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot",
        "authors": [
            "Andrew Crossman",
            "Andrew R. Plummer",
            "Chandra Sekharudu",
            "Deepak Warrier",
            "Mohammad Yekrangian"
        ],
        "published": "2025-03-12T17:54:18Z",
        "summary": "We present Auspex - a threat modeling system built using a specialized collection of generative artificial intelligence-based methods that capture threat modeling tradecraft. This new approach, called tradecraft prompting, centers on encoding the on-the-ground knowledge of threat modelers within the prompts that drive a generative AI-based threat modeling system. Auspex employs tradecraft prompts in two processing stages. The first stage centers on ingesting and processing system architecture information using prompts that encode threat modeling tradecraft knowledge pertaining to system decomposition and description. The second stage centers on chaining the resulting system analysis through a collection of prompts that encode tradecraft knowledge on threat identification, classification, and mitigation. The two-stage process yields a threat matrix for a system that specifies threat scenarios, threat types, information security categorizations and potential mitigations. Auspex produces formalized threat model output in minutes, relative to the weeks or months a manual process takes. More broadly, the focus on bespoke tradecraft prompting, as opposed to fine-tuning or agent-based add-ons, makes Auspex a lightweight, flexible, modular, and extensible foundational system capable of addressing the complexity, resource, and standardization limitations of both existing manual and automated threat modeling processes. In this connection, we establish the baseline value of Auspex to threat modelers through an evaluation procedure based on feedback collected from cybersecurity subject matter experts measuring the quality and utility of threat models generated by Auspex on real banking systems. We conclude with a discussion of system performance and plans for enhancements to Auspex.",
        "field": "Zero-Knowledge Proofs in Banking",
        "link": "http://arxiv.org/abs/2503.09586v1"
    },
    {
        "id": "2503.00070v1",
        "title": "Systematic Review of Cybersecurity in Banking: Evolution from Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain, Policies and Practice",
        "authors": [
            "Tue Nhi Tran"
        ],
        "published": "2025-02-27T14:17:06Z",
        "summary": "Throughout the history from pre-industry 4.0 to post-industry 4.0, cybersecurity at banks has undergone significant changes. Pre-industry 4.0 cyber security at banks relied on individual security methods that were highly manual and had low accuracy. When moving to post-industry 4.0, cybersecurity at banks had a major turning point with security methods that combined different technologies such as Artificial Intelligence (AI), Blockchain, IoT, automating necessary processes and significantly increasing the defence layer for banks. However, along with the development of new technologies, the current challenge of cybersecurity at banks lies in scalability, high costs and resources in both money and time for R&D of defence methods along with the threat of high-tech cybercriminals growing and expanding. This report goes from introducing the importance of cybersecurity at banks, analyzing their management, operational and business objectives, evaluating pre-industry 4.0 technologies used for cybersecurity at banks to assessing post-industry 4.0 technologies focusing on Artificial Intelligence and Blockchain, discussing current policies and practices and ending with discussing key advantages and challenges for 4.0 technologies and recommendations for further developing cybersecurity at banks.",
        "field": "Zero-Knowledge Proofs in Banking",
        "link": "http://arxiv.org/abs/2503.00070v1"
    },
    {
        "id": "2503.10644v1",
        "title": "Combined climate stress testing of supply-chain networks and the financial system with nation-wide firm-level emission estimates",
        "authors": [
            "Zlata TabachovÃ¡",
            "Christian Diem",
            "Johannes Stangl",
            "AndrÃ¡s Borsos",
            "Stefan Thurner"
        ],
        "published": "2025-02-25T20:25:47Z",
        "summary": "On the way towards carbon neutrality, climate stress testing provides estimates for the physical and transition risks that climate change poses to the economy and the financial system. Missing firm-level CO2 emissions data severely impedes the assessment of transition risks originating from carbon pricing. Based on the individual emissions of all Hungarian firms (410,523), as estimated from their fossil fuel purchases, we conduct a stress test of both actual and hypothetical carbon pricing policies. Using a simple 1:1 economic ABM and introducing the new carbon-to-profit ratio, we identify firms that become unprofitable and default, and estimate the respective loan write-offs. We find that 45% of all companies are directly exposed to carbon pricing. At a price of 45 EUR/t, direct economic losses of 1.3% of total sales and bank equity losses of 1.2% are expected. Secondary default cascades in supply chain networks could increase these losses by 300% to 4000%, depending on firms' ability to substitute essential inputs. To reduce transition risks, firms should reduce their dependence on essential inputs from supply chains with high CO2 exposure. We discuss the implications of different policy implementations on these transition risks.",
        "field": "Zero-Knowledge Proofs in Banking",
        "link": "http://arxiv.org/abs/2503.10644v1"
    },
    {
        "id": "2502.14766v2",
        "title": "Multi-Layer Deep xVA: Structural Credit Models, Measure Changes and Convergence Analysis",
        "authors": [
            "Kristoffer Andersson",
            "Alessandro Gnoatto"
        ],
        "published": "2025-02-20T17:41:55Z",
        "summary": "We propose a structural default model for portfolio-wide valuation adjustments (xVAs) and represent it as a system of coupled backward stochastic differential equations. The framework is divided into four layers, each capturing a key component: (i) clean values, (ii) initial margin and Collateral Valuation Adjustment (ColVA), (iii) Credit/Debit Valuation Adjustments (CVA/DVA) together with Margin Valuation Adjustment (MVA), and (iv) Funding Valuation Adjustment (FVA). Because these layers depend on one another through collateral and default effects, a naive Monte Carlo approach would require deeply nested simulations, making the problem computationally intractable. To address this challenge, we use an iterative deep BSDE approach, handling each layer sequentially so that earlier outputs serve as inputs to the subsequent layers. Initial margin is computed via deep quantile regression to reflect margin requirements over the Margin Period of Risk. We also adopt a change-of-measure method that highlights rare but significant defaults of the bank or counterparty, ensuring that these events are accurately captured in the training process. We further extend Han and Long's (2020) a posteriori error analysis to BSDEs on bounded domains. Due to the random exit from the domain, we obtain an order of convergence of $\\mathcal{O}(h^{1/4-\\epsilon})$ rather than the usual $\\mathcal{O}(h^{1/2})$. Numerical experiments illustrate that this method drastically reduces computational demands and successfully scales to high-dimensional, non-symmetric portfolios. The results confirm its effectiveness and accuracy, offering a practical alternative to nested Monte Carlo simulations in multi-counterparty xVA analyses.",
        "field": "Zero-Knowledge Proofs in Banking",
        "link": "http://arxiv.org/abs/2502.14766v2"
    },
    {
        "id": "2503.11634v1",
        "title": "Translating Between the Common Haar Random State Model and the Unitary Model",
        "authors": [
            "Eli Goldin",
            "Mark Zhandry"
        ],
        "published": "2025-03-14T17:52:48Z",
        "summary": "Black-box separations are a cornerstone of cryptography, indicating barriers to various goals. A recent line of work has explored black-box separations for quantum cryptographic primitives. Namely, a number of separations are known in the Common Haar Random State (CHRS) model, though this model is not considered a complete separation, but rather a starting point. A few very recent works have attempted to lift these separations to a unitary separation, which are considered complete separations. Unfortunately, we find significant errors in some of these lifting results. We prove general conditions under which CHRS separations can be generically lifted, thereby giving simple, modular, and bug-free proofs of complete unitary separations between various quantum primitives. Our techniques allow for simpler proofs of existing separations as well as new separations that were previously only known in the CHRS model.",
        "field": "Quantum-Resistant Cryptography",
        "link": "http://arxiv.org/abs/2503.11634v1"
    },
    {
        "id": "2503.11619v1",
        "title": "Tit-for-Tat: Safeguarding Large Vision-Language Models Against Jailbreak Attacks via Adversarial Defense",
        "authors": [
            "Shuyang Hao",
            "Yiwei Wang",
            "Bryan Hooi",
            "Ming-Hsuan Yang",
            "Jun Liu",
            "Chengcheng Tang",
            "Zi Huang",
            "Yujun Cai"
        ],
        "published": "2025-03-14T17:39:45Z",
        "summary": "Deploying large vision-language models (LVLMs) introduces a unique vulnerability: susceptibility to malicious attacks via visual inputs. However, existing defense methods suffer from two key limitations: (1) They solely focus on textual defenses, fail to directly address threats in the visual domain where attacks originate, and (2) the additional processing steps often incur significant computational overhead or compromise model performance on benign tasks. Building on these insights, we propose ESIII (Embedding Security Instructions Into Images), a novel methodology for transforming the visual space from a source of vulnerability into an active defense mechanism. Initially, we embed security instructions into defensive images through gradient-based optimization, obtaining security instructions in the visual dimension. Subsequently, we integrate security instructions from visual and textual dimensions with the input query. The collaboration between security instructions from different dimensions ensures comprehensive security protection. Extensive experiments demonstrate that our approach effectively fortifies the robustness of LVLMs against such attacks while preserving their performance on standard benign tasks and incurring an imperceptible increase in time costs.",
        "field": "Quantum-Resistant Cryptography",
        "link": "http://arxiv.org/abs/2503.11619v1"
    },
    {
        "id": "2503.11573v1",
        "title": "Synthesizing Access Control Policies using Large Language Models",
        "authors": [
            "Adarsh Vatsa",
            "Pratyush Patel",
            "William Eiers"
        ],
        "published": "2025-03-14T16:40:25Z",
        "summary": "Cloud compute systems allow administrators to write access control policies that govern access to private data. While policies are written in convenient languages, such as AWS Identity and Access Management Policy Language, manually written policies often become complex and error prone. In this paper, we investigate whether and how well Large Language Models (LLMs) can be used to synthesize access control policies. Our investigation focuses on the task of taking an access control request specification and zero-shot prompting LLMs to synthesize a well-formed access control policy which correctly adheres to the request specification. We consider two scenarios, one which the request specification is given as a concrete list of requests to be allowed or denied, and another in which a natural language description is used to specify sets of requests to be allowed or denied. We then argue that for zero-shot prompting, more precise and structured prompts using a syntax based approach are necessary and experimentally show preliminary results validating our approach.",
        "field": "Quantum-Resistant Cryptography",
        "link": "http://arxiv.org/abs/2503.11573v1"
    },
    {
        "id": "2503.11508v1",
        "title": "Leveraging Angle of Arrival Estimation against Impersonation Attacks in Physical Layer Authentication",
        "authors": [
            "Thuy M. Pham",
            "Linda Senigagliesi",
            "Marco Baldi",
            "Rafael F. Schaefer",
            "Gerhard P. Fettweis",
            "Arsenia Chorti"
        ],
        "published": "2025-03-14T15:29:55Z",
        "summary": "In this paper, we investigate the utilization of the angle of arrival (AoA) as a feature for robust physical layer authentication (PLA). While most of the existing approaches to PLA focus on common features of the physical layer of communication channels, such as channel frequency response, channel impulse response or received signal strength, the use of AoA in this domain has not yet been studied in depth, particularly regarding the ability to thwart impersonation attacks. In this work, we demonstrate that an impersonation attack targeting AoA based PLA is only feasible under strict conditions on the attacker's location and hardware capabilities, which highlights the AoA's potential as a strong feature for PLA. We extend previous works considering a single-antenna attacker to the case of a multiple-antenna attacker, and we develop a theoretical characterization of the conditions in which a successful impersonation attack can be mounted. Furthermore, we leverage extensive simulations in support of theoretical analyses, to validate the robustness of AoA-based PLA.",
        "field": "Quantum-Resistant Cryptography",
        "link": "http://arxiv.org/abs/2503.11508v1"
    },
    {
        "id": "2503.11404v1",
        "title": "Towards A Correct Usage of Cryptography in Semantic Watermarks for Diffusion Models",
        "authors": [
            "Jonas Thietke",
            "Andreas MÃ¼ller",
            "Denis Lukovnikov",
            "Asja Fischer",
            "Erwin Quiring"
        ],
        "published": "2025-03-14T13:45:46Z",
        "summary": "Semantic watermarking methods enable the direct integration of watermarks into the generation process of latent diffusion models by only modifying the initial latent noise. One line of approaches building on Gaussian Shading relies on cryptographic primitives to steer the sampling process of the latent noise. However, we identify several issues in the usage of cryptographic techniques in Gaussian Shading, particularly in its proof of lossless performance and key management, causing ambiguity in follow-up works, too. In this work, we therefore revisit the cryptographic primitives for semantic watermarking. We introduce a novel, general proof of lossless performance based on IND\\$-CPA security for semantic watermarks. We then discuss the configuration of the cryptographic primitives in semantic watermarks with respect to security, efficiency, and generation quality.",
        "field": "Quantum-Resistant Cryptography",
        "link": "http://arxiv.org/abs/2503.11404v1"
    },
    {
        "id": "2503.11185v1",
        "title": "Align in Depth: Defending Jailbreak Attacks via Progressive Answer Detoxification",
        "authors": [
            "Yingjie Zhang",
            "Tong Liu",
            "Zhe Zhao",
            "Guozhu Meng",
            "Kai Chen"
        ],
        "published": "2025-03-14T08:32:12Z",
        "summary": "Large Language Models (LLMs) are vulnerable to jailbreak attacks, which use crafted prompts to elicit toxic responses. These attacks exploit LLMs' difficulty in dynamically detecting harmful intents during the generation process. Traditional safety alignment methods, often relying on the initial few generation steps, are ineffective due to limited computational budget. This paper proposes DEEPALIGN, a robust defense framework that fine-tunes LLMs to progressively detoxify generated content, significantly improving both the computational budget and effectiveness of mitigating harmful generation. Our approach uses a hybrid loss function operating on hidden states to directly improve LLMs' inherent awareness of toxity during generation. Furthermore, we redefine safe responses by generating semantically relevant answers to harmful queries, thereby increasing robustness against representation-mutation attacks. Evaluations across multiple LLMs demonstrate state-of-the-art defense performance against six different attack types, reducing Attack Success Rates by up to two orders of magnitude compared to previous state-of-the-art defense while preserving utility. This work advances LLM safety by addressing limitations of conventional alignment through dynamic, context-aware mitigation.",
        "field": "AI-Driven Fraud Detection",
        "link": "http://arxiv.org/abs/2503.11185v1"
    },
    {
        "id": "2503.10944v1",
        "title": "Phishsense-1B: A Technical Perspective on an AI-Powered Phishing Detection Model",
        "authors": [
            "SE Blake"
        ],
        "published": "2025-03-13T23:03:09Z",
        "summary": "Phishing is a persistent cybersecurity threat in today's digital landscape. This paper introduces Phishsense-1B, a refined version of the Llama-Guard-3-1B model, specifically tailored for phishing detection and reasoning. This adaptation utilizes Low-Rank Adaptation (LoRA) and the GuardReasoner finetuning methodology. We outline our LoRA-based fine-tuning process, describe the balanced dataset comprising phishing and benign emails, and highlight significant performance improvements over the original model. Our findings indicate that Phishsense-1B achieves an impressive 97.5% accuracy on a custom dataset and maintains strong performance with 70% accuracy on a challenging real-world dataset. This performance notably surpasses both unadapted models and BERT-based detectors. Additionally, we examine current state-of-the-art detection methods, compare prompt-engineering with fine-tuning strategies, and explore potential deployment scenarios.",
        "field": "AI-Driven Fraud Detection",
        "link": "http://arxiv.org/abs/2503.10944v1"
    },
    {
        "id": "2503.10793v1",
        "title": "HALURust: Exploiting Hallucinations of Large Language Models to Detect Vulnerabilities in Rust",
        "authors": [
            "Yu Luo",
            "Han Zhou",
            "Mengtao Zhang",
            "Dylan De La Rosa",
            "Hafsa Ahmed",
            "Weifeng Xu",
            "Dianxiang Xu"
        ],
        "published": "2025-03-13T18:38:34Z",
        "summary": "As an emerging programming language, Rust has rapidly gained popularity and recognition among developers due to its strong emphasis on safety. It employs a unique ownership system and safe concurrency practices to ensure robust safety. Despite these safeguards, security in Rust still presents challenges. Since 2018, 442 Rust-related vulnerabilities have been reported in real-world applications. The limited availability of data has resulted in existing vulnerability detection tools performing poorly in real-world scenarios, often failing to adapt to new and complex vulnerabilities. This paper introduces HALURust, a novel framework that leverages hallucinations of large language models (LLMs) to detect vulnerabilities in real-world Rust scenarios. HALURust leverages LLMs' strength in natural language generation by transforming code into detailed vulnerability analysis reports. The key innovation lies in prompting the LLM to always assume the presence of a vulnerability. If the code sample is vulnerable, the LLM provides an accurate analysis; if not, it generates a hallucinated report. By fine-tuning LLMs on these hallucinations, HALURust can effectively distinguish between vulnerable and non-vulnerable code samples. HALURust was evaluated on a dataset of 81 real-world vulnerabilities, covering 447 functions and 18,691 lines of code across 54 applications. It outperformed existing methods, achieving an F1 score of 77.3%, with over 10% improvement. The hallucinated report-based fine-tuning improved detection by 20\\% compared to traditional code-based fine-tuning. Additionally, HALURust effectively adapted to unseen vulnerabilities and other programming languages, demonstrating strong generalization capabilities.",
        "field": "AI-Driven Fraud Detection",
        "link": "http://arxiv.org/abs/2503.10793v1"
    },
    {
        "id": "2503.10269v1",
        "title": "Targeted Data Poisoning for Black-Box Audio Datasets Ownership Verification",
        "authors": [
            "Wassim Bouaziz",
            "El-Mahdi El-Mhamdi",
            "Nicolas Usunier"
        ],
        "published": "2025-03-13T11:25:25Z",
        "summary": "Protecting the use of audio datasets is a major concern for data owners, particularly with the recent rise of audio deep learning models. While watermarks can be used to protect the data itself, they do not allow to identify a deep learning model trained on a protected dataset. In this paper, we adapt to audio data the recently introduced data taggants approach. Data taggants is a method to verify if a neural network was trained on a protected image dataset with top-$k$ predictions access to the model only. This method relies on a targeted data poisoning scheme by discreetly altering a small fraction (1%) of the dataset as to induce a harmless behavior on out-of-distribution data called keys. We evaluate our method on the Speechcommands and the ESC50 datasets and state of the art transformer models, and show that we can detect the use of the dataset with high confidence without loss of performance. We also show the robustness of our method against common data augmentation techniques, making it a practical method to protect audio datasets.",
        "field": "AI-Driven Fraud Detection",
        "link": "http://arxiv.org/abs/2503.10269v1"
    },
    {
        "id": "2503.10255v1",
        "title": "An Open-RAN Testbed for Detecting and Mitigating Radio-Access Anomalies",
        "authors": [
            "Hanna Bogucka",
            "Marcin Hoffmann",
            "PaweÅ Kryszkiewicz",
            "Åukasz KuÅacz"
        ],
        "published": "2025-03-13T11:10:29Z",
        "summary": "This paper presents the Open Radio Access Net-work (O-RAN) testbed for secure radio access. We discuss radio-originating attack detection and mitigation methods based on anomaly detection and how they can be implemented as specialized applications (xApps) in this testbed. We also pre-sent illustrating results of the methods applied in real-world scenarios and implementations.",
        "field": "AI-Driven Fraud Detection",
        "link": "http://arxiv.org/abs/2503.10255v1"
    },
    {
        "id": "2503.11404v1",
        "title": "Towards A Correct Usage of Cryptography in Semantic Watermarks for Diffusion Models",
        "authors": [
            "Jonas Thietke",
            "Andreas MÃ¼ller",
            "Denis Lukovnikov",
            "Asja Fischer",
            "Erwin Quiring"
        ],
        "published": "2025-03-14T13:45:46Z",
        "summary": "Semantic watermarking methods enable the direct integration of watermarks into the generation process of latent diffusion models by only modifying the initial latent noise. One line of approaches building on Gaussian Shading relies on cryptographic primitives to steer the sampling process of the latent noise. However, we identify several issues in the usage of cryptographic techniques in Gaussian Shading, particularly in its proof of lossless performance and key management, causing ambiguity in follow-up works, too. In this work, we therefore revisit the cryptographic primitives for semantic watermarking. We introduce a novel, general proof of lossless performance based on IND\\$-CPA security for semantic watermarks. We then discuss the configuration of the cryptographic primitives in semantic watermarks with respect to security, efficiency, and generation quality.",
        "field": "Synthetic Data Generation",
        "link": "http://arxiv.org/abs/2503.11404v1"
    },
    {
        "id": "2503.11185v1",
        "title": "Align in Depth: Defending Jailbreak Attacks via Progressive Answer Detoxification",
        "authors": [
            "Yingjie Zhang",
            "Tong Liu",
            "Zhe Zhao",
            "Guozhu Meng",
            "Kai Chen"
        ],
        "published": "2025-03-14T08:32:12Z",
        "summary": "Large Language Models (LLMs) are vulnerable to jailbreak attacks, which use crafted prompts to elicit toxic responses. These attacks exploit LLMs' difficulty in dynamically detecting harmful intents during the generation process. Traditional safety alignment methods, often relying on the initial few generation steps, are ineffective due to limited computational budget. This paper proposes DEEPALIGN, a robust defense framework that fine-tunes LLMs to progressively detoxify generated content, significantly improving both the computational budget and effectiveness of mitigating harmful generation. Our approach uses a hybrid loss function operating on hidden states to directly improve LLMs' inherent awareness of toxity during generation. Furthermore, we redefine safe responses by generating semantically relevant answers to harmful queries, thereby increasing robustness against representation-mutation attacks. Evaluations across multiple LLMs demonstrate state-of-the-art defense performance against six different attack types, reducing Attack Success Rates by up to two orders of magnitude compared to previous state-of-the-art defense while preserving utility. This work advances LLM safety by addressing limitations of conventional alignment through dynamic, context-aware mitigation.",
        "field": "Synthetic Data Generation",
        "link": "http://arxiv.org/abs/2503.11185v1"
    },
    {
        "id": "2503.11053v1",
        "title": "Pricing American Parisian Options under General Time-Inhomogeneous Markov Models",
        "authors": [
            "Yuhao Liu",
            "Nian Yang",
            "Gongqiu Zhang"
        ],
        "published": "2025-03-14T03:45:18Z",
        "summary": "This paper develops general approaches for pricing various types of American-style Parisian options (down-in/-out, perpetual/finite-maturity) with general payoff functions based on continuous-time Markov chain (CTMC) approximation under general 1D time-inhomogeneous Markov models. For the down-in types, by conditioning on the Parisian stopping time, we reduce the pricing problem to that of a series of vanilla American options with different maturities and their prices integrated with the distribution function of the Parisian stopping time yield the American Parisian down-in option price. This facilitates an efficient application of CTMC approximation to obtain the approximate option price by calculating the required quantities. For the perpetual down-in cases under time-homogeneous models, significant computational cost can be reduced. The down-out cases are more complicated, for which we use the state augmentation approach to record the excursion duration and then the approximate option price is obtained by solving a series of variational inequalities recursively with the Lemke's pivoting method. We show the convergence of CTMC approximation for all the types of American Parisian options under general time-inhomogeneous Markov models, and the accuracy and efficiency of our algorithms are confirmed with extensive numerical experiments.",
        "field": "Synthetic Data Generation",
        "link": "http://arxiv.org/abs/2503.11053v1"
    },
    {
        "id": "2503.10793v1",
        "title": "HALURust: Exploiting Hallucinations of Large Language Models to Detect Vulnerabilities in Rust",
        "authors": [
            "Yu Luo",
            "Han Zhou",
            "Mengtao Zhang",
            "Dylan De La Rosa",
            "Hafsa Ahmed",
            "Weifeng Xu",
            "Dianxiang Xu"
        ],
        "published": "2025-03-13T18:38:34Z",
        "summary": "As an emerging programming language, Rust has rapidly gained popularity and recognition among developers due to its strong emphasis on safety. It employs a unique ownership system and safe concurrency practices to ensure robust safety. Despite these safeguards, security in Rust still presents challenges. Since 2018, 442 Rust-related vulnerabilities have been reported in real-world applications. The limited availability of data has resulted in existing vulnerability detection tools performing poorly in real-world scenarios, often failing to adapt to new and complex vulnerabilities. This paper introduces HALURust, a novel framework that leverages hallucinations of large language models (LLMs) to detect vulnerabilities in real-world Rust scenarios. HALURust leverages LLMs' strength in natural language generation by transforming code into detailed vulnerability analysis reports. The key innovation lies in prompting the LLM to always assume the presence of a vulnerability. If the code sample is vulnerable, the LLM provides an accurate analysis; if not, it generates a hallucinated report. By fine-tuning LLMs on these hallucinations, HALURust can effectively distinguish between vulnerable and non-vulnerable code samples. HALURust was evaluated on a dataset of 81 real-world vulnerabilities, covering 447 functions and 18,691 lines of code across 54 applications. It outperformed existing methods, achieving an F1 score of 77.3%, with over 10% improvement. The hallucinated report-based fine-tuning improved detection by 20\\% compared to traditional code-based fine-tuning. Additionally, HALURust effectively adapted to unseen vulnerabilities and other programming languages, demonstrating strong generalization capabilities.",
        "field": "Synthetic Data Generation",
        "link": "http://arxiv.org/abs/2503.10793v1"
    }
]