[
    {
        "id": "2503.14462v1",
        "title": "Blockchain with proof of quantum work",
        "authors": [
            "Mohammad H. Amin",
            "Jack Raymond",
            "Daniel Kinn",
            "Firas Hamze",
            "Kelsey Hamer",
            "Joel Pasvolsky",
            "William Bernoudy",
            "Andrew D. King",
            "Samuel Kortas"
        ],
        "published": "2025-03-18T17:37:22Z",
        "summary": "We propose a blockchain architecture in which mining requires a quantum computer. The consensus mechanism is based on proof of quantum work, a quantum-enhanced alternative to traditional proof of work that leverages quantum supremacy to make mining intractable for classical computers. We have refined the blockchain framework to incorporate the probabilistic nature of quantum mechanics, ensuring stability against sampling errors and hardware inaccuracies. To validate our approach, we implemented a prototype blockchain on four D-Wave$^{\\rm TM}$ quantum annealing processors geographically distributed within North America, demonstrating stable operation across hundreds of thousands of quantum hashing operations. Our experimental protocol follows the same approach used in the recent demonstration of quantum supremacy [1], ensuring that classical computers cannot efficiently perform the same computation task. By replacing classical machines with quantum systems for mining, it is possible to significantly reduce the energy consumption and environmental impact traditionally associated with blockchain mining. Beyond serving as a proof of concept for a meaningful application of quantum computing, this work highlights the potential for other near-term quantum computing applications using existing technology.",
        "field": "Quantum Computing",
        "link": "http://arxiv.org/abs/2503.14462v1"
    },
    {
        "id": "2503.14279v1",
        "title": "Anti-Tamper Radio meets Reconfigurable Intelligent Surface for System-Level Tamper Detection",
        "authors": [
            "Maryam Shaygan Tabar",
            "Johannes Kortz",
            "Paul Staat",
            "Harald Elders-Boll",
            "Christof Paar",
            "Christian Zenger"
        ],
        "published": "2025-03-18T14:18:31Z",
        "summary": "Many computing systems need to be protected against physical attacks using active tamper detection based on sensors. One technical solution is to employ an ATR (Anti-Tamper Radio) approach, analyzing the radio wave propagation effects within a protected device to detect unauthorized physical alterations. However, ATR systems face key challenges in terms of susceptibility to signal manipulation attacks, limited reliability due to environmental noise, and regulatory constraints from wide bandwidth usage. In this work, we propose and experimentally evaluate an ATR system complemented by an RIS to dynamically reconfigure the wireless propagation environment. We show that this approach can enhance resistance against signal manipulation attacks, reduce bandwidth requirements from several~GHz down to as low as 20 MHz, and improve robustness to environmental disturbances such as internal fan movements. Our work demonstrates that RIS integration can strengthen the ATR performance to enhance security, sensitivity, and robustness, recognizing the potential of smart radio environments for ATR-based tamper detection",
        "field": "Quantum Computing",
        "link": "http://arxiv.org/abs/2503.14279v1"
    },
    {
        "id": "2503.14213v1",
        "title": "Rolling Forward: Enhancing LightGCN with Causal Graph Convolution for Credit Bond Recommendation",
        "authors": [
            "Ashraf Ghiye",
            "Baptiste Barreau",
            "Laurent Carlier",
            "Michalis Vazirgiannis"
        ],
        "published": "2025-03-18T12:47:01Z",
        "summary": "Graph Neural Networks have significantly advanced research in recommender systems over the past few years. These methods typically capture global interests using aggregated past interactions and rely on static embeddings of users and items over extended periods of time. While effective in some domains, these methods fall short in many real-world scenarios, especially in finance, where user interests and item popularity evolve rapidly over time. To address these challenges, we introduce a novel extension to Light Graph Convolutional Network (LightGCN) designed to learn temporal node embeddings that capture dynamic interests. Our approach employs causal convolution to maintain a forward-looking model architecture. By preserving the chronological order of user-item interactions and introducing a dynamic update mechanism for embeddings through a sliding window, the proposed model generates well-timed and contextually relevant recommendations. Extensive experiments on a real-world dataset from BNP Paribas demonstrate that our approach significantly enhances the performance of LightGCN while maintaining the simplicity and efficiency of its architecture. Our findings provide new insights into designing graph-based recommender systems in time-sensitive applications, particularly for financial product recommendations.",
        "field": "Quantum Computing",
        "link": "http://arxiv.org/abs/2503.14213v1"
    },
    {
        "id": "2503.13994v1",
        "title": "TarPro: Targeted Protection against Malicious Image Editing",
        "authors": [
            "Kaixin Shen",
            "Ruijie Quan",
            "Jiaxu Miao",
            "Jun Xiao",
            "Yi Yang"
        ],
        "published": "2025-03-18T07:54:44Z",
        "summary": "The rapid advancement of image editing techniques has raised concerns about their misuse for generating Not-Safe-for-Work (NSFW) content. This necessitates a targeted protection mechanism that blocks malicious edits while preserving normal editability. However, existing protection methods fail to achieve this balance, as they indiscriminately disrupt all edits while still allowing some harmful content to be generated. To address this, we propose TarPro, a targeted protection framework that prevents malicious edits while maintaining benign modifications. TarPro achieves this through a semantic-aware constraint that only disrupts malicious content and a lightweight perturbation generator that produces a more stable, imperceptible, and robust perturbation for image protection. Extensive experiments demonstrate that TarPro surpasses existing methods, achieving a high protection efficacy while ensuring minimal impact on normal edits. Our results highlight TarPro as a practical solution for secure and controlled image editing.",
        "field": "Quantum Computing",
        "link": "http://arxiv.org/abs/2503.13994v1"
    },
    {
        "id": "2503.13637v1",
        "title": "XChainDataGen: A Cross-Chain Dataset Generation Framework",
        "authors": [
            "André Augusto",
            "André Vasconcelos",
            "Miguel Correia",
            "Luyao Zhang"
        ],
        "published": "2025-03-17T18:39:43Z",
        "summary": "The number of blockchain interoperability protocols for transferring data and assets between blockchains has grown significantly. However, no open dataset of cross-chain transactions exists to study interoperability protocols in operation. There is also no tool to generate such datasets and make them available to the community. This paper proposes XChainDataGen, a tool to extract cross-chain data from blockchains and generate datasets of cross-chain transactions (cctxs). Using XChainDataGen, we extracted over 35 GB of data from five cross-chain protocols deployed on 11 blockchains in the last seven months of 2024, identifying 11,285,753 cctxs that moved over 28 billion USD in cross-chain token transfers. Using the data collected, we compare protocols and provide insights into their security, cost, and performance trade-offs. As examples, we highlight differences between protocols that require full finality on the source blockchain and those that only demand soft finality (\\textit{security}). We compare user costs, fee models, and the impact of variables such as the Ethereum gas price on protocol fees (\\textit{cost}). Finally, we produce the first analysis of the implications of EIP-7683 for cross-chain intents, which are increasingly popular and greatly improve the speed with which cctxs are processed (\\textit{performance}), thereby enhancing the user experience. The availability of XChainDataGen and this dataset allows various analyses, including trends in cross-chain activity, security assessments of interoperability protocols, and financial research on decentralized finance (DeFi) protocols.",
        "field": "Quantum Computing",
        "link": "http://arxiv.org/abs/2503.13637v1"
    },
    {
        "id": "2503.13637v1",
        "title": "XChainDataGen: A Cross-Chain Dataset Generation Framework",
        "authors": [
            "André Augusto",
            "André Vasconcelos",
            "Miguel Correia",
            "Luyao Zhang"
        ],
        "published": "2025-03-17T18:39:43Z",
        "summary": "The number of blockchain interoperability protocols for transferring data and assets between blockchains has grown significantly. However, no open dataset of cross-chain transactions exists to study interoperability protocols in operation. There is also no tool to generate such datasets and make them available to the community. This paper proposes XChainDataGen, a tool to extract cross-chain data from blockchains and generate datasets of cross-chain transactions (cctxs). Using XChainDataGen, we extracted over 35 GB of data from five cross-chain protocols deployed on 11 blockchains in the last seven months of 2024, identifying 11,285,753 cctxs that moved over 28 billion USD in cross-chain token transfers. Using the data collected, we compare protocols and provide insights into their security, cost, and performance trade-offs. As examples, we highlight differences between protocols that require full finality on the source blockchain and those that only demand soft finality (\\textit{security}). We compare user costs, fee models, and the impact of variables such as the Ethereum gas price on protocol fees (\\textit{cost}). Finally, we produce the first analysis of the implications of EIP-7683 for cross-chain intents, which are increasingly popular and greatly improve the speed with which cctxs are processed (\\textit{performance}), thereby enhancing the user experience. The availability of XChainDataGen and this dataset allows various analyses, including trends in cross-chain activity, security assessments of interoperability protocols, and financial research on decentralized finance (DeFi) protocols.",
        "field": "Decentralized Finance (DeFi)",
        "link": "http://arxiv.org/abs/2503.13637v1"
    },
    {
        "id": "2503.11940v1",
        "title": "Vote Delegation in DeFi Governance",
        "authors": [
            "Dion Bongaerts",
            "Thomas Lambert",
            "Daniel Liebau",
            "Peter Roosenboom"
        ],
        "published": "2025-03-15T01:15:08Z",
        "summary": "We investigate the drivers of vote delegation in Decentralized Autonomous Organizations (DAOs), using the Uniswap governance DAO as a laboratory. We show that parties with fewer self-owned votes and those affiliated with the controlling venture capital firm, Andreesen Horowitz (a16z), receive more vote delegations. These patterns suggest that while the Uniswap ecosystem values decentralization, a16z may engage in window-dressing around it. Moreover, we find that an active and successful track record in submitting improvement proposals, especially in the final stage, leads to more vote delegations, indicating that delegation in DAOs is at least partly reputation- or merit-based. Combined, our findings provide new insights into how governance and decentralization operate in DeFi.",
        "field": "Decentralized Finance (DeFi)",
        "link": "http://arxiv.org/abs/2503.11940v1"
    },
    {
        "id": "2503.06279v1",
        "title": "Mitigating Blockchain extractable value (BEV) threats by Distributed Transaction Sequencing in Blockchains",
        "authors": [
            "Xiongfei Zhao",
            "Hou-Wan Long",
            "Zhengzhe Li",
            "Jiangchuan Liu",
            "Yain-Whar Si"
        ],
        "published": "2025-03-08T16:55:52Z",
        "summary": "The rapid growth of Blockchain and Decentralized Finance (DeFi) has introduced new challenges and vulnerabilities that threaten the integrity and efficiency of the ecosystem. This study identifies critical issues such as Transaction Order Dependence (TOD), Blockchain Extractable Value (BEV), and Transaction Importance Diversity (TID), which collectively undermine the fairness and security of DeFi systems. BEV-related activities, including Sandwich attacks, Liquidations, and Transaction Replay, have emerged as significant threats, collectively generating $540.54 million in losses over 32 months across 11,289 addresses, involving 49,691 cryptocurrencies and 60,830 on-chain markets. These attacks exploit transaction mechanics to manipulate asset prices and extract value at the expense of other participants, with Sandwich attacks being particularly impactful. Additionally, the growing adoption of Blockchain in traditional finance highlights the challenge of TID, where high transaction volumes can strain systems and compromise time-sensitive operations. To address these pressing issues, we propose a novel Distributed Transaction Sequencing Strategy (DTSS), which combines forking mechanisms and the Analytic Hierarchy Process (AHP) to enforce fair and transparent transaction ordering in a decentralized manner. Our approach is further enhanced by an optimization framework and the introduction of the Normalized Allocation Disparity Metric (NADM), which ensures optimal parameter selection for transaction prioritization. Experimental evaluations demonstrate that DTSS effectively mitigates BEV risks, enhances transaction fairness, and significantly improves the security and transparency of DeFi ecosystems. This work is essential for protecting the future of decentralized finance and promoting its integration into global financial systems.",
        "field": "Decentralized Finance (DeFi)",
        "link": "http://arxiv.org/abs/2503.06279v1"
    },
    {
        "id": "2503.04850v2",
        "title": "Slow is Fast! Dissecting Ethereum's Slow Liquidity Drain Scams",
        "authors": [
            "Minh Trung Tran",
            "Nasrin Sohrabi",
            "Zahir Tari",
            "Qin Wang",
            "Xiaoyu Xia"
        ],
        "published": "2025-03-06T02:24:35Z",
        "summary": "We identify the slow liquidity drain (SLID) scam, an insidious and highly profitable threat to decentralized finance (DeFi), posing a large-scale, persistent, and growing risk to the ecosystem. Unlike traditional scams such as rug pulls or honeypots (USENIX Sec'19, USENIX Sec'23), SLID gradually siphons funds from liquidity pools over extended periods, making detection significantly more challenging. In this paper, we conducted the first large-scale empirical analysis of 319,166 liquidity pools across six major decentralized exchanges (DEXs) since 2018. We identified 3,117 SLID affected liquidity pools, resulting in cumulative losses of more than US$103 million. We propose a rule-based heuristic and an enhanced machine learning model for early detection. Our machine learning model achieves a detection speed 4.77 times faster than the heuristic while maintaining 95% accuracy. Our study establishes a foundation for protecting DeFi investors at an early stage and promoting transparency in the DeFi ecosystem.",
        "field": "Decentralized Finance (DeFi)",
        "link": "http://arxiv.org/abs/2503.04850v2"
    },
    {
        "id": "2503.01944v1",
        "title": "Protecting DeFi Platforms against Non-Price Flash Loan Attacks",
        "authors": [
            "Abdulrahman Alhaidari",
            "Balaji Palanisamy",
            "Prashant Krishnamurthy"
        ],
        "published": "2025-03-03T18:18:05Z",
        "summary": "Smart contracts in Decentralized Finance (DeFi) platforms are attractive targets for attacks as their vulnerabilities can lead to massive amounts of financial losses. Flash loan attacks, in particular, pose a major threat to DeFi protocols that hold a Total Value Locked (TVL) exceeding \\$106 billion. These attacks use the atomicity property of blockchains to drain funds from smart contracts in a single transaction. While existing research primarily focuses on price manipulation attacks, such as oracle manipulation, mitigating non-price flash loan attacks that often exploit smart contracts' zero-day vulnerabilities remains largely unaddressed. These attacks are challenging to detect because of their unique patterns, time sensitivity, and complexity. In this paper, we present FlashGuard, a runtime detection and mitigation method for non-price flash loan attacks. Our approach targets smart contract function signatures to identify attacks in real-time and counterattack by disrupting the attack transaction atomicity by leveraging the short window when transactions are visible in the mempool but not yet confirmed. When FlashGuard detects an attack, it dispatches a stealthy dusting counterattack transaction to miners to change the victim contract's state which disrupts the attack's atomicity and forces the attack transaction to revert. We evaluate our approach using 20 historical attacks and several unseen attacks. FlashGuard achieves an average real-time detection latency of 150.31ms, a detection accuracy of over 99.93\\%, and an average disruption time of 410.92ms. FlashGuard could have potentially rescued over \\$405.71 million in losses if it were deployed prior to these attack instances. FlashGuard demonstrates significant potential as a DeFi security solution to mitigate and handle rising threats of non-price flash loan attacks.",
        "field": "Decentralized Finance (DeFi)",
        "link": "http://arxiv.org/abs/2503.01944v1"
    },
    {
        "id": "2503.14284v1",
        "title": "Entente: Cross-silo Intrusion Detection on Network Log Graphs with Federated Learning",
        "authors": [
            "Jiacen Xu",
            "Chenang Li",
            "Yu Zheng",
            "Zhou Li"
        ],
        "published": "2025-03-18T14:21:24Z",
        "summary": "Graph-based Network Intrusion Detection System (GNIDS) has gained significant momentum in detecting sophisticated cyber-attacks, like Advanced Persistent Threat (APT), in an organization or across organizations. Though achieving satisfying detection accuracy and adapting to ever-changing attacks and normal patterns, all prior GNIDSs assume the centralized data settings directly, but non-trivial data collection is not always practical under privacy regulations nowadays. We argue that training a GNIDS model has to consider privacy regulations, and propose to leverage federated learning (FL) to address this prominent challenge. Yet, directly applying FL to GNIDS is unlikely to succeed, due to issues like non-IID (independent and identically distributed) graph data over clients and the diverse design choices taken by different GNIDS. We address these issues with a set of novel techniques tailored to the graph datasets, including reference graph synthesis, graph sketching and adaptive contribution scaling, and develop a new system Entente. We evaluate Entente on the large-scale LANL, OpTC and Pivoting datasets. The result shows Entente outperforms the other baseline FL algorithms and sometimes even the non-FL GNIDS. We also evaluate Entente under FL poisoning attacks tailored to the GNIDS setting, and show Entente is able to bound the attack success rate to low values. Overall, our result suggests building cross-silo GNIDS is feasible and we hope to encourage more efforts in this direction.",
        "field": "Artificial Intelligence (AI) in Fraud Detection",
        "link": "http://arxiv.org/abs/2503.14284v1"
    },
    {
        "id": "2503.14281v1",
        "title": "XOXO: Stealthy Cross-Origin Context Poisoning Attacks against AI Coding Assistants",
        "authors": [
            "Adam Štorek",
            "Mukur Gupta",
            "Noopur Bhatt",
            "Aditya Gupta",
            "Janie Kim",
            "Prashast Srivastava",
            "Suman Jana"
        ],
        "published": "2025-03-18T14:20:54Z",
        "summary": "AI coding assistants are widely used for tasks like code generation, bug detection, and comprehension. These tools now require large and complex contexts, automatically sourced from various origins$\\unicode{x2014}$across files, projects, and contributors$\\unicode{x2014}$forming part of the prompt fed to underlying LLMs. This automatic context-gathering introduces new vulnerabilities, allowing attackers to subtly poison input to compromise the assistant's outputs, potentially generating vulnerable code, overlooking flaws, or introducing critical errors. We propose a novel attack, Cross-Origin Context Poisoning (XOXO), that is particularly challenging to detect as it relies on adversarial code modifications that are semantically equivalent. Traditional program analysis techniques struggle to identify these correlations since the semantics of the code remain correct, making it appear legitimate. This allows attackers to manipulate code assistants into producing incorrect outputs, including vulnerabilities or backdoors, while shifting the blame to the victim developer or tester. We introduce a novel, task-agnostic black-box attack algorithm GCGS that systematically searches the transformation space using a Cayley Graph, achieving an 83.09% attack success rate on average across five tasks and eleven models, including GPT-4o and Claude 3.5 Sonnet v2 used by many popular AI coding assistants. Furthermore, existing defenses, including adversarial fine-tuning, are ineffective against our attack, underscoring the need for new security measures in LLM-powered coding tools.",
        "field": "Artificial Intelligence (AI) in Fraud Detection",
        "link": "http://arxiv.org/abs/2503.14281v1"
    },
    {
        "id": "2503.14279v1",
        "title": "Anti-Tamper Radio meets Reconfigurable Intelligent Surface for System-Level Tamper Detection",
        "authors": [
            "Maryam Shaygan Tabar",
            "Johannes Kortz",
            "Paul Staat",
            "Harald Elders-Boll",
            "Christof Paar",
            "Christian Zenger"
        ],
        "published": "2025-03-18T14:18:31Z",
        "summary": "Many computing systems need to be protected against physical attacks using active tamper detection based on sensors. One technical solution is to employ an ATR (Anti-Tamper Radio) approach, analyzing the radio wave propagation effects within a protected device to detect unauthorized physical alterations. However, ATR systems face key challenges in terms of susceptibility to signal manipulation attacks, limited reliability due to environmental noise, and regulatory constraints from wide bandwidth usage. In this work, we propose and experimentally evaluate an ATR system complemented by an RIS to dynamically reconfigure the wireless propagation environment. We show that this approach can enhance resistance against signal manipulation attacks, reduce bandwidth requirements from several~GHz down to as low as 20 MHz, and improve robustness to environmental disturbances such as internal fan movements. Our work demonstrates that RIS integration can strengthen the ATR performance to enhance security, sensitivity, and robustness, recognizing the potential of smart radio environments for ATR-based tamper detection",
        "field": "Artificial Intelligence (AI) in Fraud Detection",
        "link": "http://arxiv.org/abs/2503.14279v1"
    },
    {
        "id": "2503.13419v1",
        "title": "Securing Virtual Reality Experiences: Unveiling and Tackling Cybersickness Attacks with Explainable AI",
        "authors": [
            "Ripan Kumar Kundu",
            "Matthew Denton",
            "Genova Mongalo",
            "Prasad Calyam",
            "Khaza Anuarul Hoque"
        ],
        "published": "2025-03-17T17:49:51Z",
        "summary": "The synergy between virtual reality (VR) and artificial intelligence (AI), specifically deep learning (DL)-based cybersickness detection models, has ushered in unprecedented advancements in immersive experiences by automatically detecting cybersickness severity and adaptively various mitigation techniques, offering a smooth and comfortable VR experience. While this DL-enabled cybersickness detection method provides promising solutions for enhancing user experiences, it also introduces new risks since these models are vulnerable to adversarial attacks; a small perturbation of the input data that is visually undetectable to human observers can fool the cybersickness detection model and trigger unexpected mitigation, thus disrupting user immersive experiences (UIX) and even posing safety risks. In this paper, we present a new type of VR attack, i.e., a cybersickness attack, which successfully stops the triggering of cybersickness mitigation by fooling DL-based cybersickness detection models and dramatically hinders the UIX. Next, we propose a novel explainable artificial intelligence (XAI)-guided cybersickness attack detection framework to detect such attacks in VR to ensure UIX and a comfortable VR experience. We evaluate the proposed attack and the detection framework using two state-of-the-art open-source VR cybersickness datasets: Simulation 2021 and Gameplay dataset. Finally, to verify the effectiveness of our proposed method, we implement the attack and the XAI-based detection using a testbed with a custom-built VR roller coaster simulation with an HTC Vive Pro Eye headset and perform a user study. Our study shows that such an attack can dramatically hinder the UIX. However, our proposed XAI-guided cybersickness attack detection can successfully detect cybersickness attacks and trigger the proper mitigation, effectively reducing VR cybersickness.",
        "field": "Artificial Intelligence (AI) in Fraud Detection",
        "link": "http://arxiv.org/abs/2503.13419v1"
    },
    {
        "id": "2503.13572v1",
        "title": "VeriContaminated: Assessing LLM-Driven Verilog Coding for Data Contamination",
        "authors": [
            "Zeng Wang",
            "Minghao Shao",
            "Jitendra Bhandari",
            "Likhitha Mankali",
            "Ramesh Karri",
            "Ozgur Sinanoglu",
            "Muhammad Shafique",
            "Johann Knechtel"
        ],
        "published": "2025-03-17T12:26:49Z",
        "summary": "Large Language Models (LLMs) have revolutionized code generation, achieving exceptional results on various established benchmarking frameworks. However, concerns about data contamination - where benchmark data inadvertently leaks into pre-training or fine-tuning datasets - raise questions about the validity of these evaluations. While this issue is known, limiting the industrial adoption of LLM-driven software engineering, hardware coding has received little to no attention regarding these risks. For the first time, we analyze state-of-the-art (SOTA) evaluation frameworks for Verilog code generation (VerilogEval and RTLLM), using established methods for contamination detection (CCD and Min-K% Prob). We cover SOTA commercial and open-source LLMs (CodeGen2.5, Minitron 4b, Mistral 7b, phi-4 mini, LLaMA-{1,2,3.1}, GPT-{2,3.5,4o}, Deepseek-Coder, and CodeQwen 1.5), in baseline and fine-tuned models (RTLCoder and Verigen). Our study confirms that data contamination is a critical concern. We explore mitigations and the resulting trade-offs for code quality vs fairness (i.e., reducing contamination toward unbiased benchmarking).",
        "field": "Artificial Intelligence (AI) in Fraud Detection",
        "link": "http://arxiv.org/abs/2503.13572v1"
    },
    {
        "id": "2503.12952v1",
        "title": "Performance Analysis and Industry Deployment of Post-Quantum Cryptography Algorithms",
        "authors": [
            "Elif Dicle Demir",
            "Buse Bilgin",
            "Mehmet Cengiz Onbasli"
        ],
        "published": "2025-03-17T09:06:03Z",
        "summary": "As quantum computing advances, modern cryptographic standards face an existential threat, necessitating a transition to post-quantum cryptography (PQC). The National Institute of Standards and Technology (NIST) has selected CRYSTALS-Kyber and CRYSTALS-Dilithium as standardized PQC algorithms for secure key exchange and digital signatures, respectively. This study conducts a comprehensive performance analysis of these algorithms by benchmarking execution times across cryptographic operations such as key generation, encapsulation, decapsulation, signing, and verification. Additionally, the impact of AVX2 optimizations is evaluated to assess hardware acceleration benefits. Our findings demonstrate that Kyber and Dilithium achieve efficient execution times, outperforming classical cryptographic schemes such as RSA and ECDSA at equivalent security levels. Beyond technical performance, the real-world deployment of PQC introduces challenges in telecommunications networks, where large-scale infrastructure upgrades, interoperability with legacy systems, and regulatory constraints must be addressed. This paper examines the feasibility of PQC adoption in telecom environments, highlighting key transition challenges, security risks, and implementation strategies. Through industry case studies, we illustrate how telecom operators are integrating PQC into 5G authentication, subscriber identity protection, and secure communications. Our analysis provides insights into the computational trade-offs, deployment considerations, and standardization efforts shaping the future of quantum-safe cryptographic infrastructure.",
        "field": "Biometric Authentication",
        "link": "http://arxiv.org/abs/2503.12952v1"
    },
    {
        "id": "2503.11508v1",
        "title": "Leveraging Angle of Arrival Estimation against Impersonation Attacks in Physical Layer Authentication",
        "authors": [
            "Thuy M. Pham",
            "Linda Senigagliesi",
            "Marco Baldi",
            "Rafael F. Schaefer",
            "Gerhard P. Fettweis",
            "Arsenia Chorti"
        ],
        "published": "2025-03-14T15:29:55Z",
        "summary": "In this paper, we investigate the utilization of the angle of arrival (AoA) as a feature for robust physical layer authentication (PLA). While most of the existing approaches to PLA focus on common features of the physical layer of communication channels, such as channel frequency response, channel impulse response or received signal strength, the use of AoA in this domain has not yet been studied in depth, particularly regarding the ability to thwart impersonation attacks. In this work, we demonstrate that an impersonation attack targeting AoA based PLA is only feasible under strict conditions on the attacker's location and hardware capabilities, which highlights the AoA's potential as a strong feature for PLA. We extend previous works considering a single-antenna attacker to the case of a multiple-antenna attacker, and we develop a theoretical characterization of the conditions in which a successful impersonation attack can be mounted. Furthermore, we leverage extensive simulations in support of theoretical analyses, to validate the robustness of AoA-based PLA.",
        "field": "Biometric Authentication",
        "link": "http://arxiv.org/abs/2503.11508v1"
    },
    {
        "id": "2503.08632v1",
        "title": "Secret-Key Generation from Private Identifiers under Channel Uncertainty",
        "authors": [
            "Vamoua Yachongka",
            "Rémi A. Chou"
        ],
        "published": "2025-03-11T17:20:48Z",
        "summary": "This study investigates secret-key generation for device authentication using physical identifiers, such as responses from physical unclonable functions (PUFs). The system includes two legitimate terminals (encoder and decoder) and an eavesdropper (Eve), each with access to different measurements of the identifier. From the device identifier, the encoder generates a secret key, which is securely stored in a private database, along with helper data that is saved in a public database accessible by the decoder for key reconstruction. Eve, who also has access to the public database, may use both her own measurements and the helper data to attempt to estimate the secret key and identifier. Our setup focuses on authentication scenarios where channel statistics are uncertain, with the involved parties employing multiple antennas to enhance signal reception. Our contributions include deriving inner and outer bounds on the optimal trade-off among secret-key, storage, and privacy-leakage rates for general discrete sources, and showing that these bounds are tight for Gaussian sources.",
        "field": "Biometric Authentication",
        "link": "http://arxiv.org/abs/2503.08632v1"
    },
    {
        "id": "2503.08256v1",
        "title": "SoK: A cloudy view on trust relationships of CVMs -- How Confidential Virtual Machines are falling short in Public Cloud",
        "authors": [
            "Jana Eisoldt",
            "Anna Galanou",
            "Andrey Ruzhanskiy",
            "Nils Küchenmeister",
            "Yewgenij Baburkin",
            "Tianxiang Dai",
            "Ivan Gudymenko",
            "Stefan Köpsell",
            "Rüdiger Kapitza"
        ],
        "published": "2025-03-11T10:21:29Z",
        "summary": "Confidential computing in the public cloud intends to safeguard workload privacy while outsourcing infrastructure management to a cloud provider. This is achieved by executing customer workloads within so called Trusted Execution Environments (TEEs), such as Confidential Virtual Machines (CVMs), which protect them from unauthorized access by cloud administrators and privileged system software. At the core of confidential computing lies remote attestation -- a mechanism that enables workload owners to verify the initial state of their workload and furthermore authenticate the underlying hardware. hile this represents a significant advancement in cloud security, this SoK critically examines the confidential computing offerings of market-leading cloud providers to assess whether they genuinely adhere to its core principles. We develop a taxonomy based on carefully selected criteria to systematically evaluate these offerings, enabling us to analyse the components responsible for remote attestation, the evidence provided at each stage, the extent of cloud provider influence and whether this undermines the threat model of confidential computing. Specifically, we investigate how CVMs are deployed in the public cloud infrastructures, the extent to which customers can request and verify attestation evidence, and their ability to define and enforce configuration and attestation requirements. This analysis provides insight into whether confidential computing guarantees -- namely confidentiality and integrity -- are genuinely upheld. Our findings reveal that all major cloud providers retain control over critical parts of the trusted software stack and, in some cases, intervene in the standard remote attestation process. This directly contradicts their claims of delivering confidential computing, as the model fundamentally excludes the cloud provider from the set of trusted entities.",
        "field": "Biometric Authentication",
        "link": "http://arxiv.org/abs/2503.08256v1"
    },
    {
        "id": "2503.07857v1",
        "title": "Efficient Resource Management for Secure and Low-Latency O-RAN Communication",
        "authors": [
            "Zaineh Abughazzah",
            "Emna Baccour",
            "Ahmed Refaey",
            "Amr Mohamed",
            "Mounir Hamdi"
        ],
        "published": "2025-03-10T21:03:48Z",
        "summary": "Open Radio Access Networks (O-RAN) are transforming telecommunications by shifting from centralized to distributed architectures, promoting flexibility, interoperability, and innovation through open interfaces and multi-vendor environments. However, O-RAN's reliance on cloud-based architecture and enhanced observability introduces significant security and resource management challenges. Efficient resource management is crucial for secure and reliable communication in O-RAN, within the resource-constrained environment and heterogeneity of requirements, where multiple User Equipment (UE) and O-RAN Radio Units (O-RUs) coexist. This paper develops a framework to manage these aspects, ensuring each O-RU is associated with UEs based on their communication channel qualities and computational resources, and selecting appropriate encryption algorithms to safeguard data confidentiality, integrity, and authentication. A Multi-objective Optimization Problem (MOP) is formulated to minimize latency and maximize security within resource constraints. Different approaches are proposed to relax the complexity of the problem and achieve near-optimal performance, facilitating trade-offs between latency, security, and solution complexity. Simulation results demonstrate that the proposed approaches are close enough to the optimal solution, proving that our approach is both effective and efficient.",
        "field": "Biometric Authentication",
        "link": "http://arxiv.org/abs/2503.07857v1"
    },
    {
        "id": "2503.13637v1",
        "title": "XChainDataGen: A Cross-Chain Dataset Generation Framework",
        "authors": [
            "André Augusto",
            "André Vasconcelos",
            "Miguel Correia",
            "Luyao Zhang"
        ],
        "published": "2025-03-17T18:39:43Z",
        "summary": "The number of blockchain interoperability protocols for transferring data and assets between blockchains has grown significantly. However, no open dataset of cross-chain transactions exists to study interoperability protocols in operation. There is also no tool to generate such datasets and make them available to the community. This paper proposes XChainDataGen, a tool to extract cross-chain data from blockchains and generate datasets of cross-chain transactions (cctxs). Using XChainDataGen, we extracted over 35 GB of data from five cross-chain protocols deployed on 11 blockchains in the last seven months of 2024, identifying 11,285,753 cctxs that moved over 28 billion USD in cross-chain token transfers. Using the data collected, we compare protocols and provide insights into their security, cost, and performance trade-offs. As examples, we highlight differences between protocols that require full finality on the source blockchain and those that only demand soft finality (\\textit{security}). We compare user costs, fee models, and the impact of variables such as the Ethereum gas price on protocol fees (\\textit{cost}). Finally, we produce the first analysis of the implications of EIP-7683 for cross-chain intents, which are increasingly popular and greatly improve the speed with which cctxs are processed (\\textit{performance}), thereby enhancing the user experience. The availability of XChainDataGen and this dataset allows various analyses, including trends in cross-chain activity, security assessments of interoperability protocols, and financial research on decentralized finance (DeFi) protocols.",
        "field": "Blockchain Interoperability",
        "link": "http://arxiv.org/abs/2503.13637v1"
    },
    {
        "id": "2503.12952v1",
        "title": "Performance Analysis and Industry Deployment of Post-Quantum Cryptography Algorithms",
        "authors": [
            "Elif Dicle Demir",
            "Buse Bilgin",
            "Mehmet Cengiz Onbasli"
        ],
        "published": "2025-03-17T09:06:03Z",
        "summary": "As quantum computing advances, modern cryptographic standards face an existential threat, necessitating a transition to post-quantum cryptography (PQC). The National Institute of Standards and Technology (NIST) has selected CRYSTALS-Kyber and CRYSTALS-Dilithium as standardized PQC algorithms for secure key exchange and digital signatures, respectively. This study conducts a comprehensive performance analysis of these algorithms by benchmarking execution times across cryptographic operations such as key generation, encapsulation, decapsulation, signing, and verification. Additionally, the impact of AVX2 optimizations is evaluated to assess hardware acceleration benefits. Our findings demonstrate that Kyber and Dilithium achieve efficient execution times, outperforming classical cryptographic schemes such as RSA and ECDSA at equivalent security levels. Beyond technical performance, the real-world deployment of PQC introduces challenges in telecommunications networks, where large-scale infrastructure upgrades, interoperability with legacy systems, and regulatory constraints must be addressed. This paper examines the feasibility of PQC adoption in telecom environments, highlighting key transition challenges, security risks, and implementation strategies. Through industry case studies, we illustrate how telecom operators are integrating PQC into 5G authentication, subscriber identity protection, and secure communications. Our analysis provides insights into the computational trade-offs, deployment considerations, and standardization efforts shaping the future of quantum-safe cryptographic infrastructure.",
        "field": "Blockchain Interoperability",
        "link": "http://arxiv.org/abs/2503.12952v1"
    },
    {
        "id": "2503.09666v1",
        "title": "Blockchain-Enabled Management Framework for Federated Coalition Networks",
        "authors": [
            "Jorge Álvaro González",
            "Ana María Saiz García",
            "Victor Monzon Baeza"
        ],
        "published": "2025-03-12T16:59:23Z",
        "summary": "In a globalized and interconnected world, interoperability has become a key concept for advancing tactical scenarios. Federated Coalition Networks (FCN) enable cooperation between entities from multiple nations while allowing each to maintain control over their systems. However, this interoperability necessitates the sharing of increasing amounts of information between different tactical assets, raising the need for higher security measures. Emerging technologies like blockchain drive a revolution in secure communications, paving the way for new tactical scenarios. In this work, we propose a blockchain-based framework to enhance the resilience and security of the management of these networks. We offer a guide to FCN design to help a broad audience understand the military networks in international missions by a use case and key functions applied to a proposed architecture. We evaluate its effectiveness and performance in information encryption to validate this framework.",
        "field": "Blockchain Interoperability",
        "link": "http://arxiv.org/abs/2503.09666v1"
    },
    {
        "id": "2503.09165v1",
        "title": "Blockchain Data Analytics: Review and Challenges",
        "authors": [
            "Rischan Mafrur"
        ],
        "published": "2025-03-12T08:49:51Z",
        "summary": "The integration of blockchain technology with data analytics is essential for extracting insights in the cryptocurrency space. Although academic literature on blockchain data analytics is limited, various industry solutions have emerged to address these needs. This paper provides a comprehensive literature review, drawing from both academic research and industry applications. We classify blockchain analytics tools into categories such as block explorers, on-chain data providers, research platforms, and crypto market data providers. Additionally, we discuss the challenges associated with blockchain data analytics, including data accessibility, scalability, accuracy, and interoperability. Our findings emphasize the importance of bridging academic research and industry innovations to advance blockchain data analytics.",
        "field": "Blockchain Interoperability",
        "link": "http://arxiv.org/abs/2503.09165v1"
    },
    {
        "id": "2503.07857v1",
        "title": "Efficient Resource Management for Secure and Low-Latency O-RAN Communication",
        "authors": [
            "Zaineh Abughazzah",
            "Emna Baccour",
            "Ahmed Refaey",
            "Amr Mohamed",
            "Mounir Hamdi"
        ],
        "published": "2025-03-10T21:03:48Z",
        "summary": "Open Radio Access Networks (O-RAN) are transforming telecommunications by shifting from centralized to distributed architectures, promoting flexibility, interoperability, and innovation through open interfaces and multi-vendor environments. However, O-RAN's reliance on cloud-based architecture and enhanced observability introduces significant security and resource management challenges. Efficient resource management is crucial for secure and reliable communication in O-RAN, within the resource-constrained environment and heterogeneity of requirements, where multiple User Equipment (UE) and O-RAN Radio Units (O-RUs) coexist. This paper develops a framework to manage these aspects, ensuring each O-RU is associated with UEs based on their communication channel qualities and computational resources, and selecting appropriate encryption algorithms to safeguard data confidentiality, integrity, and authentication. A Multi-objective Optimization Problem (MOP) is formulated to minimize latency and maximize security within resource constraints. Different approaches are proposed to relax the complexity of the problem and achieve near-optimal performance, facilitating trade-offs between latency, security, and solution complexity. Simulation results demonstrate that the proposed approaches are close enough to the optimal solution, proving that our approach is both effective and efficient.",
        "field": "Blockchain Interoperability",
        "link": "http://arxiv.org/abs/2503.07857v1"
    },
    {
        "id": "2503.14462v1",
        "title": "Blockchain with proof of quantum work",
        "authors": [
            "Mohammad H. Amin",
            "Jack Raymond",
            "Daniel Kinn",
            "Firas Hamze",
            "Kelsey Hamer",
            "Joel Pasvolsky",
            "William Bernoudy",
            "Andrew D. King",
            "Samuel Kortas"
        ],
        "published": "2025-03-18T17:37:22Z",
        "summary": "We propose a blockchain architecture in which mining requires a quantum computer. The consensus mechanism is based on proof of quantum work, a quantum-enhanced alternative to traditional proof of work that leverages quantum supremacy to make mining intractable for classical computers. We have refined the blockchain framework to incorporate the probabilistic nature of quantum mechanics, ensuring stability against sampling errors and hardware inaccuracies. To validate our approach, we implemented a prototype blockchain on four D-Wave$^{\\rm TM}$ quantum annealing processors geographically distributed within North America, demonstrating stable operation across hundreds of thousands of quantum hashing operations. Our experimental protocol follows the same approach used in the recent demonstration of quantum supremacy [1], ensuring that classical computers cannot efficiently perform the same computation task. By replacing classical machines with quantum systems for mining, it is possible to significantly reduce the energy consumption and environmental impact traditionally associated with blockchain mining. Beyond serving as a proof of concept for a meaningful application of quantum computing, this work highlights the potential for other near-term quantum computing applications using existing technology.",
        "field": "Neuromorphic Computing",
        "link": "http://arxiv.org/abs/2503.14462v1"
    },
    {
        "id": "2503.14279v1",
        "title": "Anti-Tamper Radio meets Reconfigurable Intelligent Surface for System-Level Tamper Detection",
        "authors": [
            "Maryam Shaygan Tabar",
            "Johannes Kortz",
            "Paul Staat",
            "Harald Elders-Boll",
            "Christof Paar",
            "Christian Zenger"
        ],
        "published": "2025-03-18T14:18:31Z",
        "summary": "Many computing systems need to be protected against physical attacks using active tamper detection based on sensors. One technical solution is to employ an ATR (Anti-Tamper Radio) approach, analyzing the radio wave propagation effects within a protected device to detect unauthorized physical alterations. However, ATR systems face key challenges in terms of susceptibility to signal manipulation attacks, limited reliability due to environmental noise, and regulatory constraints from wide bandwidth usage. In this work, we propose and experimentally evaluate an ATR system complemented by an RIS to dynamically reconfigure the wireless propagation environment. We show that this approach can enhance resistance against signal manipulation attacks, reduce bandwidth requirements from several~GHz down to as low as 20 MHz, and improve robustness to environmental disturbances such as internal fan movements. Our work demonstrates that RIS integration can strengthen the ATR performance to enhance security, sensitivity, and robustness, recognizing the potential of smart radio environments for ATR-based tamper detection",
        "field": "Neuromorphic Computing",
        "link": "http://arxiv.org/abs/2503.14279v1"
    },
    {
        "id": "2503.14213v1",
        "title": "Rolling Forward: Enhancing LightGCN with Causal Graph Convolution for Credit Bond Recommendation",
        "authors": [
            "Ashraf Ghiye",
            "Baptiste Barreau",
            "Laurent Carlier",
            "Michalis Vazirgiannis"
        ],
        "published": "2025-03-18T12:47:01Z",
        "summary": "Graph Neural Networks have significantly advanced research in recommender systems over the past few years. These methods typically capture global interests using aggregated past interactions and rely on static embeddings of users and items over extended periods of time. While effective in some domains, these methods fall short in many real-world scenarios, especially in finance, where user interests and item popularity evolve rapidly over time. To address these challenges, we introduce a novel extension to Light Graph Convolutional Network (LightGCN) designed to learn temporal node embeddings that capture dynamic interests. Our approach employs causal convolution to maintain a forward-looking model architecture. By preserving the chronological order of user-item interactions and introducing a dynamic update mechanism for embeddings through a sliding window, the proposed model generates well-timed and contextually relevant recommendations. Extensive experiments on a real-world dataset from BNP Paribas demonstrate that our approach significantly enhances the performance of LightGCN while maintaining the simplicity and efficiency of its architecture. Our findings provide new insights into designing graph-based recommender systems in time-sensitive applications, particularly for financial product recommendations.",
        "field": "Neuromorphic Computing",
        "link": "http://arxiv.org/abs/2503.14213v1"
    },
    {
        "id": "2503.13994v1",
        "title": "TarPro: Targeted Protection against Malicious Image Editing",
        "authors": [
            "Kaixin Shen",
            "Ruijie Quan",
            "Jiaxu Miao",
            "Jun Xiao",
            "Yi Yang"
        ],
        "published": "2025-03-18T07:54:44Z",
        "summary": "The rapid advancement of image editing techniques has raised concerns about their misuse for generating Not-Safe-for-Work (NSFW) content. This necessitates a targeted protection mechanism that blocks malicious edits while preserving normal editability. However, existing protection methods fail to achieve this balance, as they indiscriminately disrupt all edits while still allowing some harmful content to be generated. To address this, we propose TarPro, a targeted protection framework that prevents malicious edits while maintaining benign modifications. TarPro achieves this through a semantic-aware constraint that only disrupts malicious content and a lightweight perturbation generator that produces a more stable, imperceptible, and robust perturbation for image protection. Extensive experiments demonstrate that TarPro surpasses existing methods, achieving a high protection efficacy while ensuring minimal impact on normal edits. Our results highlight TarPro as a practical solution for secure and controlled image editing.",
        "field": "Neuromorphic Computing",
        "link": "http://arxiv.org/abs/2503.13994v1"
    },
    {
        "id": "2503.13637v1",
        "title": "XChainDataGen: A Cross-Chain Dataset Generation Framework",
        "authors": [
            "André Augusto",
            "André Vasconcelos",
            "Miguel Correia",
            "Luyao Zhang"
        ],
        "published": "2025-03-17T18:39:43Z",
        "summary": "The number of blockchain interoperability protocols for transferring data and assets between blockchains has grown significantly. However, no open dataset of cross-chain transactions exists to study interoperability protocols in operation. There is also no tool to generate such datasets and make them available to the community. This paper proposes XChainDataGen, a tool to extract cross-chain data from blockchains and generate datasets of cross-chain transactions (cctxs). Using XChainDataGen, we extracted over 35 GB of data from five cross-chain protocols deployed on 11 blockchains in the last seven months of 2024, identifying 11,285,753 cctxs that moved over 28 billion USD in cross-chain token transfers. Using the data collected, we compare protocols and provide insights into their security, cost, and performance trade-offs. As examples, we highlight differences between protocols that require full finality on the source blockchain and those that only demand soft finality (\\textit{security}). We compare user costs, fee models, and the impact of variables such as the Ethereum gas price on protocol fees (\\textit{cost}). Finally, we produce the first analysis of the implications of EIP-7683 for cross-chain intents, which are increasingly popular and greatly improve the speed with which cctxs are processed (\\textit{performance}), thereby enhancing the user experience. The availability of XChainDataGen and this dataset allows various analyses, including trends in cross-chain activity, security assessments of interoperability protocols, and financial research on decentralized finance (DeFi) protocols.",
        "field": "Neuromorphic Computing",
        "link": "http://arxiv.org/abs/2503.13637v1"
    },
    {
        "id": "2503.14462v1",
        "title": "Blockchain with proof of quantum work",
        "authors": [
            "Mohammad H. Amin",
            "Jack Raymond",
            "Daniel Kinn",
            "Firas Hamze",
            "Kelsey Hamer",
            "Joel Pasvolsky",
            "William Bernoudy",
            "Andrew D. King",
            "Samuel Kortas"
        ],
        "published": "2025-03-18T17:37:22Z",
        "summary": "We propose a blockchain architecture in which mining requires a quantum computer. The consensus mechanism is based on proof of quantum work, a quantum-enhanced alternative to traditional proof of work that leverages quantum supremacy to make mining intractable for classical computers. We have refined the blockchain framework to incorporate the probabilistic nature of quantum mechanics, ensuring stability against sampling errors and hardware inaccuracies. To validate our approach, we implemented a prototype blockchain on four D-Wave$^{\\rm TM}$ quantum annealing processors geographically distributed within North America, demonstrating stable operation across hundreds of thousands of quantum hashing operations. Our experimental protocol follows the same approach used in the recent demonstration of quantum supremacy [1], ensuring that classical computers cannot efficiently perform the same computation task. By replacing classical machines with quantum systems for mining, it is possible to significantly reduce the energy consumption and environmental impact traditionally associated with blockchain mining. Beyond serving as a proof of concept for a meaningful application of quantum computing, this work highlights the potential for other near-term quantum computing applications using existing technology.",
        "field": "Edge Computing",
        "link": "http://arxiv.org/abs/2503.14462v1"
    },
    {
        "id": "2503.14279v1",
        "title": "Anti-Tamper Radio meets Reconfigurable Intelligent Surface for System-Level Tamper Detection",
        "authors": [
            "Maryam Shaygan Tabar",
            "Johannes Kortz",
            "Paul Staat",
            "Harald Elders-Boll",
            "Christof Paar",
            "Christian Zenger"
        ],
        "published": "2025-03-18T14:18:31Z",
        "summary": "Many computing systems need to be protected against physical attacks using active tamper detection based on sensors. One technical solution is to employ an ATR (Anti-Tamper Radio) approach, analyzing the radio wave propagation effects within a protected device to detect unauthorized physical alterations. However, ATR systems face key challenges in terms of susceptibility to signal manipulation attacks, limited reliability due to environmental noise, and regulatory constraints from wide bandwidth usage. In this work, we propose and experimentally evaluate an ATR system complemented by an RIS to dynamically reconfigure the wireless propagation environment. We show that this approach can enhance resistance against signal manipulation attacks, reduce bandwidth requirements from several~GHz down to as low as 20 MHz, and improve robustness to environmental disturbances such as internal fan movements. Our work demonstrates that RIS integration can strengthen the ATR performance to enhance security, sensitivity, and robustness, recognizing the potential of smart radio environments for ATR-based tamper detection",
        "field": "Edge Computing",
        "link": "http://arxiv.org/abs/2503.14279v1"
    },
    {
        "id": "2503.14213v1",
        "title": "Rolling Forward: Enhancing LightGCN with Causal Graph Convolution for Credit Bond Recommendation",
        "authors": [
            "Ashraf Ghiye",
            "Baptiste Barreau",
            "Laurent Carlier",
            "Michalis Vazirgiannis"
        ],
        "published": "2025-03-18T12:47:01Z",
        "summary": "Graph Neural Networks have significantly advanced research in recommender systems over the past few years. These methods typically capture global interests using aggregated past interactions and rely on static embeddings of users and items over extended periods of time. While effective in some domains, these methods fall short in many real-world scenarios, especially in finance, where user interests and item popularity evolve rapidly over time. To address these challenges, we introduce a novel extension to Light Graph Convolutional Network (LightGCN) designed to learn temporal node embeddings that capture dynamic interests. Our approach employs causal convolution to maintain a forward-looking model architecture. By preserving the chronological order of user-item interactions and introducing a dynamic update mechanism for embeddings through a sliding window, the proposed model generates well-timed and contextually relevant recommendations. Extensive experiments on a real-world dataset from BNP Paribas demonstrate that our approach significantly enhances the performance of LightGCN while maintaining the simplicity and efficiency of its architecture. Our findings provide new insights into designing graph-based recommender systems in time-sensitive applications, particularly for financial product recommendations.",
        "field": "Edge Computing",
        "link": "http://arxiv.org/abs/2503.14213v1"
    },
    {
        "id": "2503.13994v1",
        "title": "TarPro: Targeted Protection against Malicious Image Editing",
        "authors": [
            "Kaixin Shen",
            "Ruijie Quan",
            "Jiaxu Miao",
            "Jun Xiao",
            "Yi Yang"
        ],
        "published": "2025-03-18T07:54:44Z",
        "summary": "The rapid advancement of image editing techniques has raised concerns about their misuse for generating Not-Safe-for-Work (NSFW) content. This necessitates a targeted protection mechanism that blocks malicious edits while preserving normal editability. However, existing protection methods fail to achieve this balance, as they indiscriminately disrupt all edits while still allowing some harmful content to be generated. To address this, we propose TarPro, a targeted protection framework that prevents malicious edits while maintaining benign modifications. TarPro achieves this through a semantic-aware constraint that only disrupts malicious content and a lightweight perturbation generator that produces a more stable, imperceptible, and robust perturbation for image protection. Extensive experiments demonstrate that TarPro surpasses existing methods, achieving a high protection efficacy while ensuring minimal impact on normal edits. Our results highlight TarPro as a practical solution for secure and controlled image editing.",
        "field": "Edge Computing",
        "link": "http://arxiv.org/abs/2503.13994v1"
    },
    {
        "id": "2503.13637v1",
        "title": "XChainDataGen: A Cross-Chain Dataset Generation Framework",
        "authors": [
            "André Augusto",
            "André Vasconcelos",
            "Miguel Correia",
            "Luyao Zhang"
        ],
        "published": "2025-03-17T18:39:43Z",
        "summary": "The number of blockchain interoperability protocols for transferring data and assets between blockchains has grown significantly. However, no open dataset of cross-chain transactions exists to study interoperability protocols in operation. There is also no tool to generate such datasets and make them available to the community. This paper proposes XChainDataGen, a tool to extract cross-chain data from blockchains and generate datasets of cross-chain transactions (cctxs). Using XChainDataGen, we extracted over 35 GB of data from five cross-chain protocols deployed on 11 blockchains in the last seven months of 2024, identifying 11,285,753 cctxs that moved over 28 billion USD in cross-chain token transfers. Using the data collected, we compare protocols and provide insights into their security, cost, and performance trade-offs. As examples, we highlight differences between protocols that require full finality on the source blockchain and those that only demand soft finality (\\textit{security}). We compare user costs, fee models, and the impact of variables such as the Ethereum gas price on protocol fees (\\textit{cost}). Finally, we produce the first analysis of the implications of EIP-7683 for cross-chain intents, which are increasingly popular and greatly improve the speed with which cctxs are processed (\\textit{performance}), thereby enhancing the user experience. The availability of XChainDataGen and this dataset allows various analyses, including trends in cross-chain activity, security assessments of interoperability protocols, and financial research on decentralized finance (DeFi) protocols.",
        "field": "Edge Computing",
        "link": "http://arxiv.org/abs/2503.13637v1"
    },
    {
        "id": "2503.09823v1",
        "title": "Data Traceability for Privacy Alignment",
        "authors": [
            "Kevin Liao",
            "Shreya Thipireddy",
            "Daniel Weitzner"
        ],
        "published": "2025-03-12T20:42:23Z",
        "summary": "This paper offers a new privacy approach for the growing ecosystem of services--ranging from open banking to healthcare--dependent on sensitive personal data sharing between individuals and third-parties. While these services offer significant benefits, individuals want control over their data, transparency regarding how their data is used, and accountability from third-parties for misuse. However, existing legal and technical mechanisms are inadequate for supporting these needs. A comprehensive approach to the modern privacy challenges of accountable third-party data sharing requires a closer alignment of technical system architecture and legal institutional design. In order to achieve this privacy alignment, we extend traditional security threat modeling and analysis to encompass a broader range of privacy notions than has been typically considered. In particular, we introduce the concept of covert-accountability, which addresses adversaries that may act dishonestly but face potential identification and legal consequences. As a concrete instance of this design approach, we present the OTrace protocol, designed to provide traceable, accountable, consumer-control in third-party data sharing ecosystems. OTrace empowers consumers with the knowledge of where their data is, who has it, what it is being used for, and whom it is being shared with. By applying our alignment framework to OTrace, we demonstrate that OTrace's technical affordances can provide more confident, scalable regulatory oversight when combined with complementary legal mechanisms.",
        "field": "Artificial Intelligence (AI) in Banking",
        "link": "http://arxiv.org/abs/2503.09823v1"
    },
    {
        "id": "2503.09586v1",
        "title": "Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot",
        "authors": [
            "Andrew Crossman",
            "Andrew R. Plummer",
            "Chandra Sekharudu",
            "Deepak Warrier",
            "Mohammad Yekrangian"
        ],
        "published": "2025-03-12T17:54:18Z",
        "summary": "We present Auspex - a threat modeling system built using a specialized collection of generative artificial intelligence-based methods that capture threat modeling tradecraft. This new approach, called tradecraft prompting, centers on encoding the on-the-ground knowledge of threat modelers within the prompts that drive a generative AI-based threat modeling system. Auspex employs tradecraft prompts in two processing stages. The first stage centers on ingesting and processing system architecture information using prompts that encode threat modeling tradecraft knowledge pertaining to system decomposition and description. The second stage centers on chaining the resulting system analysis through a collection of prompts that encode tradecraft knowledge on threat identification, classification, and mitigation. The two-stage process yields a threat matrix for a system that specifies threat scenarios, threat types, information security categorizations and potential mitigations. Auspex produces formalized threat model output in minutes, relative to the weeks or months a manual process takes. More broadly, the focus on bespoke tradecraft prompting, as opposed to fine-tuning or agent-based add-ons, makes Auspex a lightweight, flexible, modular, and extensible foundational system capable of addressing the complexity, resource, and standardization limitations of both existing manual and automated threat modeling processes. In this connection, we establish the baseline value of Auspex to threat modelers through an evaluation procedure based on feedback collected from cybersecurity subject matter experts measuring the quality and utility of threat models generated by Auspex on real banking systems. We conclude with a discussion of system performance and plans for enhancements to Auspex.",
        "field": "Artificial Intelligence (AI) in Banking",
        "link": "http://arxiv.org/abs/2503.09586v1"
    },
    {
        "id": "2503.00070v1",
        "title": "Systematic Review of Cybersecurity in Banking: Evolution from Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain, Policies and Practice",
        "authors": [
            "Tue Nhi Tran"
        ],
        "published": "2025-02-27T14:17:06Z",
        "summary": "Throughout the history from pre-industry 4.0 to post-industry 4.0, cybersecurity at banks has undergone significant changes. Pre-industry 4.0 cyber security at banks relied on individual security methods that were highly manual and had low accuracy. When moving to post-industry 4.0, cybersecurity at banks had a major turning point with security methods that combined different technologies such as Artificial Intelligence (AI), Blockchain, IoT, automating necessary processes and significantly increasing the defence layer for banks. However, along with the development of new technologies, the current challenge of cybersecurity at banks lies in scalability, high costs and resources in both money and time for R&D of defence methods along with the threat of high-tech cybercriminals growing and expanding. This report goes from introducing the importance of cybersecurity at banks, analyzing their management, operational and business objectives, evaluating pre-industry 4.0 technologies used for cybersecurity at banks to assessing post-industry 4.0 technologies focusing on Artificial Intelligence and Blockchain, discussing current policies and practices and ending with discussing key advantages and challenges for 4.0 technologies and recommendations for further developing cybersecurity at banks.",
        "field": "Artificial Intelligence (AI) in Banking",
        "link": "http://arxiv.org/abs/2503.00070v1"
    },
    {
        "id": "2503.10644v1",
        "title": "Combined climate stress testing of supply-chain networks and the financial system with nation-wide firm-level emission estimates",
        "authors": [
            "Zlata Tabachová",
            "Christian Diem",
            "Johannes Stangl",
            "András Borsos",
            "Stefan Thurner"
        ],
        "published": "2025-02-25T20:25:47Z",
        "summary": "On the way towards carbon neutrality, climate stress testing provides estimates for the physical and transition risks that climate change poses to the economy and the financial system. Missing firm-level CO2 emissions data severely impedes the assessment of transition risks originating from carbon pricing. Based on the individual emissions of all Hungarian firms (410,523), as estimated from their fossil fuel purchases, we conduct a stress test of both actual and hypothetical carbon pricing policies. Using a simple 1:1 economic ABM and introducing the new carbon-to-profit ratio, we identify firms that become unprofitable and default, and estimate the respective loan write-offs. We find that 45% of all companies are directly exposed to carbon pricing. At a price of 45 EUR/t, direct economic losses of 1.3% of total sales and bank equity losses of 1.2% are expected. Secondary default cascades in supply chain networks could increase these losses by 300% to 4000%, depending on firms' ability to substitute essential inputs. To reduce transition risks, firms should reduce their dependence on essential inputs from supply chains with high CO2 exposure. We discuss the implications of different policy implementations on these transition risks.",
        "field": "Artificial Intelligence (AI) in Banking",
        "link": "http://arxiv.org/abs/2503.10644v1"
    },
    {
        "id": "2502.14766v2",
        "title": "Multi-Layer Deep xVA: Structural Credit Models, Measure Changes and Convergence Analysis",
        "authors": [
            "Kristoffer Andersson",
            "Alessandro Gnoatto"
        ],
        "published": "2025-02-20T17:41:55Z",
        "summary": "We propose a structural default model for portfolio-wide valuation adjustments (xVAs) and represent it as a system of coupled backward stochastic differential equations. The framework is divided into four layers, each capturing a key component: (i) clean values, (ii) initial margin and Collateral Valuation Adjustment (ColVA), (iii) Credit/Debit Valuation Adjustments (CVA/DVA) together with Margin Valuation Adjustment (MVA), and (iv) Funding Valuation Adjustment (FVA). Because these layers depend on one another through collateral and default effects, a naive Monte Carlo approach would require deeply nested simulations, making the problem computationally intractable. To address this challenge, we use an iterative deep BSDE approach, handling each layer sequentially so that earlier outputs serve as inputs to the subsequent layers. Initial margin is computed via deep quantile regression to reflect margin requirements over the Margin Period of Risk. We also adopt a change-of-measure method that highlights rare but significant defaults of the bank or counterparty, ensuring that these events are accurately captured in the training process. We further extend Han and Long's (2020) a posteriori error analysis to BSDEs on bounded domains. Due to the random exit from the domain, we obtain an order of convergence of $\\mathcal{O}(h^{1/4-\\epsilon})$ rather than the usual $\\mathcal{O}(h^{1/2})$. Numerical experiments illustrate that this method drastically reduces computational demands and successfully scales to high-dimensional, non-symmetric portfolios. The results confirm its effectiveness and accuracy, offering a practical alternative to nested Monte Carlo simulations in multi-counterparty xVA analyses.",
        "field": "Artificial Intelligence (AI) in Banking",
        "link": "http://arxiv.org/abs/2502.14766v2"
    },
    {
        "id": "2412.04051v1",
        "title": "How to design a Public Key Infrastructure for a Central Bank Digital Currency",
        "authors": [
            "Makan Rafiee",
            "Lars Hupel"
        ],
        "published": "2024-12-05T10:41:38Z",
        "summary": "Central Bank Digital Currency (CBDC) is a new form of money, issued by a country's or region's central bank, that can be used for a variety of payment scenarios. Depending on its concrete implementation, there are many participants in a production CBDC ecosystem, including the central bank, commercial banks, merchants, individuals, and wallet providers. There is a need for robust and scalable Public Key Infrastructure (PKI) for CBDC to ensure the continued trust of all entities in the system. This paper discusses the criteria that should flow into the design of a PKI and proposes a certificate hierarchy, together with a rollover concept ensuring continuous operation of the system. We further consider several peculiarities, such as the circulation of offline-capable hardware wallets.",
        "field": "Central Bank Digital Currencies (CBDCs)",
        "link": "http://arxiv.org/abs/2412.04051v1"
    },
    {
        "id": "2411.06362v1",
        "title": "Will Central Bank Digital Currencies (CBDC) and Blockchain Cryptocurrencies Coexist in the Post Quantum Era?",
        "authors": [
            "Abraham Itzhak Weinberg",
            "Pythagoras Petratos",
            "Alessio Faccia"
        ],
        "published": "2024-11-10T05:05:55Z",
        "summary": "This paper explores the coexistence possibilities of Central Bank Digital Currencies (CBDCs) and blockchain-based cryptocurrencies within a post-quantum computing landscape. It examines the implications of emerging quantum algorithms and cryptographic techniques such as Multi-Party Computation (MPC) and Oblivious Transfer (OT). While exploring how CBDCs and cryptocurrencies might integrate defenses like post-quantum cryptography, it highlights the substantial hurdles in transitioning legacy systems and fostering widespread adoption of new standards. The paper includes comprehensive evaluations of CBDCs in a quantum context. It also features comparisons to alternative cryptocurrency models. Additionally, the paper provides insightful analyses of pertinent quantum methodologies. Examinations of interfaces between these methods and blockchain architectures are also included. The paper carries out considered appraisals of quantum threats and their relevance for cryptocurrency schemes. Furthermore, it features discussions of the influence of anticipated advances in quantum computing on algorithms and their applications. The paper renders the judicious conclusion that long-term coexistence is viable provided challenges are constructively addressed through ongoing collaborative efforts to validate solutions and guide evolving policies.",
        "field": "Central Bank Digital Currencies (CBDCs)",
        "link": "http://arxiv.org/abs/2411.06362v1"
    },
    {
        "id": "2408.06956v1",
        "title": "PayOff: A Regulated Central Bank Digital Currency with Private Offline Payments",
        "authors": [
            "Carolin Beer",
            "Sheila Zingg",
            "Kari Kostiainen",
            "Karl Wüst",
            "Vedran Capkun",
            "Srdjan Capkun"
        ],
        "published": "2024-08-13T15:15:06Z",
        "summary": "The European Central Bank is preparing for the potential issuance of a central bank digital currency (CBDC), called the digital euro. A recent regulatory proposal by the European Commission defines several requirements for the digital euro, such as support for both online and offline payments. Offline payments are expected to enable cash-like privacy, local payment settlement, and the enforcement of holding limits. While other central banks have expressed similar desired functionality, achieving such offline payments poses a novel technical challenge. We observe that none of the existing research solutions, including offline E-cash schemes, are fully compliant. Proposed solutions based on secure elements offer no guarantees in case of compromise and can therefore lead to significant payment fraud. The main contribution of this paper is PayOff, a novel CBDC design motivated by the digital euro regulation, which focuses on offline payments. We analyze the security implications of local payment settlement and identify new security objectives. PayOff protects user privacy, supports complex regulations such as holding limits, and implements safeguards to increase robustness against secure element failure. Our analysis shows that PayOff provides strong privacy and identifies residual leakages that may arise in real-world deployments. Our evaluation shows that offline payments can be fast and that the central bank can handle high payment loads with moderate computing resources. However, the main limitation of PayOff is that offline payment messages and storage requirements grow in the number of payments that the sender makes or receives without going online in between.",
        "field": "Central Bank Digital Currencies (CBDCs)",
        "link": "http://arxiv.org/abs/2408.06956v1"
    },
    {
        "id": "2407.13776v1",
        "title": "Offline Digital Euro: a Minimum Viable CBDC using Groth-Sahai proofs",
        "authors": [
            "Leon Kempen",
            "Johan Pouwelse"
        ],
        "published": "2024-07-01T09:55:14Z",
        "summary": "Current digital payment solutions are fragile and offer less privacy than traditional cash. Their critical dependency on an online service used to perform and validate transactions makes them void if this service is unreachable. Moreover, no transaction can be executed during server malfunctions or power outages. Due to climate change, the likelihood of extreme weather increases. As extreme weather is a major cause of power outages, the frequency of power outages is expected to increase. The lack of privacy is an inherent result of their account-based design or the use of a public ledger. The critical dependency and lack of privacy can be resolved with a Central Bank Digital Currency that can be used offline. This thesis proposes a design and a first implementation for an offline-first digital euro. The protocol offers complete privacy during transactions using zero-knowledge proofs. Furthermore, transactions can be executed offline without third parties and retroactive double-spending detection is facilitated. To protect the users' privacy, but also guard against money laundering, we have added the following privacy-guarding mechanism. The bank and trusted third parties for law enforcement must collaborate to decrypt transactions, revealing the digital pseudonym used in the transaction. Importantly, the transaction can be decrypted without decrypting prior transactions attached to the digital euro. The protocol has a working initial implementation showcasing its usability and demonstrating functionality.",
        "field": "Central Bank Digital Currencies (CBDCs)",
        "link": "http://arxiv.org/abs/2407.13776v1"
    },
    {
        "id": "2405.10678v1",
        "title": "IT Strategic alignment in the decentralized finance (DeFi): CBDC and digital currencies",
        "authors": [
            "Carlos Alberto Durigan Junior",
            "Fernando Jose Barbin Laurindo"
        ],
        "published": "2024-05-17T10:19:20Z",
        "summary": "Cryptocurrency can be understood as a digital asset transacted among participants in the crypto economy. Every cryptocurrency must have an associated Blockchain. Blockchain is a Distributed Ledger Technology (DLT) which supports cryptocurrencies, this may be considered as the most promising disruptive technology in the industry 4.0 context. Decentralized finance (DeFi) is a Blockchain-based financial infrastructure, the term generally refers to an open, permissionless, and highly interoperable protocol stack built on public smart contract platforms, such as the Ethereum Blockchain. It replicates existing financial services in a more open and transparent way. DeFi does not rely on intermediaries and centralized institutions. Instead, it is based on open protocols and decentralized applications (Dapps). Considering that there are many digital coins, stablecoins and central bank digital currencies (CBDCs), these currencies should interact among each other sometime. For this interaction the Information Technology elements play an important whole as enablers and IT strategic alignment. This paper considers the strategic alignment model proposed by Henderson and Venkatraman (1993) and Luftman (1996). This paper seeks to answer two main questions 1) What are the common IT elements in the DeFi? And 2) How the elements connect to the IT strategic alignment in DeFi? Through a Systematic Literature Review (SLR). Results point out that there are many IT elements already mentioned by literature, however there is a lack in the literature about the connection between IT elements and IT strategic alignment in a Decentralized Finance (DeFi) architectural network. After final considerations, limitations and future research agenda are presented. Keywords: IT Strategic alignment, Decentralized Finance (DeFi), Cryptocurrency, Digital Economy.",
        "field": "Central Bank Digital Currencies (CBDCs)",
        "link": "http://arxiv.org/abs/2405.10678v1"
    },
    {
        "id": "2503.09823v1",
        "title": "Data Traceability for Privacy Alignment",
        "authors": [
            "Kevin Liao",
            "Shreya Thipireddy",
            "Daniel Weitzner"
        ],
        "published": "2025-03-12T20:42:23Z",
        "summary": "This paper offers a new privacy approach for the growing ecosystem of services--ranging from open banking to healthcare--dependent on sensitive personal data sharing between individuals and third-parties. While these services offer significant benefits, individuals want control over their data, transparency regarding how their data is used, and accountability from third-parties for misuse. However, existing legal and technical mechanisms are inadequate for supporting these needs. A comprehensive approach to the modern privacy challenges of accountable third-party data sharing requires a closer alignment of technical system architecture and legal institutional design. In order to achieve this privacy alignment, we extend traditional security threat modeling and analysis to encompass a broader range of privacy notions than has been typically considered. In particular, we introduce the concept of covert-accountability, which addresses adversaries that may act dishonestly but face potential identification and legal consequences. As a concrete instance of this design approach, we present the OTrace protocol, designed to provide traceable, accountable, consumer-control in third-party data sharing ecosystems. OTrace empowers consumers with the knowledge of where their data is, who has it, what it is being used for, and whom it is being shared with. By applying our alignment framework to OTrace, we demonstrate that OTrace's technical affordances can provide more confident, scalable regulatory oversight when combined with complementary legal mechanisms.",
        "field": "Biometrics in Banking",
        "link": "http://arxiv.org/abs/2503.09823v1"
    },
    {
        "id": "2503.09586v1",
        "title": "Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot",
        "authors": [
            "Andrew Crossman",
            "Andrew R. Plummer",
            "Chandra Sekharudu",
            "Deepak Warrier",
            "Mohammad Yekrangian"
        ],
        "published": "2025-03-12T17:54:18Z",
        "summary": "We present Auspex - a threat modeling system built using a specialized collection of generative artificial intelligence-based methods that capture threat modeling tradecraft. This new approach, called tradecraft prompting, centers on encoding the on-the-ground knowledge of threat modelers within the prompts that drive a generative AI-based threat modeling system. Auspex employs tradecraft prompts in two processing stages. The first stage centers on ingesting and processing system architecture information using prompts that encode threat modeling tradecraft knowledge pertaining to system decomposition and description. The second stage centers on chaining the resulting system analysis through a collection of prompts that encode tradecraft knowledge on threat identification, classification, and mitigation. The two-stage process yields a threat matrix for a system that specifies threat scenarios, threat types, information security categorizations and potential mitigations. Auspex produces formalized threat model output in minutes, relative to the weeks or months a manual process takes. More broadly, the focus on bespoke tradecraft prompting, as opposed to fine-tuning or agent-based add-ons, makes Auspex a lightweight, flexible, modular, and extensible foundational system capable of addressing the complexity, resource, and standardization limitations of both existing manual and automated threat modeling processes. In this connection, we establish the baseline value of Auspex to threat modelers through an evaluation procedure based on feedback collected from cybersecurity subject matter experts measuring the quality and utility of threat models generated by Auspex on real banking systems. We conclude with a discussion of system performance and plans for enhancements to Auspex.",
        "field": "Biometrics in Banking",
        "link": "http://arxiv.org/abs/2503.09586v1"
    },
    {
        "id": "2503.00070v1",
        "title": "Systematic Review of Cybersecurity in Banking: Evolution from Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain, Policies and Practice",
        "authors": [
            "Tue Nhi Tran"
        ],
        "published": "2025-02-27T14:17:06Z",
        "summary": "Throughout the history from pre-industry 4.0 to post-industry 4.0, cybersecurity at banks has undergone significant changes. Pre-industry 4.0 cyber security at banks relied on individual security methods that were highly manual and had low accuracy. When moving to post-industry 4.0, cybersecurity at banks had a major turning point with security methods that combined different technologies such as Artificial Intelligence (AI), Blockchain, IoT, automating necessary processes and significantly increasing the defence layer for banks. However, along with the development of new technologies, the current challenge of cybersecurity at banks lies in scalability, high costs and resources in both money and time for R&D of defence methods along with the threat of high-tech cybercriminals growing and expanding. This report goes from introducing the importance of cybersecurity at banks, analyzing their management, operational and business objectives, evaluating pre-industry 4.0 technologies used for cybersecurity at banks to assessing post-industry 4.0 technologies focusing on Artificial Intelligence and Blockchain, discussing current policies and practices and ending with discussing key advantages and challenges for 4.0 technologies and recommendations for further developing cybersecurity at banks.",
        "field": "Biometrics in Banking",
        "link": "http://arxiv.org/abs/2503.00070v1"
    },
    {
        "id": "2503.10644v1",
        "title": "Combined climate stress testing of supply-chain networks and the financial system with nation-wide firm-level emission estimates",
        "authors": [
            "Zlata Tabachová",
            "Christian Diem",
            "Johannes Stangl",
            "András Borsos",
            "Stefan Thurner"
        ],
        "published": "2025-02-25T20:25:47Z",
        "summary": "On the way towards carbon neutrality, climate stress testing provides estimates for the physical and transition risks that climate change poses to the economy and the financial system. Missing firm-level CO2 emissions data severely impedes the assessment of transition risks originating from carbon pricing. Based on the individual emissions of all Hungarian firms (410,523), as estimated from their fossil fuel purchases, we conduct a stress test of both actual and hypothetical carbon pricing policies. Using a simple 1:1 economic ABM and introducing the new carbon-to-profit ratio, we identify firms that become unprofitable and default, and estimate the respective loan write-offs. We find that 45% of all companies are directly exposed to carbon pricing. At a price of 45 EUR/t, direct economic losses of 1.3% of total sales and bank equity losses of 1.2% are expected. Secondary default cascades in supply chain networks could increase these losses by 300% to 4000%, depending on firms' ability to substitute essential inputs. To reduce transition risks, firms should reduce their dependence on essential inputs from supply chains with high CO2 exposure. We discuss the implications of different policy implementations on these transition risks.",
        "field": "Biometrics in Banking",
        "link": "http://arxiv.org/abs/2503.10644v1"
    },
    {
        "id": "2502.14766v2",
        "title": "Multi-Layer Deep xVA: Structural Credit Models, Measure Changes and Convergence Analysis",
        "authors": [
            "Kristoffer Andersson",
            "Alessandro Gnoatto"
        ],
        "published": "2025-02-20T17:41:55Z",
        "summary": "We propose a structural default model for portfolio-wide valuation adjustments (xVAs) and represent it as a system of coupled backward stochastic differential equations. The framework is divided into four layers, each capturing a key component: (i) clean values, (ii) initial margin and Collateral Valuation Adjustment (ColVA), (iii) Credit/Debit Valuation Adjustments (CVA/DVA) together with Margin Valuation Adjustment (MVA), and (iv) Funding Valuation Adjustment (FVA). Because these layers depend on one another through collateral and default effects, a naive Monte Carlo approach would require deeply nested simulations, making the problem computationally intractable. To address this challenge, we use an iterative deep BSDE approach, handling each layer sequentially so that earlier outputs serve as inputs to the subsequent layers. Initial margin is computed via deep quantile regression to reflect margin requirements over the Margin Period of Risk. We also adopt a change-of-measure method that highlights rare but significant defaults of the bank or counterparty, ensuring that these events are accurately captured in the training process. We further extend Han and Long's (2020) a posteriori error analysis to BSDEs on bounded domains. Due to the random exit from the domain, we obtain an order of convergence of $\\mathcal{O}(h^{1/4-\\epsilon})$ rather than the usual $\\mathcal{O}(h^{1/2})$. Numerical experiments illustrate that this method drastically reduces computational demands and successfully scales to high-dimensional, non-symmetric portfolios. The results confirm its effectiveness and accuracy, offering a practical alternative to nested Monte Carlo simulations in multi-counterparty xVA analyses.",
        "field": "Biometrics in Banking",
        "link": "http://arxiv.org/abs/2502.14766v2"
    },
    {
        "id": "2503.09823v1",
        "title": "Data Traceability for Privacy Alignment",
        "authors": [
            "Kevin Liao",
            "Shreya Thipireddy",
            "Daniel Weitzner"
        ],
        "published": "2025-03-12T20:42:23Z",
        "summary": "This paper offers a new privacy approach for the growing ecosystem of services--ranging from open banking to healthcare--dependent on sensitive personal data sharing between individuals and third-parties. While these services offer significant benefits, individuals want control over their data, transparency regarding how their data is used, and accountability from third-parties for misuse. However, existing legal and technical mechanisms are inadequate for supporting these needs. A comprehensive approach to the modern privacy challenges of accountable third-party data sharing requires a closer alignment of technical system architecture and legal institutional design. In order to achieve this privacy alignment, we extend traditional security threat modeling and analysis to encompass a broader range of privacy notions than has been typically considered. In particular, we introduce the concept of covert-accountability, which addresses adversaries that may act dishonestly but face potential identification and legal consequences. As a concrete instance of this design approach, we present the OTrace protocol, designed to provide traceable, accountable, consumer-control in third-party data sharing ecosystems. OTrace empowers consumers with the knowledge of where their data is, who has it, what it is being used for, and whom it is being shared with. By applying our alignment framework to OTrace, we demonstrate that OTrace's technical affordances can provide more confident, scalable regulatory oversight when combined with complementary legal mechanisms.",
        "field": "Edge Computing in Banking",
        "link": "http://arxiv.org/abs/2503.09823v1"
    },
    {
        "id": "2503.09586v1",
        "title": "Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot",
        "authors": [
            "Andrew Crossman",
            "Andrew R. Plummer",
            "Chandra Sekharudu",
            "Deepak Warrier",
            "Mohammad Yekrangian"
        ],
        "published": "2025-03-12T17:54:18Z",
        "summary": "We present Auspex - a threat modeling system built using a specialized collection of generative artificial intelligence-based methods that capture threat modeling tradecraft. This new approach, called tradecraft prompting, centers on encoding the on-the-ground knowledge of threat modelers within the prompts that drive a generative AI-based threat modeling system. Auspex employs tradecraft prompts in two processing stages. The first stage centers on ingesting and processing system architecture information using prompts that encode threat modeling tradecraft knowledge pertaining to system decomposition and description. The second stage centers on chaining the resulting system analysis through a collection of prompts that encode tradecraft knowledge on threat identification, classification, and mitigation. The two-stage process yields a threat matrix for a system that specifies threat scenarios, threat types, information security categorizations and potential mitigations. Auspex produces formalized threat model output in minutes, relative to the weeks or months a manual process takes. More broadly, the focus on bespoke tradecraft prompting, as opposed to fine-tuning or agent-based add-ons, makes Auspex a lightweight, flexible, modular, and extensible foundational system capable of addressing the complexity, resource, and standardization limitations of both existing manual and automated threat modeling processes. In this connection, we establish the baseline value of Auspex to threat modelers through an evaluation procedure based on feedback collected from cybersecurity subject matter experts measuring the quality and utility of threat models generated by Auspex on real banking systems. We conclude with a discussion of system performance and plans for enhancements to Auspex.",
        "field": "Edge Computing in Banking",
        "link": "http://arxiv.org/abs/2503.09586v1"
    },
    {
        "id": "2503.00070v1",
        "title": "Systematic Review of Cybersecurity in Banking: Evolution from Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain, Policies and Practice",
        "authors": [
            "Tue Nhi Tran"
        ],
        "published": "2025-02-27T14:17:06Z",
        "summary": "Throughout the history from pre-industry 4.0 to post-industry 4.0, cybersecurity at banks has undergone significant changes. Pre-industry 4.0 cyber security at banks relied on individual security methods that were highly manual and had low accuracy. When moving to post-industry 4.0, cybersecurity at banks had a major turning point with security methods that combined different technologies such as Artificial Intelligence (AI), Blockchain, IoT, automating necessary processes and significantly increasing the defence layer for banks. However, along with the development of new technologies, the current challenge of cybersecurity at banks lies in scalability, high costs and resources in both money and time for R&D of defence methods along with the threat of high-tech cybercriminals growing and expanding. This report goes from introducing the importance of cybersecurity at banks, analyzing their management, operational and business objectives, evaluating pre-industry 4.0 technologies used for cybersecurity at banks to assessing post-industry 4.0 technologies focusing on Artificial Intelligence and Blockchain, discussing current policies and practices and ending with discussing key advantages and challenges for 4.0 technologies and recommendations for further developing cybersecurity at banks.",
        "field": "Edge Computing in Banking",
        "link": "http://arxiv.org/abs/2503.00070v1"
    },
    {
        "id": "2503.10644v1",
        "title": "Combined climate stress testing of supply-chain networks and the financial system with nation-wide firm-level emission estimates",
        "authors": [
            "Zlata Tabachová",
            "Christian Diem",
            "Johannes Stangl",
            "András Borsos",
            "Stefan Thurner"
        ],
        "published": "2025-02-25T20:25:47Z",
        "summary": "On the way towards carbon neutrality, climate stress testing provides estimates for the physical and transition risks that climate change poses to the economy and the financial system. Missing firm-level CO2 emissions data severely impedes the assessment of transition risks originating from carbon pricing. Based on the individual emissions of all Hungarian firms (410,523), as estimated from their fossil fuel purchases, we conduct a stress test of both actual and hypothetical carbon pricing policies. Using a simple 1:1 economic ABM and introducing the new carbon-to-profit ratio, we identify firms that become unprofitable and default, and estimate the respective loan write-offs. We find that 45% of all companies are directly exposed to carbon pricing. At a price of 45 EUR/t, direct economic losses of 1.3% of total sales and bank equity losses of 1.2% are expected. Secondary default cascades in supply chain networks could increase these losses by 300% to 4000%, depending on firms' ability to substitute essential inputs. To reduce transition risks, firms should reduce their dependence on essential inputs from supply chains with high CO2 exposure. We discuss the implications of different policy implementations on these transition risks.",
        "field": "Edge Computing in Banking",
        "link": "http://arxiv.org/abs/2503.10644v1"
    },
    {
        "id": "2502.14766v2",
        "title": "Multi-Layer Deep xVA: Structural Credit Models, Measure Changes and Convergence Analysis",
        "authors": [
            "Kristoffer Andersson",
            "Alessandro Gnoatto"
        ],
        "published": "2025-02-20T17:41:55Z",
        "summary": "We propose a structural default model for portfolio-wide valuation adjustments (xVAs) and represent it as a system of coupled backward stochastic differential equations. The framework is divided into four layers, each capturing a key component: (i) clean values, (ii) initial margin and Collateral Valuation Adjustment (ColVA), (iii) Credit/Debit Valuation Adjustments (CVA/DVA) together with Margin Valuation Adjustment (MVA), and (iv) Funding Valuation Adjustment (FVA). Because these layers depend on one another through collateral and default effects, a naive Monte Carlo approach would require deeply nested simulations, making the problem computationally intractable. To address this challenge, we use an iterative deep BSDE approach, handling each layer sequentially so that earlier outputs serve as inputs to the subsequent layers. Initial margin is computed via deep quantile regression to reflect margin requirements over the Margin Period of Risk. We also adopt a change-of-measure method that highlights rare but significant defaults of the bank or counterparty, ensuring that these events are accurately captured in the training process. We further extend Han and Long's (2020) a posteriori error analysis to BSDEs on bounded domains. Due to the random exit from the domain, we obtain an order of convergence of $\\mathcal{O}(h^{1/4-\\epsilon})$ rather than the usual $\\mathcal{O}(h^{1/2})$. Numerical experiments illustrate that this method drastically reduces computational demands and successfully scales to high-dimensional, non-symmetric portfolios. The results confirm its effectiveness and accuracy, offering a practical alternative to nested Monte Carlo simulations in multi-counterparty xVA analyses.",
        "field": "Edge Computing in Banking",
        "link": "http://arxiv.org/abs/2502.14766v2"
    },
    {
        "id": "2503.14462v1",
        "title": "Blockchain with proof of quantum work",
        "authors": [
            "Mohammad H. Amin",
            "Jack Raymond",
            "Daniel Kinn",
            "Firas Hamze",
            "Kelsey Hamer",
            "Joel Pasvolsky",
            "William Bernoudy",
            "Andrew D. King",
            "Samuel Kortas"
        ],
        "published": "2025-03-18T17:37:22Z",
        "summary": "We propose a blockchain architecture in which mining requires a quantum computer. The consensus mechanism is based on proof of quantum work, a quantum-enhanced alternative to traditional proof of work that leverages quantum supremacy to make mining intractable for classical computers. We have refined the blockchain framework to incorporate the probabilistic nature of quantum mechanics, ensuring stability against sampling errors and hardware inaccuracies. To validate our approach, we implemented a prototype blockchain on four D-Wave$^{\\rm TM}$ quantum annealing processors geographically distributed within North America, demonstrating stable operation across hundreds of thousands of quantum hashing operations. Our experimental protocol follows the same approach used in the recent demonstration of quantum supremacy [1], ensuring that classical computers cannot efficiently perform the same computation task. By replacing classical machines with quantum systems for mining, it is possible to significantly reduce the energy consumption and environmental impact traditionally associated with blockchain mining. Beyond serving as a proof of concept for a meaningful application of quantum computing, this work highlights the potential for other near-term quantum computing applications using existing technology.",
        "field": "RegTech (Regulatory Technology)",
        "link": "http://arxiv.org/abs/2503.14462v1"
    },
    {
        "id": "2503.14279v1",
        "title": "Anti-Tamper Radio meets Reconfigurable Intelligent Surface for System-Level Tamper Detection",
        "authors": [
            "Maryam Shaygan Tabar",
            "Johannes Kortz",
            "Paul Staat",
            "Harald Elders-Boll",
            "Christof Paar",
            "Christian Zenger"
        ],
        "published": "2025-03-18T14:18:31Z",
        "summary": "Many computing systems need to be protected against physical attacks using active tamper detection based on sensors. One technical solution is to employ an ATR (Anti-Tamper Radio) approach, analyzing the radio wave propagation effects within a protected device to detect unauthorized physical alterations. However, ATR systems face key challenges in terms of susceptibility to signal manipulation attacks, limited reliability due to environmental noise, and regulatory constraints from wide bandwidth usage. In this work, we propose and experimentally evaluate an ATR system complemented by an RIS to dynamically reconfigure the wireless propagation environment. We show that this approach can enhance resistance against signal manipulation attacks, reduce bandwidth requirements from several~GHz down to as low as 20 MHz, and improve robustness to environmental disturbances such as internal fan movements. Our work demonstrates that RIS integration can strengthen the ATR performance to enhance security, sensitivity, and robustness, recognizing the potential of smart radio environments for ATR-based tamper detection",
        "field": "RegTech (Regulatory Technology)",
        "link": "http://arxiv.org/abs/2503.14279v1"
    },
    {
        "id": "2503.14006v1",
        "title": "Securing Automated Insulin Delivery Systems: A Review of Security Threats and Protectives Strategies",
        "authors": [
            "Yuchen Niu",
            "Siew-Kei Lam"
        ],
        "published": "2025-03-18T08:11:19Z",
        "summary": "Automated insulin delivery (AID) systems have emerged as a significant technological advancement in diabetes care. These systems integrate a continuous glucose monitor, an insulin pump, and control algorithms to automate insulin delivery, reducing the burden of self-management and offering enhanced glucose control. However, the increasing reliance on wireless connectivity and software control has exposed AID systems to critical security risks that could result in life-threatening treatment errors. This review first presents a comprehensive examination of the security landscape, covering technical vulnerabilities, legal frameworks, and commercial product considerations, and an analysis of existing research on attack vectors, defence mechanisms, as well as evaluation methods and resources for AID systems. Despite recent advancements, several open challenges remain in achieving secure AID systems, particularly in standardising security evaluation frameworks and developing comprehensive, lightweight, and adaptive defence strategies. As one of the most widely adopted and extensively studied physiologic closed-loop control systems, this review serves as a valuable reference for understanding security challenges and solutions applicable to analogous medical systems.",
        "field": "RegTech (Regulatory Technology)",
        "link": "http://arxiv.org/abs/2503.14006v1"
    },
    {
        "id": "2503.13419v1",
        "title": "Securing Virtual Reality Experiences: Unveiling and Tackling Cybersickness Attacks with Explainable AI",
        "authors": [
            "Ripan Kumar Kundu",
            "Matthew Denton",
            "Genova Mongalo",
            "Prasad Calyam",
            "Khaza Anuarul Hoque"
        ],
        "published": "2025-03-17T17:49:51Z",
        "summary": "The synergy between virtual reality (VR) and artificial intelligence (AI), specifically deep learning (DL)-based cybersickness detection models, has ushered in unprecedented advancements in immersive experiences by automatically detecting cybersickness severity and adaptively various mitigation techniques, offering a smooth and comfortable VR experience. While this DL-enabled cybersickness detection method provides promising solutions for enhancing user experiences, it also introduces new risks since these models are vulnerable to adversarial attacks; a small perturbation of the input data that is visually undetectable to human observers can fool the cybersickness detection model and trigger unexpected mitigation, thus disrupting user immersive experiences (UIX) and even posing safety risks. In this paper, we present a new type of VR attack, i.e., a cybersickness attack, which successfully stops the triggering of cybersickness mitigation by fooling DL-based cybersickness detection models and dramatically hinders the UIX. Next, we propose a novel explainable artificial intelligence (XAI)-guided cybersickness attack detection framework to detect such attacks in VR to ensure UIX and a comfortable VR experience. We evaluate the proposed attack and the detection framework using two state-of-the-art open-source VR cybersickness datasets: Simulation 2021 and Gameplay dataset. Finally, to verify the effectiveness of our proposed method, we implement the attack and the XAI-based detection using a testbed with a custom-built VR roller coaster simulation with an HTC Vive Pro Eye headset and perform a user study. Our study shows that such an attack can dramatically hinder the UIX. However, our proposed XAI-guided cybersickness attack detection can successfully detect cybersickness attacks and trigger the proper mitigation, effectively reducing VR cybersickness.",
        "field": "RegTech (Regulatory Technology)",
        "link": "http://arxiv.org/abs/2503.13419v1"
    },
    {
        "id": "2503.13255v1",
        "title": "Zero-Knowledge Proof-Based Consensus for Blockchain-Secured Federated Learning",
        "authors": [
            "Tianxing Fu",
            "Jia Hu",
            "Geyong Min",
            "Zi Wang"
        ],
        "published": "2025-03-17T15:13:10Z",
        "summary": "Federated learning (FL) enables multiple participants to collaboratively train machine learning models while ensuring their data remains private and secure. Blockchain technology further enhances FL by providing stronger security, a transparent audit trail, and protection against data tampering and model manipulation. Most blockchain-secured FL systems rely on conventional consensus mechanisms: Proof-of-Work (PoW) is computationally expensive, while Proof-of-Stake (PoS) improves energy efficiency but risks centralization as it inherently favors participants with larger stakes. Recently, learning-based consensus has emerged as an alternative by replacing cryptographic tasks with model training to save energy. However, this approach introduces potential privacy vulnerabilities, as the training process may inadvertently expose sensitive information through gradient sharing and model updates. To address these challenges, we propose a novel Zero-Knowledge Proof of Training (ZKPoT) consensus mechanism. This method leverages the zero-knowledge succinct non-interactive argument of knowledge proof (zk-SNARK) protocol to validate participants' contributions based on their model performance, effectively eliminating the inefficiencies of traditional consensus methods and mitigating the privacy risks posed by learning-based consensus. We analyze our system's security, demonstrating its capacity to prevent the disclosure of sensitive information about local models or training data to untrusted parties during the entire FL process. Extensive experiments demonstrate that our system is robust against privacy and Byzantine attacks while maintaining accuracy and utility without trade-offs, scalable across various blockchain settings, and efficient in both computation and communication.",
        "field": "RegTech (Regulatory Technology)",
        "link": "http://arxiv.org/abs/2503.13255v1"
    },
    {
        "id": "2503.12952v1",
        "title": "Performance Analysis and Industry Deployment of Post-Quantum Cryptography Algorithms",
        "authors": [
            "Elif Dicle Demir",
            "Buse Bilgin",
            "Mehmet Cengiz Onbasli"
        ],
        "published": "2025-03-17T09:06:03Z",
        "summary": "As quantum computing advances, modern cryptographic standards face an existential threat, necessitating a transition to post-quantum cryptography (PQC). The National Institute of Standards and Technology (NIST) has selected CRYSTALS-Kyber and CRYSTALS-Dilithium as standardized PQC algorithms for secure key exchange and digital signatures, respectively. This study conducts a comprehensive performance analysis of these algorithms by benchmarking execution times across cryptographic operations such as key generation, encapsulation, decapsulation, signing, and verification. Additionally, the impact of AVX2 optimizations is evaluated to assess hardware acceleration benefits. Our findings demonstrate that Kyber and Dilithium achieve efficient execution times, outperforming classical cryptographic schemes such as RSA and ECDSA at equivalent security levels. Beyond technical performance, the real-world deployment of PQC introduces challenges in telecommunications networks, where large-scale infrastructure upgrades, interoperability with legacy systems, and regulatory constraints must be addressed. This paper examines the feasibility of PQC adoption in telecom environments, highlighting key transition challenges, security risks, and implementation strategies. Through industry case studies, we illustrate how telecom operators are integrating PQC into 5G authentication, subscriber identity protection, and secure communications. Our analysis provides insights into the computational trade-offs, deployment considerations, and standardization efforts shaping the future of quantum-safe cryptographic infrastructure.",
        "field": "Digital Identity Verification",
        "link": "http://arxiv.org/abs/2503.12952v1"
    },
    {
        "id": "2503.14284v1",
        "title": "Entente: Cross-silo Intrusion Detection on Network Log Graphs with Federated Learning",
        "authors": [
            "Jiacen Xu",
            "Chenang Li",
            "Yu Zheng",
            "Zhou Li"
        ],
        "published": "2025-03-18T14:21:24Z",
        "summary": "Graph-based Network Intrusion Detection System (GNIDS) has gained significant momentum in detecting sophisticated cyber-attacks, like Advanced Persistent Threat (APT), in an organization or across organizations. Though achieving satisfying detection accuracy and adapting to ever-changing attacks and normal patterns, all prior GNIDSs assume the centralized data settings directly, but non-trivial data collection is not always practical under privacy regulations nowadays. We argue that training a GNIDS model has to consider privacy regulations, and propose to leverage federated learning (FL) to address this prominent challenge. Yet, directly applying FL to GNIDS is unlikely to succeed, due to issues like non-IID (independent and identically distributed) graph data over clients and the diverse design choices taken by different GNIDS. We address these issues with a set of novel techniques tailored to the graph datasets, including reference graph synthesis, graph sketching and adaptive contribution scaling, and develop a new system Entente. We evaluate Entente on the large-scale LANL, OpTC and Pivoting datasets. The result shows Entente outperforms the other baseline FL algorithms and sometimes even the non-FL GNIDS. We also evaluate Entente under FL poisoning attacks tailored to the GNIDS setting, and show Entente is able to bound the attack success rate to low values. Overall, our result suggests building cross-silo GNIDS is feasible and we hope to encourage more efforts in this direction.",
        "field": "5G and 6G Networks",
        "link": "http://arxiv.org/abs/2503.14284v1"
    },
    {
        "id": "2503.14213v1",
        "title": "Rolling Forward: Enhancing LightGCN with Causal Graph Convolution for Credit Bond Recommendation",
        "authors": [
            "Ashraf Ghiye",
            "Baptiste Barreau",
            "Laurent Carlier",
            "Michalis Vazirgiannis"
        ],
        "published": "2025-03-18T12:47:01Z",
        "summary": "Graph Neural Networks have significantly advanced research in recommender systems over the past few years. These methods typically capture global interests using aggregated past interactions and rely on static embeddings of users and items over extended periods of time. While effective in some domains, these methods fall short in many real-world scenarios, especially in finance, where user interests and item popularity evolve rapidly over time. To address these challenges, we introduce a novel extension to Light Graph Convolutional Network (LightGCN) designed to learn temporal node embeddings that capture dynamic interests. Our approach employs causal convolution to maintain a forward-looking model architecture. By preserving the chronological order of user-item interactions and introducing a dynamic update mechanism for embeddings through a sliding window, the proposed model generates well-timed and contextually relevant recommendations. Extensive experiments on a real-world dataset from BNP Paribas demonstrate that our approach significantly enhances the performance of LightGCN while maintaining the simplicity and efficiency of its architecture. Our findings provide new insights into designing graph-based recommender systems in time-sensitive applications, particularly for financial product recommendations.",
        "field": "5G and 6G Networks",
        "link": "http://arxiv.org/abs/2503.14213v1"
    },
    {
        "id": "2503.13224v1",
        "title": "ProDiF: Protecting Domain-Invariant Features to Secure Pre-Trained Models Against Extraction",
        "authors": [
            "Tong Zhou",
            "Shijin Duan",
            "Gaowen Liu",
            "Charles Fleming",
            "Ramana Rao Kompella",
            "Shaolei Ren",
            "Xiaolin Xu"
        ],
        "published": "2025-03-17T14:37:42Z",
        "summary": "Pre-trained models are valuable intellectual property, capturing both domain-specific and domain-invariant features within their weight spaces. However, model extraction attacks threaten these assets by enabling unauthorized source-domain inference and facilitating cross-domain transfer via the exploitation of domain-invariant features. In this work, we introduce **ProDiF**, a novel framework that leverages targeted weight space manipulation to secure pre-trained models against extraction attacks. **ProDiF** quantifies the transferability of filters and perturbs the weights of critical filters in unsecured memory, while preserving actual critical weights in a Trusted Execution Environment (TEE) for authorized users. A bi-level optimization further ensures resilience against adaptive fine-tuning attacks. Experimental results show that **ProDiF** reduces source-domain accuracy to near-random levels and decreases cross-domain transferability by 74.65\\%, providing robust protection for pre-trained models. This work offers comprehensive protection for pre-trained DNN models and highlights the potential of weight space manipulation as a novel approach to model security.",
        "field": "5G and 6G Networks",
        "link": "http://arxiv.org/abs/2503.13224v1"
    },
    {
        "id": "2503.12952v1",
        "title": "Performance Analysis and Industry Deployment of Post-Quantum Cryptography Algorithms",
        "authors": [
            "Elif Dicle Demir",
            "Buse Bilgin",
            "Mehmet Cengiz Onbasli"
        ],
        "published": "2025-03-17T09:06:03Z",
        "summary": "As quantum computing advances, modern cryptographic standards face an existential threat, necessitating a transition to post-quantum cryptography (PQC). The National Institute of Standards and Technology (NIST) has selected CRYSTALS-Kyber and CRYSTALS-Dilithium as standardized PQC algorithms for secure key exchange and digital signatures, respectively. This study conducts a comprehensive performance analysis of these algorithms by benchmarking execution times across cryptographic operations such as key generation, encapsulation, decapsulation, signing, and verification. Additionally, the impact of AVX2 optimizations is evaluated to assess hardware acceleration benefits. Our findings demonstrate that Kyber and Dilithium achieve efficient execution times, outperforming classical cryptographic schemes such as RSA and ECDSA at equivalent security levels. Beyond technical performance, the real-world deployment of PQC introduces challenges in telecommunications networks, where large-scale infrastructure upgrades, interoperability with legacy systems, and regulatory constraints must be addressed. This paper examines the feasibility of PQC adoption in telecom environments, highlighting key transition challenges, security risks, and implementation strategies. Through industry case studies, we illustrate how telecom operators are integrating PQC into 5G authentication, subscriber identity protection, and secure communications. Our analysis provides insights into the computational trade-offs, deployment considerations, and standardization efforts shaping the future of quantum-safe cryptographic infrastructure.",
        "field": "5G and 6G Networks",
        "link": "http://arxiv.org/abs/2503.12952v1"
    },
    {
        "id": "2503.12625v1",
        "title": "SCOOP: CoSt-effective COngestiOn Attacks in Payment Channel Networks",
        "authors": [
            "Mohammed Ababneh",
            "Kartick Kolachala",
            "Roopa Vishwanathan"
        ],
        "published": "2025-03-16T19:41:56Z",
        "summary": "Payment channel networks (PCNs) are a promising solution to address blockchain scalability and throughput challenges, However, the security of PCNs and their vulnerability to attacks are not sufficiently studied. In this paper, we introduce SCOOP, a framework that includes two novel congestion attacks on PCNs. These attacks consider the minimum transferable amount along a path (path capacity) and the number of channels involved (path length), formulated as linear optimization problems. The first attack allocates the attacker's budget to achieve a specific congestion threshold, while the second maximizes congestion under budget constraints. Simulation results show the effectiveness of the proposed attack formulations in comparison to other attack strategies. Specifically, the results indicate that the first attack provides around a 40\\% improvement in congestion performance, while the second attack offers approximately a 50\\% improvement in comparison to the state-of-the-art. Moreover, in terms of payment to congestion efficiency, the first attack is about 60\\% more efficient, and the second attack is around 90\\% more efficient in comparison to state-of-the-art",
        "field": "5G and 6G Networks",
        "link": "http://arxiv.org/abs/2503.12625v1"
    },
    {
        "id": "2503.10171v1",
        "title": "Verifiable, Efficient and Confidentiality-Preserving Graph Search with Transparency",
        "authors": [
            "Qiuhao Wang",
            "Xu Yang",
            "Yiwei Liu",
            "Saiyu Qi",
            "Hongguang Zhao",
            "Ke Li",
            "Yong Qi"
        ],
        "published": "2025-03-13T08:53:53Z",
        "summary": "Graph databases have garnered extensive attention and research due to their ability to manage relationships between entities efficiently. Today, many graph search services have been outsourced to a third-party server to facilitate storage and computational support. Nevertheless, the outsourcing paradigm may invade the privacy of graphs. PeGraph is the latest scheme achieving encrypted search over social graphs to address the privacy leakage, which maintains two data structures XSet and TSet motivated by the OXT technology to support encrypted conjunctive search. However, PeGraph still exhibits limitations inherent to the underlying OXT. It does not provide transparent search capabilities, suffers from expensive computation and result pattern leakages, and it fails to support search over dynamic encrypted graph database and results verification. In this paper, we propose SecGraph to address the first two limitations, which adopts a novel system architecture that leverages an SGX-enabled cloud server to provide users with secure and transparent search services since the secret key protection and computational overhead have been offloaded to the cloud server. Besides, we design an LDCF-encoded XSet based on the Logarithmic Dynamic Cuckoo Filter to facilitate efficient plaintext computation in trusted memory, effectively mitigating the risks of result pattern leakage and performance degradation due to exceeding the limited trusted memory capacity. Finally, we design a new dynamic version of TSet named Twin-TSet to enable conjunctive search over dynamic encrypted graph database. In order to support verifiable search, we further propose VSecGraph, which utilizes a procedure-oriented verification method to verify all data structures loaded into the trusted memory, thus bypassing the computational overhead associated with the client's local verification.",
        "field": "Digital Twins",
        "link": "http://arxiv.org/abs/2503.10171v1"
    },
    {
        "id": "2503.03539v1",
        "title": "Data Sharing, Privacy and Security Considerations in the Energy Sector: A Review from Technical Landscape to Regulatory Specifications",
        "authors": [
            "Shiliang Zhang",
            "Sabita Maharjan",
            "Lee Andrew Bygrave",
            "Shui Yu"
        ],
        "published": "2025-03-05T14:23:56Z",
        "summary": "Decarbonization, decentralization and digitalization are the three key elements driving the twin energy transition. The energy system is evolving to a more data driven ecosystem, leading to the need of communication and storage of large amount of data of different resolution from the prosumers and other stakeholders in the energy ecosystem. While the energy system is certainly advancing, this paradigm shift is bringing in new privacy and security issues related to collection, processing and storage of data - not only from the technical dimension, but also from the regulatory perspective. Understanding data privacy and security in the evolving energy system, regarding regulatory compliance, is an immature field of research. Contextualized knowledge of how related issues are regulated is still in its infancy, and the practical and technical basis for the regulatory framework for data privacy and security is not clear. To fill this gap, this paper conducts a comprehensive review of the data-related issues for the energy system by integrating both technical and regulatory dimensions. We start by reviewing open-access data, data communication and data-processing techniques for the energy system, and use it as the basis to connect the analysis of data-related issues from the integrated perspective. We classify the issues into three categories: (i) data-sharing among energy end users and stakeholders (ii) privacy of end users, and (iii) cyber security, and then explore these issues from a regulatory perspective. We analyze the evolution of related regulations, and introduce the relevant regulatory initiatives for the categorized issues in terms of regulatory definitions, concepts, principles, rights and obligations in the context of energy systems. Finally, we provide reflections on the gaps that still exist, and guidelines for regulatory frameworks for a truly participatory energy system.",
        "field": "Digital Twins",
        "link": "http://arxiv.org/abs/2503.03539v1"
    },
    {
        "id": "2502.19341v1",
        "title": "Unveiling Wireless Users' Locations via Modulation Classification-based Passive Attack",
        "authors": [
            "Ali Hanif",
            "Abdulrahman Katranji",
            "Nour Kouzayha",
            "Muhammad Mahboob Ur Rahman",
            "Tareq Y. Al-Naffouri"
        ],
        "published": "2025-02-26T17:32:38Z",
        "summary": "The broadcast nature of the wireless medium and openness of wireless standards, e.g., 3GPP releases 16-20, invite adversaries to launch various active and passive attacks on cellular and other wireless networks. This work identifies one such loose end of wireless standards and presents a novel passive attack method enabling an eavesdropper (Eve) to localize a line of sight wireless user (Bob) who is communicating with a base station or WiFi access point (Alice). The proposed attack involves two phases. In the first phase, Eve performs modulation classification by intercepting the downlink channel between Alice and Bob. This enables Eve to utilize the publicly available modulation and coding scheme (MCS) tables to do pesudo-ranging, i.e., the Eve determines the ring within which Bob is located, which drastically reduces the search space. In the second phase, Eve sniffs the uplink channel, and employs multiple strategies to further refine Bob's location within the ring. Towards the end, we present our thoughts on how this attack can be extended to non-line-of-sight scenarios, and how this attack could act as a scaffolding to construct a malicious digital twin map.",
        "field": "Digital Twins",
        "link": "http://arxiv.org/abs/2502.19341v1"
    },
    {
        "id": "2502.03403v1",
        "title": "Lightweight Authenticated Task Offloading in 6G-Cloud Vehicular Twin Networks",
        "authors": [
            "Sarah Al-Shareeda",
            "Fusun Ozguner",
            "Keith Redmill",
            "Trung Q. Duong",
            "Berk Canberk"
        ],
        "published": "2025-02-05T17:43:55Z",
        "summary": "Task offloading management in 6G vehicular networks is crucial for maintaining network efficiency, particularly as vehicles generate substantial data. Integrating secure communication through authentication introduces additional computational and communication overhead, significantly impacting offloading efficiency and latency. This paper presents a unified framework incorporating lightweight Identity-Based Cryptographic (IBC) authentication into task offloading within cloud-based 6G Vehicular Twin Networks (VTNs). Utilizing Proximal Policy Optimization (PPO) in Deep Reinforcement Learning (DRL), our approach optimizes authenticated offloading decisions to minimize latency and enhance resource allocation. Performance evaluation under varying network sizes, task sizes, and data rates reveals that IBC authentication can reduce offloading efficiency by up to 50% due to the added overhead. Besides, increasing network size and task size can further reduce offloading efficiency by up to 91.7%. As a countermeasure, increasing the transmission data rate can improve the offloading performance by as much as 63%, even in the presence of authentication overhead. The code for the simulations and experiments detailed in this paper is available on GitHub for further reference and reproducibility [1].",
        "field": "Digital Twins",
        "link": "http://arxiv.org/abs/2502.03403v1"
    },
    {
        "id": "2501.09802v1",
        "title": "W3ID: A Quantum Computing-Secure Digital Identity System Redefining Standards for Web3 and Digital Twins",
        "authors": [
            "Joseph Yun",
            "Eli Lifton",
            "Eunseo Lee",
            "Yohan Yun",
            "Abigail Song",
            "Joshua Lee",
            "Cristian Jimenez-Bert",
            "Benedict Song",
            "Yejun Lee",
            "Alex Seo",
            "Sijung Yun"
        ],
        "published": "2025-01-16T19:16:08Z",
        "summary": "The rapid advancements in quantum computing present significant threats to existing encryption standards and internet security. Simultaneously, the advent of Web 3.0 marks a transformative era in internet history, emphasizing enhanced data security, decentralization, and user ownership. This white paper introduces the W3ID, an abbreviation of Web3 standard meeting universal digital ID, which is a Universal Digital Identity (UDI) model designed to meet Web3 standards while addressing vulnerabilities posed by quantum computing. W3ID innovatively generates secure Digital Object Identifiers (DOIs) tailored for the decentralized Web 3.0 ecosystem. Additionally, W3ID employs a dual-key system for secure authentication, enhancing both public and private verification mechanisms. To further enhance encryption strength and authentication integrity in the quantum computing era, W3ID incorporates an advanced security mechanism. By requiring quadruple application of SHA-256, with consecutive matches for validation, the system expands the number of possibilities to 256^4, which is approximately 4.3 billion times the current SHA-256 capacity. This dramatic increase in computational complexity ensures that even advanced quantum computing systems would face significant challenges in executing brute-force attacks. W3ID redefines digital identity standards for Web 3.0 and the quantum computing era, setting a new benchmark for security, scalability, and decentralization in the global digital twin ecosystem.",
        "field": "Digital Twins",
        "link": "http://arxiv.org/abs/2501.09802v1"
    },
    {
        "id": "2503.09823v1",
        "title": "Data Traceability for Privacy Alignment",
        "authors": [
            "Kevin Liao",
            "Shreya Thipireddy",
            "Daniel Weitzner"
        ],
        "published": "2025-03-12T20:42:23Z",
        "summary": "This paper offers a new privacy approach for the growing ecosystem of services--ranging from open banking to healthcare--dependent on sensitive personal data sharing between individuals and third-parties. While these services offer significant benefits, individuals want control over their data, transparency regarding how their data is used, and accountability from third-parties for misuse. However, existing legal and technical mechanisms are inadequate for supporting these needs. A comprehensive approach to the modern privacy challenges of accountable third-party data sharing requires a closer alignment of technical system architecture and legal institutional design. In order to achieve this privacy alignment, we extend traditional security threat modeling and analysis to encompass a broader range of privacy notions than has been typically considered. In particular, we introduce the concept of covert-accountability, which addresses adversaries that may act dishonestly but face potential identification and legal consequences. As a concrete instance of this design approach, we present the OTrace protocol, designed to provide traceable, accountable, consumer-control in third-party data sharing ecosystems. OTrace empowers consumers with the knowledge of where their data is, who has it, what it is being used for, and whom it is being shared with. By applying our alignment framework to OTrace, we demonstrate that OTrace's technical affordances can provide more confident, scalable regulatory oversight when combined with complementary legal mechanisms.",
        "field": "Metaverse Banking",
        "link": "http://arxiv.org/abs/2503.09823v1"
    },
    {
        "id": "2503.09586v1",
        "title": "Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot",
        "authors": [
            "Andrew Crossman",
            "Andrew R. Plummer",
            "Chandra Sekharudu",
            "Deepak Warrier",
            "Mohammad Yekrangian"
        ],
        "published": "2025-03-12T17:54:18Z",
        "summary": "We present Auspex - a threat modeling system built using a specialized collection of generative artificial intelligence-based methods that capture threat modeling tradecraft. This new approach, called tradecraft prompting, centers on encoding the on-the-ground knowledge of threat modelers within the prompts that drive a generative AI-based threat modeling system. Auspex employs tradecraft prompts in two processing stages. The first stage centers on ingesting and processing system architecture information using prompts that encode threat modeling tradecraft knowledge pertaining to system decomposition and description. The second stage centers on chaining the resulting system analysis through a collection of prompts that encode tradecraft knowledge on threat identification, classification, and mitigation. The two-stage process yields a threat matrix for a system that specifies threat scenarios, threat types, information security categorizations and potential mitigations. Auspex produces formalized threat model output in minutes, relative to the weeks or months a manual process takes. More broadly, the focus on bespoke tradecraft prompting, as opposed to fine-tuning or agent-based add-ons, makes Auspex a lightweight, flexible, modular, and extensible foundational system capable of addressing the complexity, resource, and standardization limitations of both existing manual and automated threat modeling processes. In this connection, we establish the baseline value of Auspex to threat modelers through an evaluation procedure based on feedback collected from cybersecurity subject matter experts measuring the quality and utility of threat models generated by Auspex on real banking systems. We conclude with a discussion of system performance and plans for enhancements to Auspex.",
        "field": "Metaverse Banking",
        "link": "http://arxiv.org/abs/2503.09586v1"
    },
    {
        "id": "2503.00070v1",
        "title": "Systematic Review of Cybersecurity in Banking: Evolution from Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain, Policies and Practice",
        "authors": [
            "Tue Nhi Tran"
        ],
        "published": "2025-02-27T14:17:06Z",
        "summary": "Throughout the history from pre-industry 4.0 to post-industry 4.0, cybersecurity at banks has undergone significant changes. Pre-industry 4.0 cyber security at banks relied on individual security methods that were highly manual and had low accuracy. When moving to post-industry 4.0, cybersecurity at banks had a major turning point with security methods that combined different technologies such as Artificial Intelligence (AI), Blockchain, IoT, automating necessary processes and significantly increasing the defence layer for banks. However, along with the development of new technologies, the current challenge of cybersecurity at banks lies in scalability, high costs and resources in both money and time for R&D of defence methods along with the threat of high-tech cybercriminals growing and expanding. This report goes from introducing the importance of cybersecurity at banks, analyzing their management, operational and business objectives, evaluating pre-industry 4.0 technologies used for cybersecurity at banks to assessing post-industry 4.0 technologies focusing on Artificial Intelligence and Blockchain, discussing current policies and practices and ending with discussing key advantages and challenges for 4.0 technologies and recommendations for further developing cybersecurity at banks.",
        "field": "Metaverse Banking",
        "link": "http://arxiv.org/abs/2503.00070v1"
    },
    {
        "id": "2503.10644v1",
        "title": "Combined climate stress testing of supply-chain networks and the financial system with nation-wide firm-level emission estimates",
        "authors": [
            "Zlata Tabachová",
            "Christian Diem",
            "Johannes Stangl",
            "András Borsos",
            "Stefan Thurner"
        ],
        "published": "2025-02-25T20:25:47Z",
        "summary": "On the way towards carbon neutrality, climate stress testing provides estimates for the physical and transition risks that climate change poses to the economy and the financial system. Missing firm-level CO2 emissions data severely impedes the assessment of transition risks originating from carbon pricing. Based on the individual emissions of all Hungarian firms (410,523), as estimated from their fossil fuel purchases, we conduct a stress test of both actual and hypothetical carbon pricing policies. Using a simple 1:1 economic ABM and introducing the new carbon-to-profit ratio, we identify firms that become unprofitable and default, and estimate the respective loan write-offs. We find that 45% of all companies are directly exposed to carbon pricing. At a price of 45 EUR/t, direct economic losses of 1.3% of total sales and bank equity losses of 1.2% are expected. Secondary default cascades in supply chain networks could increase these losses by 300% to 4000%, depending on firms' ability to substitute essential inputs. To reduce transition risks, firms should reduce their dependence on essential inputs from supply chains with high CO2 exposure. We discuss the implications of different policy implementations on these transition risks.",
        "field": "Metaverse Banking",
        "link": "http://arxiv.org/abs/2503.10644v1"
    },
    {
        "id": "2502.14766v2",
        "title": "Multi-Layer Deep xVA: Structural Credit Models, Measure Changes and Convergence Analysis",
        "authors": [
            "Kristoffer Andersson",
            "Alessandro Gnoatto"
        ],
        "published": "2025-02-20T17:41:55Z",
        "summary": "We propose a structural default model for portfolio-wide valuation adjustments (xVAs) and represent it as a system of coupled backward stochastic differential equations. The framework is divided into four layers, each capturing a key component: (i) clean values, (ii) initial margin and Collateral Valuation Adjustment (ColVA), (iii) Credit/Debit Valuation Adjustments (CVA/DVA) together with Margin Valuation Adjustment (MVA), and (iv) Funding Valuation Adjustment (FVA). Because these layers depend on one another through collateral and default effects, a naive Monte Carlo approach would require deeply nested simulations, making the problem computationally intractable. To address this challenge, we use an iterative deep BSDE approach, handling each layer sequentially so that earlier outputs serve as inputs to the subsequent layers. Initial margin is computed via deep quantile regression to reflect margin requirements over the Margin Period of Risk. We also adopt a change-of-measure method that highlights rare but significant defaults of the bank or counterparty, ensuring that these events are accurately captured in the training process. We further extend Han and Long's (2020) a posteriori error analysis to BSDEs on bounded domains. Due to the random exit from the domain, we obtain an order of convergence of $\\mathcal{O}(h^{1/4-\\epsilon})$ rather than the usual $\\mathcal{O}(h^{1/2})$. Numerical experiments illustrate that this method drastically reduces computational demands and successfully scales to high-dimensional, non-symmetric portfolios. The results confirm its effectiveness and accuracy, offering a practical alternative to nested Monte Carlo simulations in multi-counterparty xVA analyses.",
        "field": "Metaverse Banking",
        "link": "http://arxiv.org/abs/2502.14766v2"
    },
    {
        "id": "2503.14462v1",
        "title": "Blockchain with proof of quantum work",
        "authors": [
            "Mohammad H. Amin",
            "Jack Raymond",
            "Daniel Kinn",
            "Firas Hamze",
            "Kelsey Hamer",
            "Joel Pasvolsky",
            "William Bernoudy",
            "Andrew D. King",
            "Samuel Kortas"
        ],
        "published": "2025-03-18T17:37:22Z",
        "summary": "We propose a blockchain architecture in which mining requires a quantum computer. The consensus mechanism is based on proof of quantum work, a quantum-enhanced alternative to traditional proof of work that leverages quantum supremacy to make mining intractable for classical computers. We have refined the blockchain framework to incorporate the probabilistic nature of quantum mechanics, ensuring stability against sampling errors and hardware inaccuracies. To validate our approach, we implemented a prototype blockchain on four D-Wave$^{\\rm TM}$ quantum annealing processors geographically distributed within North America, demonstrating stable operation across hundreds of thousands of quantum hashing operations. Our experimental protocol follows the same approach used in the recent demonstration of quantum supremacy [1], ensuring that classical computers cannot efficiently perform the same computation task. By replacing classical machines with quantum systems for mining, it is possible to significantly reduce the energy consumption and environmental impact traditionally associated with blockchain mining. Beyond serving as a proof of concept for a meaningful application of quantum computing, this work highlights the potential for other near-term quantum computing applications using existing technology.",
        "field": "Green Finance Technology",
        "link": "http://arxiv.org/abs/2503.14462v1"
    },
    {
        "id": "2503.14006v1",
        "title": "Securing Automated Insulin Delivery Systems: A Review of Security Threats and Protectives Strategies",
        "authors": [
            "Yuchen Niu",
            "Siew-Kei Lam"
        ],
        "published": "2025-03-18T08:11:19Z",
        "summary": "Automated insulin delivery (AID) systems have emerged as a significant technological advancement in diabetes care. These systems integrate a continuous glucose monitor, an insulin pump, and control algorithms to automate insulin delivery, reducing the burden of self-management and offering enhanced glucose control. However, the increasing reliance on wireless connectivity and software control has exposed AID systems to critical security risks that could result in life-threatening treatment errors. This review first presents a comprehensive examination of the security landscape, covering technical vulnerabilities, legal frameworks, and commercial product considerations, and an analysis of existing research on attack vectors, defence mechanisms, as well as evaluation methods and resources for AID systems. Despite recent advancements, several open challenges remain in achieving secure AID systems, particularly in standardising security evaluation frameworks and developing comprehensive, lightweight, and adaptive defence strategies. As one of the most widely adopted and extensively studied physiologic closed-loop control systems, this review serves as a valuable reference for understanding security challenges and solutions applicable to analogous medical systems.",
        "field": "Green Finance Technology",
        "link": "http://arxiv.org/abs/2503.14006v1"
    },
    {
        "id": "2503.13419v1",
        "title": "Securing Virtual Reality Experiences: Unveiling and Tackling Cybersickness Attacks with Explainable AI",
        "authors": [
            "Ripan Kumar Kundu",
            "Matthew Denton",
            "Genova Mongalo",
            "Prasad Calyam",
            "Khaza Anuarul Hoque"
        ],
        "published": "2025-03-17T17:49:51Z",
        "summary": "The synergy between virtual reality (VR) and artificial intelligence (AI), specifically deep learning (DL)-based cybersickness detection models, has ushered in unprecedented advancements in immersive experiences by automatically detecting cybersickness severity and adaptively various mitigation techniques, offering a smooth and comfortable VR experience. While this DL-enabled cybersickness detection method provides promising solutions for enhancing user experiences, it also introduces new risks since these models are vulnerable to adversarial attacks; a small perturbation of the input data that is visually undetectable to human observers can fool the cybersickness detection model and trigger unexpected mitigation, thus disrupting user immersive experiences (UIX) and even posing safety risks. In this paper, we present a new type of VR attack, i.e., a cybersickness attack, which successfully stops the triggering of cybersickness mitigation by fooling DL-based cybersickness detection models and dramatically hinders the UIX. Next, we propose a novel explainable artificial intelligence (XAI)-guided cybersickness attack detection framework to detect such attacks in VR to ensure UIX and a comfortable VR experience. We evaluate the proposed attack and the detection framework using two state-of-the-art open-source VR cybersickness datasets: Simulation 2021 and Gameplay dataset. Finally, to verify the effectiveness of our proposed method, we implement the attack and the XAI-based detection using a testbed with a custom-built VR roller coaster simulation with an HTC Vive Pro Eye headset and perform a user study. Our study shows that such an attack can dramatically hinder the UIX. However, our proposed XAI-guided cybersickness attack detection can successfully detect cybersickness attacks and trigger the proper mitigation, effectively reducing VR cybersickness.",
        "field": "Green Finance Technology",
        "link": "http://arxiv.org/abs/2503.13419v1"
    },
    {
        "id": "2503.13255v1",
        "title": "Zero-Knowledge Proof-Based Consensus for Blockchain-Secured Federated Learning",
        "authors": [
            "Tianxing Fu",
            "Jia Hu",
            "Geyong Min",
            "Zi Wang"
        ],
        "published": "2025-03-17T15:13:10Z",
        "summary": "Federated learning (FL) enables multiple participants to collaboratively train machine learning models while ensuring their data remains private and secure. Blockchain technology further enhances FL by providing stronger security, a transparent audit trail, and protection against data tampering and model manipulation. Most blockchain-secured FL systems rely on conventional consensus mechanisms: Proof-of-Work (PoW) is computationally expensive, while Proof-of-Stake (PoS) improves energy efficiency but risks centralization as it inherently favors participants with larger stakes. Recently, learning-based consensus has emerged as an alternative by replacing cryptographic tasks with model training to save energy. However, this approach introduces potential privacy vulnerabilities, as the training process may inadvertently expose sensitive information through gradient sharing and model updates. To address these challenges, we propose a novel Zero-Knowledge Proof of Training (ZKPoT) consensus mechanism. This method leverages the zero-knowledge succinct non-interactive argument of knowledge proof (zk-SNARK) protocol to validate participants' contributions based on their model performance, effectively eliminating the inefficiencies of traditional consensus methods and mitigating the privacy risks posed by learning-based consensus. We analyze our system's security, demonstrating its capacity to prevent the disclosure of sensitive information about local models or training data to untrusted parties during the entire FL process. Extensive experiments demonstrate that our system is robust against privacy and Byzantine attacks while maintaining accuracy and utility without trade-offs, scalable across various blockchain settings, and efficient in both computation and communication.",
        "field": "Green Finance Technology",
        "link": "http://arxiv.org/abs/2503.13255v1"
    },
    {
        "id": "2503.12958v1",
        "title": "FedSDP: Explainable Differential Privacy in Federated Learning via Shapley Values",
        "authors": [
            "Yunbo Li",
            "Jiaping Gui",
            "Yue Wu"
        ],
        "published": "2025-03-17T09:14:19Z",
        "summary": "Federated learning (FL) enables participants to store data locally while collaborating in training, yet it remains vulnerable to privacy attacks, such as data reconstruction. Existing differential privacy (DP) technologies inject noise dynamically into the training process to mitigate the impact of excessive noise. However, this dynamic scheduling is often grounded in factors indirectly related to privacy, making it difficult to clearly explain the intricate relationship between dynamic noise adjustments and privacy requirements. To address this issue, we propose FedSDP, a novel and explainable DP-based privacy protection mechanism that guides noise injection based on privacy contribution. Specifically, FedSDP leverages Shapley values to assess the contribution of private attributes to local model training and dynamically adjusts the amount of noise injected accordingly. By providing theoretical insights into the injection of varying scales of noise into local training, FedSDP enhances interpretability. Extensive experiments demonstrate that FedSDP can achieve a superior balance between privacy preservation and model performance, surpassing state-of-the-art (SOTA) solutions.",
        "field": "Green Finance Technology",
        "link": "http://arxiv.org/abs/2503.12958v1"
    },
    {
        "id": "2503.14213v1",
        "title": "Rolling Forward: Enhancing LightGCN with Causal Graph Convolution for Credit Bond Recommendation",
        "authors": [
            "Ashraf Ghiye",
            "Baptiste Barreau",
            "Laurent Carlier",
            "Michalis Vazirgiannis"
        ],
        "published": "2025-03-18T12:47:01Z",
        "summary": "Graph Neural Networks have significantly advanced research in recommender systems over the past few years. These methods typically capture global interests using aggregated past interactions and rely on static embeddings of users and items over extended periods of time. While effective in some domains, these methods fall short in many real-world scenarios, especially in finance, where user interests and item popularity evolve rapidly over time. To address these challenges, we introduce a novel extension to Light Graph Convolutional Network (LightGCN) designed to learn temporal node embeddings that capture dynamic interests. Our approach employs causal convolution to maintain a forward-looking model architecture. By preserving the chronological order of user-item interactions and introducing a dynamic update mechanism for embeddings through a sliding window, the proposed model generates well-timed and contextually relevant recommendations. Extensive experiments on a real-world dataset from BNP Paribas demonstrate that our approach significantly enhances the performance of LightGCN while maintaining the simplicity and efficiency of its architecture. Our findings provide new insights into designing graph-based recommender systems in time-sensitive applications, particularly for financial product recommendations.",
        "field": "Autonomous Finance",
        "link": "http://arxiv.org/abs/2503.14213v1"
    },
    {
        "id": "2503.13637v1",
        "title": "XChainDataGen: A Cross-Chain Dataset Generation Framework",
        "authors": [
            "André Augusto",
            "André Vasconcelos",
            "Miguel Correia",
            "Luyao Zhang"
        ],
        "published": "2025-03-17T18:39:43Z",
        "summary": "The number of blockchain interoperability protocols for transferring data and assets between blockchains has grown significantly. However, no open dataset of cross-chain transactions exists to study interoperability protocols in operation. There is also no tool to generate such datasets and make them available to the community. This paper proposes XChainDataGen, a tool to extract cross-chain data from blockchains and generate datasets of cross-chain transactions (cctxs). Using XChainDataGen, we extracted over 35 GB of data from five cross-chain protocols deployed on 11 blockchains in the last seven months of 2024, identifying 11,285,753 cctxs that moved over 28 billion USD in cross-chain token transfers. Using the data collected, we compare protocols and provide insights into their security, cost, and performance trade-offs. As examples, we highlight differences between protocols that require full finality on the source blockchain and those that only demand soft finality (\\textit{security}). We compare user costs, fee models, and the impact of variables such as the Ethereum gas price on protocol fees (\\textit{cost}). Finally, we produce the first analysis of the implications of EIP-7683 for cross-chain intents, which are increasingly popular and greatly improve the speed with which cctxs are processed (\\textit{performance}), thereby enhancing the user experience. The availability of XChainDataGen and this dataset allows various analyses, including trends in cross-chain activity, security assessments of interoperability protocols, and financial research on decentralized finance (DeFi) protocols.",
        "field": "Autonomous Finance",
        "link": "http://arxiv.org/abs/2503.13637v1"
    },
    {
        "id": "2503.13193v1",
        "title": "The deep multi-FBSDE method: a robust deep learning method for coupled FBSDEs",
        "authors": [
            "Kristoffer Andersson",
            "Adam Andersson",
            "Cornelis W. Oosterlee"
        ],
        "published": "2025-03-17T14:01:37Z",
        "summary": "We introduce the deep multi-FBSDE method for robust approximation of coupled forward-backward stochastic differential equations (FBSDEs), focusing on cases where the deep BSDE method of Han, Jentzen, and E (2018) fails to converge. To overcome the convergence issues, we consider a family of FBSDEs that are equivalent to the original problem in the sense that they satisfy the same associated partial differential equation (PDE). Our algorithm proceeds in two phases: first, we approximate the initial condition for the FBSDE family, and second, we approximate the original FBSDE using the initial condition approximated in the first phase. Numerical experiments show that our method converges even when the standard deep BSDE method does not.",
        "field": "Autonomous Finance",
        "link": "http://arxiv.org/abs/2503.13193v1"
    },
    {
        "id": "2503.13056v1",
        "title": "Deep Hedging of Green PPAs in Electricity Markets",
        "authors": [
            "Richard Biegler-König",
            "Daniel Oeltz"
        ],
        "published": "2025-03-17T11:02:23Z",
        "summary": "In power markets, Green Power Purchase Agreements have become an important contractual tool of the energy transition from fossil fuels to renewable sources such as wind or solar radiation. Trading Green PPAs exposes agents to price risks and weather risks. Also, developed electricity markets feature the so-called cannibalisation effect : large infeeds induce low prices and vice versa. As weather is a non-tradable entity the question arises how to hedge and risk-manage in this highly incom-plete setting. We propose a ''deep hedging'' framework utilising machine learning methods to construct hedging strategies. The resulting strategies outperform static and dynamic benchmark strategies with respect to different risk measures.",
        "field": "Autonomous Finance",
        "link": "http://arxiv.org/abs/2503.13056v1"
    },
    {
        "id": "2503.12719v1",
        "title": "Enabling High-Frequency Trading with Near-Instant, Trustless Cross-Chain Transactions via Pre-Signing Adaptor Signatures",
        "authors": [
            "Ethan Francolla",
            "Arnav Shah"
        ],
        "published": "2025-03-17T01:15:33Z",
        "summary": "Atomic swaps have been widely considered to be an ideal solution for cross-chain cryptocurrency transactions due to their trustless and decentralized nature. However, their adoption in practice has been strictly limited compared to centralized exchange order books because of long transaction times (anywhere from 20 to 60 minutes) prohibiting market makers from accurately pricing atomic swap spreads. For the decentralized finance ecosystem to expand and benefit all users, this would require accommodating market makers and high-frequency traders to reduce spreads and dramatically boost liquidity. This white paper will introduce a protocol for atomic swaps that eliminates the need for an intermediary currency or centralized trusted third party, reducing transaction times between Bitcoin and Ethereum swaps to approximately 15 seconds for a market maker, and could be reduced further with future Layer 2 solutions.",
        "field": "Autonomous Finance",
        "link": "http://arxiv.org/abs/2503.12719v1"
    },
    {
        "id": "2503.14279v1",
        "title": "Anti-Tamper Radio meets Reconfigurable Intelligent Surface for System-Level Tamper Detection",
        "authors": [
            "Maryam Shaygan Tabar",
            "Johannes Kortz",
            "Paul Staat",
            "Harald Elders-Boll",
            "Christof Paar",
            "Christian Zenger"
        ],
        "published": "2025-03-18T14:18:31Z",
        "summary": "Many computing systems need to be protected against physical attacks using active tamper detection based on sensors. One technical solution is to employ an ATR (Anti-Tamper Radio) approach, analyzing the radio wave propagation effects within a protected device to detect unauthorized physical alterations. However, ATR systems face key challenges in terms of susceptibility to signal manipulation attacks, limited reliability due to environmental noise, and regulatory constraints from wide bandwidth usage. In this work, we propose and experimentally evaluate an ATR system complemented by an RIS to dynamically reconfigure the wireless propagation environment. We show that this approach can enhance resistance against signal manipulation attacks, reduce bandwidth requirements from several~GHz down to as low as 20 MHz, and improve robustness to environmental disturbances such as internal fan movements. Our work demonstrates that RIS integration can strengthen the ATR performance to enhance security, sensitivity, and robustness, recognizing the potential of smart radio environments for ATR-based tamper detection",
        "field": "Blockchain Interoperability Solutions",
        "link": "http://arxiv.org/abs/2503.14279v1"
    },
    {
        "id": "2503.14006v1",
        "title": "Securing Automated Insulin Delivery Systems: A Review of Security Threats and Protectives Strategies",
        "authors": [
            "Yuchen Niu",
            "Siew-Kei Lam"
        ],
        "published": "2025-03-18T08:11:19Z",
        "summary": "Automated insulin delivery (AID) systems have emerged as a significant technological advancement in diabetes care. These systems integrate a continuous glucose monitor, an insulin pump, and control algorithms to automate insulin delivery, reducing the burden of self-management and offering enhanced glucose control. However, the increasing reliance on wireless connectivity and software control has exposed AID systems to critical security risks that could result in life-threatening treatment errors. This review first presents a comprehensive examination of the security landscape, covering technical vulnerabilities, legal frameworks, and commercial product considerations, and an analysis of existing research on attack vectors, defence mechanisms, as well as evaluation methods and resources for AID systems. Despite recent advancements, several open challenges remain in achieving secure AID systems, particularly in standardising security evaluation frameworks and developing comprehensive, lightweight, and adaptive defence strategies. As one of the most widely adopted and extensively studied physiologic closed-loop control systems, this review serves as a valuable reference for understanding security challenges and solutions applicable to analogous medical systems.",
        "field": "Blockchain Interoperability Solutions",
        "link": "http://arxiv.org/abs/2503.14006v1"
    },
    {
        "id": "2503.13994v1",
        "title": "TarPro: Targeted Protection against Malicious Image Editing",
        "authors": [
            "Kaixin Shen",
            "Ruijie Quan",
            "Jiaxu Miao",
            "Jun Xiao",
            "Yi Yang"
        ],
        "published": "2025-03-18T07:54:44Z",
        "summary": "The rapid advancement of image editing techniques has raised concerns about their misuse for generating Not-Safe-for-Work (NSFW) content. This necessitates a targeted protection mechanism that blocks malicious edits while preserving normal editability. However, existing protection methods fail to achieve this balance, as they indiscriminately disrupt all edits while still allowing some harmful content to be generated. To address this, we propose TarPro, a targeted protection framework that prevents malicious edits while maintaining benign modifications. TarPro achieves this through a semantic-aware constraint that only disrupts malicious content and a lightweight perturbation generator that produces a more stable, imperceptible, and robust perturbation for image protection. Extensive experiments demonstrate that TarPro surpasses existing methods, achieving a high protection efficacy while ensuring minimal impact on normal edits. Our results highlight TarPro as a practical solution for secure and controlled image editing.",
        "field": "Blockchain Interoperability Solutions",
        "link": "http://arxiv.org/abs/2503.13994v1"
    },
    {
        "id": "2503.13419v1",
        "title": "Securing Virtual Reality Experiences: Unveiling and Tackling Cybersickness Attacks with Explainable AI",
        "authors": [
            "Ripan Kumar Kundu",
            "Matthew Denton",
            "Genova Mongalo",
            "Prasad Calyam",
            "Khaza Anuarul Hoque"
        ],
        "published": "2025-03-17T17:49:51Z",
        "summary": "The synergy between virtual reality (VR) and artificial intelligence (AI), specifically deep learning (DL)-based cybersickness detection models, has ushered in unprecedented advancements in immersive experiences by automatically detecting cybersickness severity and adaptively various mitigation techniques, offering a smooth and comfortable VR experience. While this DL-enabled cybersickness detection method provides promising solutions for enhancing user experiences, it also introduces new risks since these models are vulnerable to adversarial attacks; a small perturbation of the input data that is visually undetectable to human observers can fool the cybersickness detection model and trigger unexpected mitigation, thus disrupting user immersive experiences (UIX) and even posing safety risks. In this paper, we present a new type of VR attack, i.e., a cybersickness attack, which successfully stops the triggering of cybersickness mitigation by fooling DL-based cybersickness detection models and dramatically hinders the UIX. Next, we propose a novel explainable artificial intelligence (XAI)-guided cybersickness attack detection framework to detect such attacks in VR to ensure UIX and a comfortable VR experience. We evaluate the proposed attack and the detection framework using two state-of-the-art open-source VR cybersickness datasets: Simulation 2021 and Gameplay dataset. Finally, to verify the effectiveness of our proposed method, we implement the attack and the XAI-based detection using a testbed with a custom-built VR roller coaster simulation with an HTC Vive Pro Eye headset and perform a user study. Our study shows that such an attack can dramatically hinder the UIX. However, our proposed XAI-guided cybersickness attack detection can successfully detect cybersickness attacks and trigger the proper mitigation, effectively reducing VR cybersickness.",
        "field": "Blockchain Interoperability Solutions",
        "link": "http://arxiv.org/abs/2503.13419v1"
    },
    {
        "id": "2503.12958v1",
        "title": "FedSDP: Explainable Differential Privacy in Federated Learning via Shapley Values",
        "authors": [
            "Yunbo Li",
            "Jiaping Gui",
            "Yue Wu"
        ],
        "published": "2025-03-17T09:14:19Z",
        "summary": "Federated learning (FL) enables participants to store data locally while collaborating in training, yet it remains vulnerable to privacy attacks, such as data reconstruction. Existing differential privacy (DP) technologies inject noise dynamically into the training process to mitigate the impact of excessive noise. However, this dynamic scheduling is often grounded in factors indirectly related to privacy, making it difficult to clearly explain the intricate relationship between dynamic noise adjustments and privacy requirements. To address this issue, we propose FedSDP, a novel and explainable DP-based privacy protection mechanism that guides noise injection based on privacy contribution. Specifically, FedSDP leverages Shapley values to assess the contribution of private attributes to local model training and dynamically adjusts the amount of noise injected accordingly. By providing theoretical insights into the injection of varying scales of noise into local training, FedSDP enhances interpretability. Extensive experiments demonstrate that FedSDP can achieve a superior balance between privacy preservation and model performance, surpassing state-of-the-art (SOTA) solutions.",
        "field": "Blockchain Interoperability Solutions",
        "link": "http://arxiv.org/abs/2503.12958v1"
    },
    {
        "id": "2503.14462v1",
        "title": "Blockchain with proof of quantum work",
        "authors": [
            "Mohammad H. Amin",
            "Jack Raymond",
            "Daniel Kinn",
            "Firas Hamze",
            "Kelsey Hamer",
            "Joel Pasvolsky",
            "William Bernoudy",
            "Andrew D. King",
            "Samuel Kortas"
        ],
        "published": "2025-03-18T17:37:22Z",
        "summary": "We propose a blockchain architecture in which mining requires a quantum computer. The consensus mechanism is based on proof of quantum work, a quantum-enhanced alternative to traditional proof of work that leverages quantum supremacy to make mining intractable for classical computers. We have refined the blockchain framework to incorporate the probabilistic nature of quantum mechanics, ensuring stability against sampling errors and hardware inaccuracies. To validate our approach, we implemented a prototype blockchain on four D-Wave$^{\\rm TM}$ quantum annealing processors geographically distributed within North America, demonstrating stable operation across hundreds of thousands of quantum hashing operations. Our experimental protocol follows the same approach used in the recent demonstration of quantum supremacy [1], ensuring that classical computers cannot efficiently perform the same computation task. By replacing classical machines with quantum systems for mining, it is possible to significantly reduce the energy consumption and environmental impact traditionally associated with blockchain mining. Beyond serving as a proof of concept for a meaningful application of quantum computing, this work highlights the potential for other near-term quantum computing applications using existing technology.",
        "field": "Quantum-Safe Cryptography",
        "link": "http://arxiv.org/abs/2503.14462v1"
    },
    {
        "id": "2503.14388v1",
        "title": "Vexed by VEX tools: Consistency evaluation of container vulnerability scanners",
        "authors": [
            "Yekatierina Churakova Mathias Ekstedt"
        ],
        "published": "2025-03-18T16:22:43Z",
        "summary": "This paper presents a study that analyzed state-of-the-art vulnerability scanning tools applied to containers. We have focused the work on tools following the Vulnerability Exploitability eXchange (VEX) format, which has been introduced to complement Software Bills of Material (SBOM) with security advisories of known vulnerabilities. Being able to get an accurate understanding of vulnerabilities found in the dependencies of third-party software is critical for secure software development and risk analysis. Accepting the overwhelming challenge of estimating the precise accuracy and precision of a vulnerability scanner, we have in this study instead set out to explore how consistently different tools perform. By doing this, we aim to assess the maturity of the VEX tool field as a whole (rather than any particular tool). We have used the Jaccard and Tversky indices to produce similarity scores of tool performance for several different datasets created from container images. Overall, our results show a low level of consistency among the tools, thus indicating a low level of maturity in VEX tool space. We have performed a number of experiments to find and explanation to our results, but largely they are inconclusive and further research is needed to understand the underlying causalities of our findings.",
        "field": "Quantum-Safe Cryptography",
        "link": "http://arxiv.org/abs/2503.14388v1"
    },
    {
        "id": "2503.14284v1",
        "title": "Entente: Cross-silo Intrusion Detection on Network Log Graphs with Federated Learning",
        "authors": [
            "Jiacen Xu",
            "Chenang Li",
            "Yu Zheng",
            "Zhou Li"
        ],
        "published": "2025-03-18T14:21:24Z",
        "summary": "Graph-based Network Intrusion Detection System (GNIDS) has gained significant momentum in detecting sophisticated cyber-attacks, like Advanced Persistent Threat (APT), in an organization or across organizations. Though achieving satisfying detection accuracy and adapting to ever-changing attacks and normal patterns, all prior GNIDSs assume the centralized data settings directly, but non-trivial data collection is not always practical under privacy regulations nowadays. We argue that training a GNIDS model has to consider privacy regulations, and propose to leverage federated learning (FL) to address this prominent challenge. Yet, directly applying FL to GNIDS is unlikely to succeed, due to issues like non-IID (independent and identically distributed) graph data over clients and the diverse design choices taken by different GNIDS. We address these issues with a set of novel techniques tailored to the graph datasets, including reference graph synthesis, graph sketching and adaptive contribution scaling, and develop a new system Entente. We evaluate Entente on the large-scale LANL, OpTC and Pivoting datasets. The result shows Entente outperforms the other baseline FL algorithms and sometimes even the non-FL GNIDS. We also evaluate Entente under FL poisoning attacks tailored to the GNIDS setting, and show Entente is able to bound the attack success rate to low values. Overall, our result suggests building cross-silo GNIDS is feasible and we hope to encourage more efforts in this direction.",
        "field": "Quantum-Safe Cryptography",
        "link": "http://arxiv.org/abs/2503.14284v1"
    },
    {
        "id": "2503.14281v1",
        "title": "XOXO: Stealthy Cross-Origin Context Poisoning Attacks against AI Coding Assistants",
        "authors": [
            "Adam Štorek",
            "Mukur Gupta",
            "Noopur Bhatt",
            "Aditya Gupta",
            "Janie Kim",
            "Prashast Srivastava",
            "Suman Jana"
        ],
        "published": "2025-03-18T14:20:54Z",
        "summary": "AI coding assistants are widely used for tasks like code generation, bug detection, and comprehension. These tools now require large and complex contexts, automatically sourced from various origins$\\unicode{x2014}$across files, projects, and contributors$\\unicode{x2014}$forming part of the prompt fed to underlying LLMs. This automatic context-gathering introduces new vulnerabilities, allowing attackers to subtly poison input to compromise the assistant's outputs, potentially generating vulnerable code, overlooking flaws, or introducing critical errors. We propose a novel attack, Cross-Origin Context Poisoning (XOXO), that is particularly challenging to detect as it relies on adversarial code modifications that are semantically equivalent. Traditional program analysis techniques struggle to identify these correlations since the semantics of the code remain correct, making it appear legitimate. This allows attackers to manipulate code assistants into producing incorrect outputs, including vulnerabilities or backdoors, while shifting the blame to the victim developer or tester. We introduce a novel, task-agnostic black-box attack algorithm GCGS that systematically searches the transformation space using a Cayley Graph, achieving an 83.09% attack success rate on average across five tasks and eleven models, including GPT-4o and Claude 3.5 Sonnet v2 used by many popular AI coding assistants. Furthermore, existing defenses, including adversarial fine-tuning, are ineffective against our attack, underscoring the need for new security measures in LLM-powered coding tools.",
        "field": "Quantum-Safe Cryptography",
        "link": "http://arxiv.org/abs/2503.14281v1"
    },
    {
        "id": "2503.14279v1",
        "title": "Anti-Tamper Radio meets Reconfigurable Intelligent Surface for System-Level Tamper Detection",
        "authors": [
            "Maryam Shaygan Tabar",
            "Johannes Kortz",
            "Paul Staat",
            "Harald Elders-Boll",
            "Christof Paar",
            "Christian Zenger"
        ],
        "published": "2025-03-18T14:18:31Z",
        "summary": "Many computing systems need to be protected against physical attacks using active tamper detection based on sensors. One technical solution is to employ an ATR (Anti-Tamper Radio) approach, analyzing the radio wave propagation effects within a protected device to detect unauthorized physical alterations. However, ATR systems face key challenges in terms of susceptibility to signal manipulation attacks, limited reliability due to environmental noise, and regulatory constraints from wide bandwidth usage. In this work, we propose and experimentally evaluate an ATR system complemented by an RIS to dynamically reconfigure the wireless propagation environment. We show that this approach can enhance resistance against signal manipulation attacks, reduce bandwidth requirements from several~GHz down to as low as 20 MHz, and improve robustness to environmental disturbances such as internal fan movements. Our work demonstrates that RIS integration can strengthen the ATR performance to enhance security, sensitivity, and robustness, recognizing the potential of smart radio environments for ATR-based tamper detection",
        "field": "Quantum-Safe Cryptography",
        "link": "http://arxiv.org/abs/2503.14279v1"
    },
    {
        "id": "2503.09823v1",
        "title": "Data Traceability for Privacy Alignment",
        "authors": [
            "Kevin Liao",
            "Shreya Thipireddy",
            "Daniel Weitzner"
        ],
        "published": "2025-03-12T20:42:23Z",
        "summary": "This paper offers a new privacy approach for the growing ecosystem of services--ranging from open banking to healthcare--dependent on sensitive personal data sharing between individuals and third-parties. While these services offer significant benefits, individuals want control over their data, transparency regarding how their data is used, and accountability from third-parties for misuse. However, existing legal and technical mechanisms are inadequate for supporting these needs. A comprehensive approach to the modern privacy challenges of accountable third-party data sharing requires a closer alignment of technical system architecture and legal institutional design. In order to achieve this privacy alignment, we extend traditional security threat modeling and analysis to encompass a broader range of privacy notions than has been typically considered. In particular, we introduce the concept of covert-accountability, which addresses adversaries that may act dishonestly but face potential identification and legal consequences. As a concrete instance of this design approach, we present the OTrace protocol, designed to provide traceable, accountable, consumer-control in third-party data sharing ecosystems. OTrace empowers consumers with the knowledge of where their data is, who has it, what it is being used for, and whom it is being shared with. By applying our alignment framework to OTrace, we demonstrate that OTrace's technical affordances can provide more confident, scalable regulatory oversight when combined with complementary legal mechanisms.",
        "field": "Synthetic Data in Banking",
        "link": "http://arxiv.org/abs/2503.09823v1"
    },
    {
        "id": "2503.09586v1",
        "title": "Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot",
        "authors": [
            "Andrew Crossman",
            "Andrew R. Plummer",
            "Chandra Sekharudu",
            "Deepak Warrier",
            "Mohammad Yekrangian"
        ],
        "published": "2025-03-12T17:54:18Z",
        "summary": "We present Auspex - a threat modeling system built using a specialized collection of generative artificial intelligence-based methods that capture threat modeling tradecraft. This new approach, called tradecraft prompting, centers on encoding the on-the-ground knowledge of threat modelers within the prompts that drive a generative AI-based threat modeling system. Auspex employs tradecraft prompts in two processing stages. The first stage centers on ingesting and processing system architecture information using prompts that encode threat modeling tradecraft knowledge pertaining to system decomposition and description. The second stage centers on chaining the resulting system analysis through a collection of prompts that encode tradecraft knowledge on threat identification, classification, and mitigation. The two-stage process yields a threat matrix for a system that specifies threat scenarios, threat types, information security categorizations and potential mitigations. Auspex produces formalized threat model output in minutes, relative to the weeks or months a manual process takes. More broadly, the focus on bespoke tradecraft prompting, as opposed to fine-tuning or agent-based add-ons, makes Auspex a lightweight, flexible, modular, and extensible foundational system capable of addressing the complexity, resource, and standardization limitations of both existing manual and automated threat modeling processes. In this connection, we establish the baseline value of Auspex to threat modelers through an evaluation procedure based on feedback collected from cybersecurity subject matter experts measuring the quality and utility of threat models generated by Auspex on real banking systems. We conclude with a discussion of system performance and plans for enhancements to Auspex.",
        "field": "Synthetic Data in Banking",
        "link": "http://arxiv.org/abs/2503.09586v1"
    },
    {
        "id": "2503.00070v1",
        "title": "Systematic Review of Cybersecurity in Banking: Evolution from Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain, Policies and Practice",
        "authors": [
            "Tue Nhi Tran"
        ],
        "published": "2025-02-27T14:17:06Z",
        "summary": "Throughout the history from pre-industry 4.0 to post-industry 4.0, cybersecurity at banks has undergone significant changes. Pre-industry 4.0 cyber security at banks relied on individual security methods that were highly manual and had low accuracy. When moving to post-industry 4.0, cybersecurity at banks had a major turning point with security methods that combined different technologies such as Artificial Intelligence (AI), Blockchain, IoT, automating necessary processes and significantly increasing the defence layer for banks. However, along with the development of new technologies, the current challenge of cybersecurity at banks lies in scalability, high costs and resources in both money and time for R&D of defence methods along with the threat of high-tech cybercriminals growing and expanding. This report goes from introducing the importance of cybersecurity at banks, analyzing their management, operational and business objectives, evaluating pre-industry 4.0 technologies used for cybersecurity at banks to assessing post-industry 4.0 technologies focusing on Artificial Intelligence and Blockchain, discussing current policies and practices and ending with discussing key advantages and challenges for 4.0 technologies and recommendations for further developing cybersecurity at banks.",
        "field": "Synthetic Data in Banking",
        "link": "http://arxiv.org/abs/2503.00070v1"
    },
    {
        "id": "2503.10644v1",
        "title": "Combined climate stress testing of supply-chain networks and the financial system with nation-wide firm-level emission estimates",
        "authors": [
            "Zlata Tabachová",
            "Christian Diem",
            "Johannes Stangl",
            "András Borsos",
            "Stefan Thurner"
        ],
        "published": "2025-02-25T20:25:47Z",
        "summary": "On the way towards carbon neutrality, climate stress testing provides estimates for the physical and transition risks that climate change poses to the economy and the financial system. Missing firm-level CO2 emissions data severely impedes the assessment of transition risks originating from carbon pricing. Based on the individual emissions of all Hungarian firms (410,523), as estimated from their fossil fuel purchases, we conduct a stress test of both actual and hypothetical carbon pricing policies. Using a simple 1:1 economic ABM and introducing the new carbon-to-profit ratio, we identify firms that become unprofitable and default, and estimate the respective loan write-offs. We find that 45% of all companies are directly exposed to carbon pricing. At a price of 45 EUR/t, direct economic losses of 1.3% of total sales and bank equity losses of 1.2% are expected. Secondary default cascades in supply chain networks could increase these losses by 300% to 4000%, depending on firms' ability to substitute essential inputs. To reduce transition risks, firms should reduce their dependence on essential inputs from supply chains with high CO2 exposure. We discuss the implications of different policy implementations on these transition risks.",
        "field": "Synthetic Data in Banking",
        "link": "http://arxiv.org/abs/2503.10644v1"
    },
    {
        "id": "2502.14766v2",
        "title": "Multi-Layer Deep xVA: Structural Credit Models, Measure Changes and Convergence Analysis",
        "authors": [
            "Kristoffer Andersson",
            "Alessandro Gnoatto"
        ],
        "published": "2025-02-20T17:41:55Z",
        "summary": "We propose a structural default model for portfolio-wide valuation adjustments (xVAs) and represent it as a system of coupled backward stochastic differential equations. The framework is divided into four layers, each capturing a key component: (i) clean values, (ii) initial margin and Collateral Valuation Adjustment (ColVA), (iii) Credit/Debit Valuation Adjustments (CVA/DVA) together with Margin Valuation Adjustment (MVA), and (iv) Funding Valuation Adjustment (FVA). Because these layers depend on one another through collateral and default effects, a naive Monte Carlo approach would require deeply nested simulations, making the problem computationally intractable. To address this challenge, we use an iterative deep BSDE approach, handling each layer sequentially so that earlier outputs serve as inputs to the subsequent layers. Initial margin is computed via deep quantile regression to reflect margin requirements over the Margin Period of Risk. We also adopt a change-of-measure method that highlights rare but significant defaults of the bank or counterparty, ensuring that these events are accurately captured in the training process. We further extend Han and Long's (2020) a posteriori error analysis to BSDEs on bounded domains. Due to the random exit from the domain, we obtain an order of convergence of $\\mathcal{O}(h^{1/4-\\epsilon})$ rather than the usual $\\mathcal{O}(h^{1/2})$. Numerical experiments illustrate that this method drastically reduces computational demands and successfully scales to high-dimensional, non-symmetric portfolios. The results confirm its effectiveness and accuracy, offering a practical alternative to nested Monte Carlo simulations in multi-counterparty xVA analyses.",
        "field": "Synthetic Data in Banking",
        "link": "http://arxiv.org/abs/2502.14766v2"
    },
    {
        "id": "2503.09823v1",
        "title": "Data Traceability for Privacy Alignment",
        "authors": [
            "Kevin Liao",
            "Shreya Thipireddy",
            "Daniel Weitzner"
        ],
        "published": "2025-03-12T20:42:23Z",
        "summary": "This paper offers a new privacy approach for the growing ecosystem of services--ranging from open banking to healthcare--dependent on sensitive personal data sharing between individuals and third-parties. While these services offer significant benefits, individuals want control over their data, transparency regarding how their data is used, and accountability from third-parties for misuse. However, existing legal and technical mechanisms are inadequate for supporting these needs. A comprehensive approach to the modern privacy challenges of accountable third-party data sharing requires a closer alignment of technical system architecture and legal institutional design. In order to achieve this privacy alignment, we extend traditional security threat modeling and analysis to encompass a broader range of privacy notions than has been typically considered. In particular, we introduce the concept of covert-accountability, which addresses adversaries that may act dishonestly but face potential identification and legal consequences. As a concrete instance of this design approach, we present the OTrace protocol, designed to provide traceable, accountable, consumer-control in third-party data sharing ecosystems. OTrace empowers consumers with the knowledge of where their data is, who has it, what it is being used for, and whom it is being shared with. By applying our alignment framework to OTrace, we demonstrate that OTrace's technical affordances can provide more confident, scalable regulatory oversight when combined with complementary legal mechanisms.",
        "field": "Federated Learning in Banking",
        "link": "http://arxiv.org/abs/2503.09823v1"
    },
    {
        "id": "2503.09586v1",
        "title": "Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot",
        "authors": [
            "Andrew Crossman",
            "Andrew R. Plummer",
            "Chandra Sekharudu",
            "Deepak Warrier",
            "Mohammad Yekrangian"
        ],
        "published": "2025-03-12T17:54:18Z",
        "summary": "We present Auspex - a threat modeling system built using a specialized collection of generative artificial intelligence-based methods that capture threat modeling tradecraft. This new approach, called tradecraft prompting, centers on encoding the on-the-ground knowledge of threat modelers within the prompts that drive a generative AI-based threat modeling system. Auspex employs tradecraft prompts in two processing stages. The first stage centers on ingesting and processing system architecture information using prompts that encode threat modeling tradecraft knowledge pertaining to system decomposition and description. The second stage centers on chaining the resulting system analysis through a collection of prompts that encode tradecraft knowledge on threat identification, classification, and mitigation. The two-stage process yields a threat matrix for a system that specifies threat scenarios, threat types, information security categorizations and potential mitigations. Auspex produces formalized threat model output in minutes, relative to the weeks or months a manual process takes. More broadly, the focus on bespoke tradecraft prompting, as opposed to fine-tuning or agent-based add-ons, makes Auspex a lightweight, flexible, modular, and extensible foundational system capable of addressing the complexity, resource, and standardization limitations of both existing manual and automated threat modeling processes. In this connection, we establish the baseline value of Auspex to threat modelers through an evaluation procedure based on feedback collected from cybersecurity subject matter experts measuring the quality and utility of threat models generated by Auspex on real banking systems. We conclude with a discussion of system performance and plans for enhancements to Auspex.",
        "field": "Federated Learning in Banking",
        "link": "http://arxiv.org/abs/2503.09586v1"
    },
    {
        "id": "2503.00070v1",
        "title": "Systematic Review of Cybersecurity in Banking: Evolution from Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain, Policies and Practice",
        "authors": [
            "Tue Nhi Tran"
        ],
        "published": "2025-02-27T14:17:06Z",
        "summary": "Throughout the history from pre-industry 4.0 to post-industry 4.0, cybersecurity at banks has undergone significant changes. Pre-industry 4.0 cyber security at banks relied on individual security methods that were highly manual and had low accuracy. When moving to post-industry 4.0, cybersecurity at banks had a major turning point with security methods that combined different technologies such as Artificial Intelligence (AI), Blockchain, IoT, automating necessary processes and significantly increasing the defence layer for banks. However, along with the development of new technologies, the current challenge of cybersecurity at banks lies in scalability, high costs and resources in both money and time for R&D of defence methods along with the threat of high-tech cybercriminals growing and expanding. This report goes from introducing the importance of cybersecurity at banks, analyzing their management, operational and business objectives, evaluating pre-industry 4.0 technologies used for cybersecurity at banks to assessing post-industry 4.0 technologies focusing on Artificial Intelligence and Blockchain, discussing current policies and practices and ending with discussing key advantages and challenges for 4.0 technologies and recommendations for further developing cybersecurity at banks.",
        "field": "Federated Learning in Banking",
        "link": "http://arxiv.org/abs/2503.00070v1"
    },
    {
        "id": "2503.10644v1",
        "title": "Combined climate stress testing of supply-chain networks and the financial system with nation-wide firm-level emission estimates",
        "authors": [
            "Zlata Tabachová",
            "Christian Diem",
            "Johannes Stangl",
            "András Borsos",
            "Stefan Thurner"
        ],
        "published": "2025-02-25T20:25:47Z",
        "summary": "On the way towards carbon neutrality, climate stress testing provides estimates for the physical and transition risks that climate change poses to the economy and the financial system. Missing firm-level CO2 emissions data severely impedes the assessment of transition risks originating from carbon pricing. Based on the individual emissions of all Hungarian firms (410,523), as estimated from their fossil fuel purchases, we conduct a stress test of both actual and hypothetical carbon pricing policies. Using a simple 1:1 economic ABM and introducing the new carbon-to-profit ratio, we identify firms that become unprofitable and default, and estimate the respective loan write-offs. We find that 45% of all companies are directly exposed to carbon pricing. At a price of 45 EUR/t, direct economic losses of 1.3% of total sales and bank equity losses of 1.2% are expected. Secondary default cascades in supply chain networks could increase these losses by 300% to 4000%, depending on firms' ability to substitute essential inputs. To reduce transition risks, firms should reduce their dependence on essential inputs from supply chains with high CO2 exposure. We discuss the implications of different policy implementations on these transition risks.",
        "field": "Federated Learning in Banking",
        "link": "http://arxiv.org/abs/2503.10644v1"
    },
    {
        "id": "2502.14766v2",
        "title": "Multi-Layer Deep xVA: Structural Credit Models, Measure Changes and Convergence Analysis",
        "authors": [
            "Kristoffer Andersson",
            "Alessandro Gnoatto"
        ],
        "published": "2025-02-20T17:41:55Z",
        "summary": "We propose a structural default model for portfolio-wide valuation adjustments (xVAs) and represent it as a system of coupled backward stochastic differential equations. The framework is divided into four layers, each capturing a key component: (i) clean values, (ii) initial margin and Collateral Valuation Adjustment (ColVA), (iii) Credit/Debit Valuation Adjustments (CVA/DVA) together with Margin Valuation Adjustment (MVA), and (iv) Funding Valuation Adjustment (FVA). Because these layers depend on one another through collateral and default effects, a naive Monte Carlo approach would require deeply nested simulations, making the problem computationally intractable. To address this challenge, we use an iterative deep BSDE approach, handling each layer sequentially so that earlier outputs serve as inputs to the subsequent layers. Initial margin is computed via deep quantile regression to reflect margin requirements over the Margin Period of Risk. We also adopt a change-of-measure method that highlights rare but significant defaults of the bank or counterparty, ensuring that these events are accurately captured in the training process. We further extend Han and Long's (2020) a posteriori error analysis to BSDEs on bounded domains. Due to the random exit from the domain, we obtain an order of convergence of $\\mathcal{O}(h^{1/4-\\epsilon})$ rather than the usual $\\mathcal{O}(h^{1/2})$. Numerical experiments illustrate that this method drastically reduces computational demands and successfully scales to high-dimensional, non-symmetric portfolios. The results confirm its effectiveness and accuracy, offering a practical alternative to nested Monte Carlo simulations in multi-counterparty xVA analyses.",
        "field": "Federated Learning in Banking",
        "link": "http://arxiv.org/abs/2502.14766v2"
    },
    {
        "id": "2503.09823v1",
        "title": "Data Traceability for Privacy Alignment",
        "authors": [
            "Kevin Liao",
            "Shreya Thipireddy",
            "Daniel Weitzner"
        ],
        "published": "2025-03-12T20:42:23Z",
        "summary": "This paper offers a new privacy approach for the growing ecosystem of services--ranging from open banking to healthcare--dependent on sensitive personal data sharing between individuals and third-parties. While these services offer significant benefits, individuals want control over their data, transparency regarding how their data is used, and accountability from third-parties for misuse. However, existing legal and technical mechanisms are inadequate for supporting these needs. A comprehensive approach to the modern privacy challenges of accountable third-party data sharing requires a closer alignment of technical system architecture and legal institutional design. In order to achieve this privacy alignment, we extend traditional security threat modeling and analysis to encompass a broader range of privacy notions than has been typically considered. In particular, we introduce the concept of covert-accountability, which addresses adversaries that may act dishonestly but face potential identification and legal consequences. As a concrete instance of this design approach, we present the OTrace protocol, designed to provide traceable, accountable, consumer-control in third-party data sharing ecosystems. OTrace empowers consumers with the knowledge of where their data is, who has it, what it is being used for, and whom it is being shared with. By applying our alignment framework to OTrace, we demonstrate that OTrace's technical affordances can provide more confident, scalable regulatory oversight when combined with complementary legal mechanisms.",
        "field": "Ambient Banking",
        "link": "http://arxiv.org/abs/2503.09823v1"
    },
    {
        "id": "2503.09586v1",
        "title": "Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot",
        "authors": [
            "Andrew Crossman",
            "Andrew R. Plummer",
            "Chandra Sekharudu",
            "Deepak Warrier",
            "Mohammad Yekrangian"
        ],
        "published": "2025-03-12T17:54:18Z",
        "summary": "We present Auspex - a threat modeling system built using a specialized collection of generative artificial intelligence-based methods that capture threat modeling tradecraft. This new approach, called tradecraft prompting, centers on encoding the on-the-ground knowledge of threat modelers within the prompts that drive a generative AI-based threat modeling system. Auspex employs tradecraft prompts in two processing stages. The first stage centers on ingesting and processing system architecture information using prompts that encode threat modeling tradecraft knowledge pertaining to system decomposition and description. The second stage centers on chaining the resulting system analysis through a collection of prompts that encode tradecraft knowledge on threat identification, classification, and mitigation. The two-stage process yields a threat matrix for a system that specifies threat scenarios, threat types, information security categorizations and potential mitigations. Auspex produces formalized threat model output in minutes, relative to the weeks or months a manual process takes. More broadly, the focus on bespoke tradecraft prompting, as opposed to fine-tuning or agent-based add-ons, makes Auspex a lightweight, flexible, modular, and extensible foundational system capable of addressing the complexity, resource, and standardization limitations of both existing manual and automated threat modeling processes. In this connection, we establish the baseline value of Auspex to threat modelers through an evaluation procedure based on feedback collected from cybersecurity subject matter experts measuring the quality and utility of threat models generated by Auspex on real banking systems. We conclude with a discussion of system performance and plans for enhancements to Auspex.",
        "field": "Ambient Banking",
        "link": "http://arxiv.org/abs/2503.09586v1"
    },
    {
        "id": "2503.00070v1",
        "title": "Systematic Review of Cybersecurity in Banking: Evolution from Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain, Policies and Practice",
        "authors": [
            "Tue Nhi Tran"
        ],
        "published": "2025-02-27T14:17:06Z",
        "summary": "Throughout the history from pre-industry 4.0 to post-industry 4.0, cybersecurity at banks has undergone significant changes. Pre-industry 4.0 cyber security at banks relied on individual security methods that were highly manual and had low accuracy. When moving to post-industry 4.0, cybersecurity at banks had a major turning point with security methods that combined different technologies such as Artificial Intelligence (AI), Blockchain, IoT, automating necessary processes and significantly increasing the defence layer for banks. However, along with the development of new technologies, the current challenge of cybersecurity at banks lies in scalability, high costs and resources in both money and time for R&D of defence methods along with the threat of high-tech cybercriminals growing and expanding. This report goes from introducing the importance of cybersecurity at banks, analyzing their management, operational and business objectives, evaluating pre-industry 4.0 technologies used for cybersecurity at banks to assessing post-industry 4.0 technologies focusing on Artificial Intelligence and Blockchain, discussing current policies and practices and ending with discussing key advantages and challenges for 4.0 technologies and recommendations for further developing cybersecurity at banks.",
        "field": "Ambient Banking",
        "link": "http://arxiv.org/abs/2503.00070v1"
    },
    {
        "id": "2503.10644v1",
        "title": "Combined climate stress testing of supply-chain networks and the financial system with nation-wide firm-level emission estimates",
        "authors": [
            "Zlata Tabachová",
            "Christian Diem",
            "Johannes Stangl",
            "András Borsos",
            "Stefan Thurner"
        ],
        "published": "2025-02-25T20:25:47Z",
        "summary": "On the way towards carbon neutrality, climate stress testing provides estimates for the physical and transition risks that climate change poses to the economy and the financial system. Missing firm-level CO2 emissions data severely impedes the assessment of transition risks originating from carbon pricing. Based on the individual emissions of all Hungarian firms (410,523), as estimated from their fossil fuel purchases, we conduct a stress test of both actual and hypothetical carbon pricing policies. Using a simple 1:1 economic ABM and introducing the new carbon-to-profit ratio, we identify firms that become unprofitable and default, and estimate the respective loan write-offs. We find that 45% of all companies are directly exposed to carbon pricing. At a price of 45 EUR/t, direct economic losses of 1.3% of total sales and bank equity losses of 1.2% are expected. Secondary default cascades in supply chain networks could increase these losses by 300% to 4000%, depending on firms' ability to substitute essential inputs. To reduce transition risks, firms should reduce their dependence on essential inputs from supply chains with high CO2 exposure. We discuss the implications of different policy implementations on these transition risks.",
        "field": "Ambient Banking",
        "link": "http://arxiv.org/abs/2503.10644v1"
    },
    {
        "id": "2502.14766v2",
        "title": "Multi-Layer Deep xVA: Structural Credit Models, Measure Changes and Convergence Analysis",
        "authors": [
            "Kristoffer Andersson",
            "Alessandro Gnoatto"
        ],
        "published": "2025-02-20T17:41:55Z",
        "summary": "We propose a structural default model for portfolio-wide valuation adjustments (xVAs) and represent it as a system of coupled backward stochastic differential equations. The framework is divided into four layers, each capturing a key component: (i) clean values, (ii) initial margin and Collateral Valuation Adjustment (ColVA), (iii) Credit/Debit Valuation Adjustments (CVA/DVA) together with Margin Valuation Adjustment (MVA), and (iv) Funding Valuation Adjustment (FVA). Because these layers depend on one another through collateral and default effects, a naive Monte Carlo approach would require deeply nested simulations, making the problem computationally intractable. To address this challenge, we use an iterative deep BSDE approach, handling each layer sequentially so that earlier outputs serve as inputs to the subsequent layers. Initial margin is computed via deep quantile regression to reflect margin requirements over the Margin Period of Risk. We also adopt a change-of-measure method that highlights rare but significant defaults of the bank or counterparty, ensuring that these events are accurately captured in the training process. We further extend Han and Long's (2020) a posteriori error analysis to BSDEs on bounded domains. Due to the random exit from the domain, we obtain an order of convergence of $\\mathcal{O}(h^{1/4-\\epsilon})$ rather than the usual $\\mathcal{O}(h^{1/2})$. Numerical experiments illustrate that this method drastically reduces computational demands and successfully scales to high-dimensional, non-symmetric portfolios. The results confirm its effectiveness and accuracy, offering a practical alternative to nested Monte Carlo simulations in multi-counterparty xVA analyses.",
        "field": "Ambient Banking",
        "link": "http://arxiv.org/abs/2502.14766v2"
    },
    {
        "id": "2503.13056v1",
        "title": "Deep Hedging of Green PPAs in Electricity Markets",
        "authors": [
            "Richard Biegler-König",
            "Daniel Oeltz"
        ],
        "published": "2025-03-17T11:02:23Z",
        "summary": "In power markets, Green Power Purchase Agreements have become an important contractual tool of the energy transition from fossil fuels to renewable sources such as wind or solar radiation. Trading Green PPAs exposes agents to price risks and weather risks. Also, developed electricity markets feature the so-called cannibalisation effect : large infeeds induce low prices and vice versa. As weather is a non-tradable entity the question arises how to hedge and risk-manage in this highly incom-plete setting. We propose a ''deep hedging'' framework utilising machine learning methods to construct hedging strategies. The resulting strategies outperform static and dynamic benchmark strategies with respect to different risk measures.",
        "field": "Autonomous Banking Agents",
        "link": "http://arxiv.org/abs/2503.13056v1"
    },
    {
        "id": "2503.12217v1",
        "title": "TFHE-Coder: Evaluating LLM-agentic Fully Homomorphic Encryption Code Generation",
        "authors": [
            "Mayank Kumar",
            "Jiaqi Xue",
            "Mengxin Zheng",
            "Qian Lou"
        ],
        "published": "2025-03-15T17:57:44Z",
        "summary": "Fully Homomorphic Encryption over the torus (TFHE) enables computation on encrypted data without decryption, making it a cornerstone of secure and confidential computing. Despite its potential in privacy preserving machine learning, secure multi party computation, private blockchain transactions, and secure medical diagnostics, its adoption remains limited due to cryptographic complexity and usability challenges. While various TFHE libraries and compilers exist, practical code generation remains a hurdle. We propose a compiler integrated framework to evaluate LLM inference and agentic optimization for TFHE code generation, focusing on logic gates and ReLU activation. Our methodology assesses error rates, compilability, and structural similarity across open and closedsource LLMs. Results highlight significant limitations in off-the-shelf models, while agentic optimizations such as retrieval augmented generation (RAG) and few-shot prompting reduce errors and enhance code fidelity. This work establishes the first benchmark for TFHE code generation, demonstrating how LLMs, when augmented with domain-specific feedback, can bridge the expertise gap in FHE code generation.",
        "field": "Autonomous Banking Agents",
        "link": "http://arxiv.org/abs/2503.12217v1"
    },
    {
        "id": "2503.12188v1",
        "title": "Multi-Agent Systems Execute Arbitrary Malicious Code",
        "authors": [
            "Harold Triedman",
            "Rishi Jha",
            "Vitaly Shmatikov"
        ],
        "published": "2025-03-15T16:16:08Z",
        "summary": "Multi-agent systems coordinate LLM-based agents to perform tasks on users' behalf. In real-world applications, multi-agent systems will inevitably interact with untrusted inputs, such as malicious Web content, files, email attachments, etc. Using several recently proposed multi-agent frameworks as concrete examples, we demonstrate that adversarial content can hijack control and communication within the system to invoke unsafe agents and functionalities. This results in a complete security breach, up to execution of arbitrary malicious code on the user's device and/or exfiltration of sensitive data from the user's containerized environment. We show that control-flow hijacking attacks succeed even if the individual agents are not susceptible to direct or indirect prompt injection, and even if they refuse to perform harmful actions.",
        "field": "Autonomous Banking Agents",
        "link": "http://arxiv.org/abs/2503.12188v1"
    },
    {
        "id": "2503.10809v1",
        "title": "Attacking Multimodal OS Agents with Malicious Image Patches",
        "authors": [
            "Lukas Aichberger",
            "Alasdair Paren",
            "Yarin Gal",
            "Philip Torr",
            "Adel Bibi"
        ],
        "published": "2025-03-13T18:59:12Z",
        "summary": "Recent advances in operating system (OS) agents enable vision-language models to interact directly with the graphical user interface of an OS. These multimodal OS agents autonomously perform computer-based tasks in response to a single prompt via application programming interfaces (APIs). Such APIs typically support low-level operations, including mouse clicks, keyboard inputs, and screenshot captures. We introduce a novel attack vector: malicious image patches (MIPs) that have been adversarially perturbed so that, when captured in a screenshot, they cause an OS agent to perform harmful actions by exploiting specific APIs. For instance, MIPs embedded in desktop backgrounds or shared on social media can redirect an agent to a malicious website, enabling further exploitation. These MIPs generalise across different user requests and screen layouts, and remain effective for multiple OS agents. The existence of such attacks highlights critical security vulnerabilities in OS agents, which should be carefully addressed before their widespread adoption.",
        "field": "Autonomous Banking Agents",
        "link": "http://arxiv.org/abs/2503.10809v1"
    },
    {
        "id": "2503.09586v1",
        "title": "Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot",
        "authors": [
            "Andrew Crossman",
            "Andrew R. Plummer",
            "Chandra Sekharudu",
            "Deepak Warrier",
            "Mohammad Yekrangian"
        ],
        "published": "2025-03-12T17:54:18Z",
        "summary": "We present Auspex - a threat modeling system built using a specialized collection of generative artificial intelligence-based methods that capture threat modeling tradecraft. This new approach, called tradecraft prompting, centers on encoding the on-the-ground knowledge of threat modelers within the prompts that drive a generative AI-based threat modeling system. Auspex employs tradecraft prompts in two processing stages. The first stage centers on ingesting and processing system architecture information using prompts that encode threat modeling tradecraft knowledge pertaining to system decomposition and description. The second stage centers on chaining the resulting system analysis through a collection of prompts that encode tradecraft knowledge on threat identification, classification, and mitigation. The two-stage process yields a threat matrix for a system that specifies threat scenarios, threat types, information security categorizations and potential mitigations. Auspex produces formalized threat model output in minutes, relative to the weeks or months a manual process takes. More broadly, the focus on bespoke tradecraft prompting, as opposed to fine-tuning or agent-based add-ons, makes Auspex a lightweight, flexible, modular, and extensible foundational system capable of addressing the complexity, resource, and standardization limitations of both existing manual and automated threat modeling processes. In this connection, we establish the baseline value of Auspex to threat modelers through an evaluation procedure based on feedback collected from cybersecurity subject matter experts measuring the quality and utility of threat models generated by Auspex on real banking systems. We conclude with a discussion of system performance and plans for enhancements to Auspex.",
        "field": "Autonomous Banking Agents",
        "link": "http://arxiv.org/abs/2503.09586v1"
    },
    {
        "id": "2503.14279v1",
        "title": "Anti-Tamper Radio meets Reconfigurable Intelligent Surface for System-Level Tamper Detection",
        "authors": [
            "Maryam Shaygan Tabar",
            "Johannes Kortz",
            "Paul Staat",
            "Harald Elders-Boll",
            "Christof Paar",
            "Christian Zenger"
        ],
        "published": "2025-03-18T14:18:31Z",
        "summary": "Many computing systems need to be protected against physical attacks using active tamper detection based on sensors. One technical solution is to employ an ATR (Anti-Tamper Radio) approach, analyzing the radio wave propagation effects within a protected device to detect unauthorized physical alterations. However, ATR systems face key challenges in terms of susceptibility to signal manipulation attacks, limited reliability due to environmental noise, and regulatory constraints from wide bandwidth usage. In this work, we propose and experimentally evaluate an ATR system complemented by an RIS to dynamically reconfigure the wireless propagation environment. We show that this approach can enhance resistance against signal manipulation attacks, reduce bandwidth requirements from several~GHz down to as low as 20 MHz, and improve robustness to environmental disturbances such as internal fan movements. Our work demonstrates that RIS integration can strengthen the ATR performance to enhance security, sensitivity, and robustness, recognizing the potential of smart radio environments for ATR-based tamper detection",
        "field": "Digital Asset Custody Solutions",
        "link": "http://arxiv.org/abs/2503.14279v1"
    },
    {
        "id": "2503.14006v1",
        "title": "Securing Automated Insulin Delivery Systems: A Review of Security Threats and Protectives Strategies",
        "authors": [
            "Yuchen Niu",
            "Siew-Kei Lam"
        ],
        "published": "2025-03-18T08:11:19Z",
        "summary": "Automated insulin delivery (AID) systems have emerged as a significant technological advancement in diabetes care. These systems integrate a continuous glucose monitor, an insulin pump, and control algorithms to automate insulin delivery, reducing the burden of self-management and offering enhanced glucose control. However, the increasing reliance on wireless connectivity and software control has exposed AID systems to critical security risks that could result in life-threatening treatment errors. This review first presents a comprehensive examination of the security landscape, covering technical vulnerabilities, legal frameworks, and commercial product considerations, and an analysis of existing research on attack vectors, defence mechanisms, as well as evaluation methods and resources for AID systems. Despite recent advancements, several open challenges remain in achieving secure AID systems, particularly in standardising security evaluation frameworks and developing comprehensive, lightweight, and adaptive defence strategies. As one of the most widely adopted and extensively studied physiologic closed-loop control systems, this review serves as a valuable reference for understanding security challenges and solutions applicable to analogous medical systems.",
        "field": "Digital Asset Custody Solutions",
        "link": "http://arxiv.org/abs/2503.14006v1"
    },
    {
        "id": "2503.13994v1",
        "title": "TarPro: Targeted Protection against Malicious Image Editing",
        "authors": [
            "Kaixin Shen",
            "Ruijie Quan",
            "Jiaxu Miao",
            "Jun Xiao",
            "Yi Yang"
        ],
        "published": "2025-03-18T07:54:44Z",
        "summary": "The rapid advancement of image editing techniques has raised concerns about their misuse for generating Not-Safe-for-Work (NSFW) content. This necessitates a targeted protection mechanism that blocks malicious edits while preserving normal editability. However, existing protection methods fail to achieve this balance, as they indiscriminately disrupt all edits while still allowing some harmful content to be generated. To address this, we propose TarPro, a targeted protection framework that prevents malicious edits while maintaining benign modifications. TarPro achieves this through a semantic-aware constraint that only disrupts malicious content and a lightweight perturbation generator that produces a more stable, imperceptible, and robust perturbation for image protection. Extensive experiments demonstrate that TarPro surpasses existing methods, achieving a high protection efficacy while ensuring minimal impact on normal edits. Our results highlight TarPro as a practical solution for secure and controlled image editing.",
        "field": "Digital Asset Custody Solutions",
        "link": "http://arxiv.org/abs/2503.13994v1"
    },
    {
        "id": "2503.13419v1",
        "title": "Securing Virtual Reality Experiences: Unveiling and Tackling Cybersickness Attacks with Explainable AI",
        "authors": [
            "Ripan Kumar Kundu",
            "Matthew Denton",
            "Genova Mongalo",
            "Prasad Calyam",
            "Khaza Anuarul Hoque"
        ],
        "published": "2025-03-17T17:49:51Z",
        "summary": "The synergy between virtual reality (VR) and artificial intelligence (AI), specifically deep learning (DL)-based cybersickness detection models, has ushered in unprecedented advancements in immersive experiences by automatically detecting cybersickness severity and adaptively various mitigation techniques, offering a smooth and comfortable VR experience. While this DL-enabled cybersickness detection method provides promising solutions for enhancing user experiences, it also introduces new risks since these models are vulnerable to adversarial attacks; a small perturbation of the input data that is visually undetectable to human observers can fool the cybersickness detection model and trigger unexpected mitigation, thus disrupting user immersive experiences (UIX) and even posing safety risks. In this paper, we present a new type of VR attack, i.e., a cybersickness attack, which successfully stops the triggering of cybersickness mitigation by fooling DL-based cybersickness detection models and dramatically hinders the UIX. Next, we propose a novel explainable artificial intelligence (XAI)-guided cybersickness attack detection framework to detect such attacks in VR to ensure UIX and a comfortable VR experience. We evaluate the proposed attack and the detection framework using two state-of-the-art open-source VR cybersickness datasets: Simulation 2021 and Gameplay dataset. Finally, to verify the effectiveness of our proposed method, we implement the attack and the XAI-based detection using a testbed with a custom-built VR roller coaster simulation with an HTC Vive Pro Eye headset and perform a user study. Our study shows that such an attack can dramatically hinder the UIX. However, our proposed XAI-guided cybersickness attack detection can successfully detect cybersickness attacks and trigger the proper mitigation, effectively reducing VR cybersickness.",
        "field": "Digital Asset Custody Solutions",
        "link": "http://arxiv.org/abs/2503.13419v1"
    },
    {
        "id": "2503.12958v1",
        "title": "FedSDP: Explainable Differential Privacy in Federated Learning via Shapley Values",
        "authors": [
            "Yunbo Li",
            "Jiaping Gui",
            "Yue Wu"
        ],
        "published": "2025-03-17T09:14:19Z",
        "summary": "Federated learning (FL) enables participants to store data locally while collaborating in training, yet it remains vulnerable to privacy attacks, such as data reconstruction. Existing differential privacy (DP) technologies inject noise dynamically into the training process to mitigate the impact of excessive noise. However, this dynamic scheduling is often grounded in factors indirectly related to privacy, making it difficult to clearly explain the intricate relationship between dynamic noise adjustments and privacy requirements. To address this issue, we propose FedSDP, a novel and explainable DP-based privacy protection mechanism that guides noise injection based on privacy contribution. Specifically, FedSDP leverages Shapley values to assess the contribution of private attributes to local model training and dynamically adjusts the amount of noise injected accordingly. By providing theoretical insights into the injection of varying scales of noise into local training, FedSDP enhances interpretability. Extensive experiments demonstrate that FedSDP can achieve a superior balance between privacy preservation and model performance, surpassing state-of-the-art (SOTA) solutions.",
        "field": "Digital Asset Custody Solutions",
        "link": "http://arxiv.org/abs/2503.12958v1"
    },
    {
        "id": "2503.09823v1",
        "title": "Data Traceability for Privacy Alignment",
        "authors": [
            "Kevin Liao",
            "Shreya Thipireddy",
            "Daniel Weitzner"
        ],
        "published": "2025-03-12T20:42:23Z",
        "summary": "This paper offers a new privacy approach for the growing ecosystem of services--ranging from open banking to healthcare--dependent on sensitive personal data sharing between individuals and third-parties. While these services offer significant benefits, individuals want control over their data, transparency regarding how their data is used, and accountability from third-parties for misuse. However, existing legal and technical mechanisms are inadequate for supporting these needs. A comprehensive approach to the modern privacy challenges of accountable third-party data sharing requires a closer alignment of technical system architecture and legal institutional design. In order to achieve this privacy alignment, we extend traditional security threat modeling and analysis to encompass a broader range of privacy notions than has been typically considered. In particular, we introduce the concept of covert-accountability, which addresses adversaries that may act dishonestly but face potential identification and legal consequences. As a concrete instance of this design approach, we present the OTrace protocol, designed to provide traceable, accountable, consumer-control in third-party data sharing ecosystems. OTrace empowers consumers with the knowledge of where their data is, who has it, what it is being used for, and whom it is being shared with. By applying our alignment framework to OTrace, we demonstrate that OTrace's technical affordances can provide more confident, scalable regulatory oversight when combined with complementary legal mechanisms.",
        "field": "Smart Contracts in Banking",
        "link": "http://arxiv.org/abs/2503.09823v1"
    },
    {
        "id": "2503.09586v1",
        "title": "Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot",
        "authors": [
            "Andrew Crossman",
            "Andrew R. Plummer",
            "Chandra Sekharudu",
            "Deepak Warrier",
            "Mohammad Yekrangian"
        ],
        "published": "2025-03-12T17:54:18Z",
        "summary": "We present Auspex - a threat modeling system built using a specialized collection of generative artificial intelligence-based methods that capture threat modeling tradecraft. This new approach, called tradecraft prompting, centers on encoding the on-the-ground knowledge of threat modelers within the prompts that drive a generative AI-based threat modeling system. Auspex employs tradecraft prompts in two processing stages. The first stage centers on ingesting and processing system architecture information using prompts that encode threat modeling tradecraft knowledge pertaining to system decomposition and description. The second stage centers on chaining the resulting system analysis through a collection of prompts that encode tradecraft knowledge on threat identification, classification, and mitigation. The two-stage process yields a threat matrix for a system that specifies threat scenarios, threat types, information security categorizations and potential mitigations. Auspex produces formalized threat model output in minutes, relative to the weeks or months a manual process takes. More broadly, the focus on bespoke tradecraft prompting, as opposed to fine-tuning or agent-based add-ons, makes Auspex a lightweight, flexible, modular, and extensible foundational system capable of addressing the complexity, resource, and standardization limitations of both existing manual and automated threat modeling processes. In this connection, we establish the baseline value of Auspex to threat modelers through an evaluation procedure based on feedback collected from cybersecurity subject matter experts measuring the quality and utility of threat models generated by Auspex on real banking systems. We conclude with a discussion of system performance and plans for enhancements to Auspex.",
        "field": "Smart Contracts in Banking",
        "link": "http://arxiv.org/abs/2503.09586v1"
    },
    {
        "id": "2503.00070v1",
        "title": "Systematic Review of Cybersecurity in Banking: Evolution from Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain, Policies and Practice",
        "authors": [
            "Tue Nhi Tran"
        ],
        "published": "2025-02-27T14:17:06Z",
        "summary": "Throughout the history from pre-industry 4.0 to post-industry 4.0, cybersecurity at banks has undergone significant changes. Pre-industry 4.0 cyber security at banks relied on individual security methods that were highly manual and had low accuracy. When moving to post-industry 4.0, cybersecurity at banks had a major turning point with security methods that combined different technologies such as Artificial Intelligence (AI), Blockchain, IoT, automating necessary processes and significantly increasing the defence layer for banks. However, along with the development of new technologies, the current challenge of cybersecurity at banks lies in scalability, high costs and resources in both money and time for R&D of defence methods along with the threat of high-tech cybercriminals growing and expanding. This report goes from introducing the importance of cybersecurity at banks, analyzing their management, operational and business objectives, evaluating pre-industry 4.0 technologies used for cybersecurity at banks to assessing post-industry 4.0 technologies focusing on Artificial Intelligence and Blockchain, discussing current policies and practices and ending with discussing key advantages and challenges for 4.0 technologies and recommendations for further developing cybersecurity at banks.",
        "field": "Smart Contracts in Banking",
        "link": "http://arxiv.org/abs/2503.00070v1"
    },
    {
        "id": "2503.10644v1",
        "title": "Combined climate stress testing of supply-chain networks and the financial system with nation-wide firm-level emission estimates",
        "authors": [
            "Zlata Tabachová",
            "Christian Diem",
            "Johannes Stangl",
            "András Borsos",
            "Stefan Thurner"
        ],
        "published": "2025-02-25T20:25:47Z",
        "summary": "On the way towards carbon neutrality, climate stress testing provides estimates for the physical and transition risks that climate change poses to the economy and the financial system. Missing firm-level CO2 emissions data severely impedes the assessment of transition risks originating from carbon pricing. Based on the individual emissions of all Hungarian firms (410,523), as estimated from their fossil fuel purchases, we conduct a stress test of both actual and hypothetical carbon pricing policies. Using a simple 1:1 economic ABM and introducing the new carbon-to-profit ratio, we identify firms that become unprofitable and default, and estimate the respective loan write-offs. We find that 45% of all companies are directly exposed to carbon pricing. At a price of 45 EUR/t, direct economic losses of 1.3% of total sales and bank equity losses of 1.2% are expected. Secondary default cascades in supply chain networks could increase these losses by 300% to 4000%, depending on firms' ability to substitute essential inputs. To reduce transition risks, firms should reduce their dependence on essential inputs from supply chains with high CO2 exposure. We discuss the implications of different policy implementations on these transition risks.",
        "field": "Smart Contracts in Banking",
        "link": "http://arxiv.org/abs/2503.10644v1"
    },
    {
        "id": "2502.14766v2",
        "title": "Multi-Layer Deep xVA: Structural Credit Models, Measure Changes and Convergence Analysis",
        "authors": [
            "Kristoffer Andersson",
            "Alessandro Gnoatto"
        ],
        "published": "2025-02-20T17:41:55Z",
        "summary": "We propose a structural default model for portfolio-wide valuation adjustments (xVAs) and represent it as a system of coupled backward stochastic differential equations. The framework is divided into four layers, each capturing a key component: (i) clean values, (ii) initial margin and Collateral Valuation Adjustment (ColVA), (iii) Credit/Debit Valuation Adjustments (CVA/DVA) together with Margin Valuation Adjustment (MVA), and (iv) Funding Valuation Adjustment (FVA). Because these layers depend on one another through collateral and default effects, a naive Monte Carlo approach would require deeply nested simulations, making the problem computationally intractable. To address this challenge, we use an iterative deep BSDE approach, handling each layer sequentially so that earlier outputs serve as inputs to the subsequent layers. Initial margin is computed via deep quantile regression to reflect margin requirements over the Margin Period of Risk. We also adopt a change-of-measure method that highlights rare but significant defaults of the bank or counterparty, ensuring that these events are accurately captured in the training process. We further extend Han and Long's (2020) a posteriori error analysis to BSDEs on bounded domains. Due to the random exit from the domain, we obtain an order of convergence of $\\mathcal{O}(h^{1/4-\\epsilon})$ rather than the usual $\\mathcal{O}(h^{1/2})$. Numerical experiments illustrate that this method drastically reduces computational demands and successfully scales to high-dimensional, non-symmetric portfolios. The results confirm its effectiveness and accuracy, offering a practical alternative to nested Monte Carlo simulations in multi-counterparty xVA analyses.",
        "field": "Smart Contracts in Banking",
        "link": "http://arxiv.org/abs/2502.14766v2"
    },
    {
        "id": "2503.13637v1",
        "title": "XChainDataGen: A Cross-Chain Dataset Generation Framework",
        "authors": [
            "André Augusto",
            "André Vasconcelos",
            "Miguel Correia",
            "Luyao Zhang"
        ],
        "published": "2025-03-17T18:39:43Z",
        "summary": "The number of blockchain interoperability protocols for transferring data and assets between blockchains has grown significantly. However, no open dataset of cross-chain transactions exists to study interoperability protocols in operation. There is also no tool to generate such datasets and make them available to the community. This paper proposes XChainDataGen, a tool to extract cross-chain data from blockchains and generate datasets of cross-chain transactions (cctxs). Using XChainDataGen, we extracted over 35 GB of data from five cross-chain protocols deployed on 11 blockchains in the last seven months of 2024, identifying 11,285,753 cctxs that moved over 28 billion USD in cross-chain token transfers. Using the data collected, we compare protocols and provide insights into their security, cost, and performance trade-offs. As examples, we highlight differences between protocols that require full finality on the source blockchain and those that only demand soft finality (\\textit{security}). We compare user costs, fee models, and the impact of variables such as the Ethereum gas price on protocol fees (\\textit{cost}). Finally, we produce the first analysis of the implications of EIP-7683 for cross-chain intents, which are increasingly popular and greatly improve the speed with which cctxs are processed (\\textit{performance}), thereby enhancing the user experience. The availability of XChainDataGen and this dataset allows various analyses, including trends in cross-chain activity, security assessments of interoperability protocols, and financial research on decentralized finance (DeFi) protocols.",
        "field": "Digital Wallet Interoperability",
        "link": "http://arxiv.org/abs/2503.13637v1"
    },
    {
        "id": "2503.12952v1",
        "title": "Performance Analysis and Industry Deployment of Post-Quantum Cryptography Algorithms",
        "authors": [
            "Elif Dicle Demir",
            "Buse Bilgin",
            "Mehmet Cengiz Onbasli"
        ],
        "published": "2025-03-17T09:06:03Z",
        "summary": "As quantum computing advances, modern cryptographic standards face an existential threat, necessitating a transition to post-quantum cryptography (PQC). The National Institute of Standards and Technology (NIST) has selected CRYSTALS-Kyber and CRYSTALS-Dilithium as standardized PQC algorithms for secure key exchange and digital signatures, respectively. This study conducts a comprehensive performance analysis of these algorithms by benchmarking execution times across cryptographic operations such as key generation, encapsulation, decapsulation, signing, and verification. Additionally, the impact of AVX2 optimizations is evaluated to assess hardware acceleration benefits. Our findings demonstrate that Kyber and Dilithium achieve efficient execution times, outperforming classical cryptographic schemes such as RSA and ECDSA at equivalent security levels. Beyond technical performance, the real-world deployment of PQC introduces challenges in telecommunications networks, where large-scale infrastructure upgrades, interoperability with legacy systems, and regulatory constraints must be addressed. This paper examines the feasibility of PQC adoption in telecom environments, highlighting key transition challenges, security risks, and implementation strategies. Through industry case studies, we illustrate how telecom operators are integrating PQC into 5G authentication, subscriber identity protection, and secure communications. Our analysis provides insights into the computational trade-offs, deployment considerations, and standardization efforts shaping the future of quantum-safe cryptographic infrastructure.",
        "field": "Digital Wallet Interoperability",
        "link": "http://arxiv.org/abs/2503.12952v1"
    },
    {
        "id": "2503.09666v1",
        "title": "Blockchain-Enabled Management Framework for Federated Coalition Networks",
        "authors": [
            "Jorge Álvaro González",
            "Ana María Saiz García",
            "Victor Monzon Baeza"
        ],
        "published": "2025-03-12T16:59:23Z",
        "summary": "In a globalized and interconnected world, interoperability has become a key concept for advancing tactical scenarios. Federated Coalition Networks (FCN) enable cooperation between entities from multiple nations while allowing each to maintain control over their systems. However, this interoperability necessitates the sharing of increasing amounts of information between different tactical assets, raising the need for higher security measures. Emerging technologies like blockchain drive a revolution in secure communications, paving the way for new tactical scenarios. In this work, we propose a blockchain-based framework to enhance the resilience and security of the management of these networks. We offer a guide to FCN design to help a broad audience understand the military networks in international missions by a use case and key functions applied to a proposed architecture. We evaluate its effectiveness and performance in information encryption to validate this framework.",
        "field": "Digital Wallet Interoperability",
        "link": "http://arxiv.org/abs/2503.09666v1"
    },
    {
        "id": "2503.09165v1",
        "title": "Blockchain Data Analytics: Review and Challenges",
        "authors": [
            "Rischan Mafrur"
        ],
        "published": "2025-03-12T08:49:51Z",
        "summary": "The integration of blockchain technology with data analytics is essential for extracting insights in the cryptocurrency space. Although academic literature on blockchain data analytics is limited, various industry solutions have emerged to address these needs. This paper provides a comprehensive literature review, drawing from both academic research and industry applications. We classify blockchain analytics tools into categories such as block explorers, on-chain data providers, research platforms, and crypto market data providers. Additionally, we discuss the challenges associated with blockchain data analytics, including data accessibility, scalability, accuracy, and interoperability. Our findings emphasize the importance of bridging academic research and industry innovations to advance blockchain data analytics.",
        "field": "Digital Wallet Interoperability",
        "link": "http://arxiv.org/abs/2503.09165v1"
    },
    {
        "id": "2503.07857v1",
        "title": "Efficient Resource Management for Secure and Low-Latency O-RAN Communication",
        "authors": [
            "Zaineh Abughazzah",
            "Emna Baccour",
            "Ahmed Refaey",
            "Amr Mohamed",
            "Mounir Hamdi"
        ],
        "published": "2025-03-10T21:03:48Z",
        "summary": "Open Radio Access Networks (O-RAN) are transforming telecommunications by shifting from centralized to distributed architectures, promoting flexibility, interoperability, and innovation through open interfaces and multi-vendor environments. However, O-RAN's reliance on cloud-based architecture and enhanced observability introduces significant security and resource management challenges. Efficient resource management is crucial for secure and reliable communication in O-RAN, within the resource-constrained environment and heterogeneity of requirements, where multiple User Equipment (UE) and O-RAN Radio Units (O-RUs) coexist. This paper develops a framework to manage these aspects, ensuring each O-RU is associated with UEs based on their communication channel qualities and computational resources, and selecting appropriate encryption algorithms to safeguard data confidentiality, integrity, and authentication. A Multi-objective Optimization Problem (MOP) is formulated to minimize latency and maximize security within resource constraints. Different approaches are proposed to relax the complexity of the problem and achieve near-optimal performance, facilitating trade-offs between latency, security, and solution complexity. Simulation results demonstrate that the proposed approaches are close enough to the optimal solution, proving that our approach is both effective and efficient.",
        "field": "Digital Wallet Interoperability",
        "link": "http://arxiv.org/abs/2503.07857v1"
    },
    {
        "id": "2503.10809v1",
        "title": "Attacking Multimodal OS Agents with Malicious Image Patches",
        "authors": [
            "Lukas Aichberger",
            "Alasdair Paren",
            "Yarin Gal",
            "Philip Torr",
            "Adel Bibi"
        ],
        "published": "2025-03-13T18:59:12Z",
        "summary": "Recent advances in operating system (OS) agents enable vision-language models to interact directly with the graphical user interface of an OS. These multimodal OS agents autonomously perform computer-based tasks in response to a single prompt via application programming interfaces (APIs). Such APIs typically support low-level operations, including mouse clicks, keyboard inputs, and screenshot captures. We introduce a novel attack vector: malicious image patches (MIPs) that have been adversarially perturbed so that, when captured in a screenshot, they cause an OS agent to perform harmful actions by exploiting specific APIs. For instance, MIPs embedded in desktop backgrounds or shared on social media can redirect an agent to a malicious website, enabling further exploitation. These MIPs generalise across different user requests and screen layouts, and remain effective for multiple OS agents. The existence of such attacks highlights critical security vulnerabilities in OS agents, which should be carefully addressed before their widespread adoption.",
        "field": "Holographic Banking Interfaces",
        "link": "http://arxiv.org/abs/2503.10809v1"
    },
    {
        "id": "2503.08284v1",
        "title": "Neural cyberattacks applied to the vision under realistic visual stimuli",
        "authors": [
            "Victoria Magdalena López Madejska",
            "Sergio López Bernal",
            "Gregorio Martínez Pérez",
            "Alberto Huertas Celdrán"
        ],
        "published": "2025-03-11T10:58:58Z",
        "summary": "Brain-Computer Interfaces (BCIs) are systems traditionally used in medicine and designed to interact with the brain to record or stimulate neurons. Despite their benefits, the literature has demonstrated that invasive BCIs focused on neurostimulation present vulnerabilities allowing attackers to gain control. In this context, neural cyberattacks emerged as threats able to disrupt spontaneous neural activity by performing neural overstimulation or inhibition. Previous work validated these attacks in small-scale simulations with a reduced number of neurons, lacking real-world complexity. Thus, this work tackles this limitation by analyzing the impact of two existing neural attacks, Neuronal Flooding (FLO) and Neuronal Jamming (JAM), on a complex neuronal topology of the primary visual cortex of mice consisting of approximately 230,000 neurons, tested on three realistic visual stimuli: flash effect, movie, and drifting gratings. Each attack was evaluated over three relevant events per stimulus, also testing the impact of attacking 25% and 50% of the neurons. The results, based on the number of spikes and shift percentages metrics, showed that the attacks caused the greatest impact on the movie, while dark and fixed events are the most robust. Although both attacks can significantly affect neural activity, JAM was generally more damaging, producing longer temporal delays, and had a larger prevalence. Finally, JAM did not require to alter many neurons to significantly affect neural activity, while the impact in FLO increased with the number of neurons attacked.",
        "field": "Holographic Banking Interfaces",
        "link": "http://arxiv.org/abs/2503.08284v1"
    },
    {
        "id": "2503.07857v1",
        "title": "Efficient Resource Management for Secure and Low-Latency O-RAN Communication",
        "authors": [
            "Zaineh Abughazzah",
            "Emna Baccour",
            "Ahmed Refaey",
            "Amr Mohamed",
            "Mounir Hamdi"
        ],
        "published": "2025-03-10T21:03:48Z",
        "summary": "Open Radio Access Networks (O-RAN) are transforming telecommunications by shifting from centralized to distributed architectures, promoting flexibility, interoperability, and innovation through open interfaces and multi-vendor environments. However, O-RAN's reliance on cloud-based architecture and enhanced observability introduces significant security and resource management challenges. Efficient resource management is crucial for secure and reliable communication in O-RAN, within the resource-constrained environment and heterogeneity of requirements, where multiple User Equipment (UE) and O-RAN Radio Units (O-RUs) coexist. This paper develops a framework to manage these aspects, ensuring each O-RU is associated with UEs based on their communication channel qualities and computational resources, and selecting appropriate encryption algorithms to safeguard data confidentiality, integrity, and authentication. A Multi-objective Optimization Problem (MOP) is formulated to minimize latency and maximize security within resource constraints. Different approaches are proposed to relax the complexity of the problem and achieve near-optimal performance, facilitating trade-offs between latency, security, and solution complexity. Simulation results demonstrate that the proposed approaches are close enough to the optimal solution, proving that our approach is both effective and efficient.",
        "field": "Holographic Banking Interfaces",
        "link": "http://arxiv.org/abs/2503.07857v1"
    },
    {
        "id": "2503.08716v1",
        "title": "AuthorMist: Evading AI Text Detectors with Reinforcement Learning",
        "authors": [
            "Isaac David",
            "Arthur Gervais"
        ],
        "published": "2025-03-10T12:41:05Z",
        "summary": "In the age of powerful AI-generated text, automatic detectors have emerged to identify machine-written content. This poses a threat to author privacy and freedom, as text authored with AI assistance may be unfairly flagged. We propose AuthorMist, a novel reinforcement learning-based system to transform AI-generated text into human-like writing. AuthorMist leverages a 3-billion-parameter language model as a backbone, fine-tuned with Group Relative Policy Optimization (GPRO) to paraphrase text in a way that evades AI detectors. Our framework establishes a generic approach where external detector APIs (GPTZero, WinstonAI, Originality.ai, etc.) serve as reward functions within the reinforcement learning loop, enabling the model to systematically learn outputs that these detectors are less likely to classify as AI-generated. This API-as-reward methodology can be applied broadly to optimize text against any detector with an accessible interface. Experiments on multiple datasets and detectors demonstrate that AuthorMist effectively reduces the detectability of AI-generated text while preserving the original meaning. Our evaluation shows attack success rates ranging from 78.6% to 96.2% against individual detectors, significantly outperforming baseline paraphrasing methods. AuthorMist maintains high semantic similarity (above 0.94) with the original text while successfully evading detection. These results highlight limitations in current AI text detection technologies and raise questions about the sustainability of the detection-evasion arms race.",
        "field": "Holographic Banking Interfaces",
        "link": "http://arxiv.org/abs/2503.08716v1"
    },
    {
        "id": "2503.05303v1",
        "title": "Robust Intrusion Detection System with Explainable Artificial Intelligence",
        "authors": [
            "Betül Güvenç Paltun",
            "Ramin Fuladi",
            "Rim El Malki"
        ],
        "published": "2025-03-07T10:31:59Z",
        "summary": "Machine learning (ML) models serve as powerful tools for threat detection and mitigation; however, they also introduce potential new risks. Adversarial input can exploit these models through standard interfaces, thus creating new attack pathways that threaten critical network operations. As ML advancements progress, adversarial strategies become more advanced, and conventional defenses such as adversarial training are costly in computational terms and often fail to provide real-time detection. These methods typically require a balance between robustness and model performance, which presents challenges for applications that demand instant response. To further investigate this vulnerability, we suggest a novel strategy for detecting and mitigating adversarial attacks using eXplainable Artificial Intelligence (XAI). This approach is evaluated in real time within intrusion detection systems (IDS), leading to the development of a zero-touch mitigation strategy. Additionally, we explore various scenarios in the Radio Resource Control (RRC) layer within the Open Radio Access Network (O-RAN) framework, emphasizing the critical need for enhanced mitigation techniques to strengthen IDS defenses against advanced threats and implement a zero-touch mitigation solution. Extensive testing across different scenarios in the RRC layer of the O-RAN infrastructure validates the ability of the framework to detect and counteract integrated RRC-layer attacks when paired with adversarial strategies, emphasizing the essential need for robust defensive mechanisms to strengthen IDS against complex threats.",
        "field": "Holographic Banking Interfaces",
        "link": "http://arxiv.org/abs/2503.05303v1"
    },
    {
        "id": "2503.12952v1",
        "title": "Performance Analysis and Industry Deployment of Post-Quantum Cryptography Algorithms",
        "authors": [
            "Elif Dicle Demir",
            "Buse Bilgin",
            "Mehmet Cengiz Onbasli"
        ],
        "published": "2025-03-17T09:06:03Z",
        "summary": "As quantum computing advances, modern cryptographic standards face an existential threat, necessitating a transition to post-quantum cryptography (PQC). The National Institute of Standards and Technology (NIST) has selected CRYSTALS-Kyber and CRYSTALS-Dilithium as standardized PQC algorithms for secure key exchange and digital signatures, respectively. This study conducts a comprehensive performance analysis of these algorithms by benchmarking execution times across cryptographic operations such as key generation, encapsulation, decapsulation, signing, and verification. Additionally, the impact of AVX2 optimizations is evaluated to assess hardware acceleration benefits. Our findings demonstrate that Kyber and Dilithium achieve efficient execution times, outperforming classical cryptographic schemes such as RSA and ECDSA at equivalent security levels. Beyond technical performance, the real-world deployment of PQC introduces challenges in telecommunications networks, where large-scale infrastructure upgrades, interoperability with legacy systems, and regulatory constraints must be addressed. This paper examines the feasibility of PQC adoption in telecom environments, highlighting key transition challenges, security risks, and implementation strategies. Through industry case studies, we illustrate how telecom operators are integrating PQC into 5G authentication, subscriber identity protection, and secure communications. Our analysis provides insights into the computational trade-offs, deployment considerations, and standardization efforts shaping the future of quantum-safe cryptographic infrastructure.",
        "field": "Space-Based Banking Infrastructure",
        "link": "http://arxiv.org/abs/2503.12952v1"
    },
    {
        "id": "2503.08256v1",
        "title": "SoK: A cloudy view on trust relationships of CVMs -- How Confidential Virtual Machines are falling short in Public Cloud",
        "authors": [
            "Jana Eisoldt",
            "Anna Galanou",
            "Andrey Ruzhanskiy",
            "Nils Küchenmeister",
            "Yewgenij Baburkin",
            "Tianxiang Dai",
            "Ivan Gudymenko",
            "Stefan Köpsell",
            "Rüdiger Kapitza"
        ],
        "published": "2025-03-11T10:21:29Z",
        "summary": "Confidential computing in the public cloud intends to safeguard workload privacy while outsourcing infrastructure management to a cloud provider. This is achieved by executing customer workloads within so called Trusted Execution Environments (TEEs), such as Confidential Virtual Machines (CVMs), which protect them from unauthorized access by cloud administrators and privileged system software. At the core of confidential computing lies remote attestation -- a mechanism that enables workload owners to verify the initial state of their workload and furthermore authenticate the underlying hardware. hile this represents a significant advancement in cloud security, this SoK critically examines the confidential computing offerings of market-leading cloud providers to assess whether they genuinely adhere to its core principles. We develop a taxonomy based on carefully selected criteria to systematically evaluate these offerings, enabling us to analyse the components responsible for remote attestation, the evidence provided at each stage, the extent of cloud provider influence and whether this undermines the threat model of confidential computing. Specifically, we investigate how CVMs are deployed in the public cloud infrastructures, the extent to which customers can request and verify attestation evidence, and their ability to define and enforce configuration and attestation requirements. This analysis provides insight into whether confidential computing guarantees -- namely confidentiality and integrity -- are genuinely upheld. Our findings reveal that all major cloud providers retain control over critical parts of the trusted software stack and, in some cases, intervene in the standard remote attestation process. This directly contradicts their claims of delivering confidential computing, as the model fundamentally excludes the cloud provider from the set of trusted entities.",
        "field": "Space-Based Banking Infrastructure",
        "link": "http://arxiv.org/abs/2503.08256v1"
    },
    {
        "id": "2503.07196v1",
        "title": "QKD-KEM: Hybrid QKD Integration into TLS with OpenSSL Providers",
        "authors": [
            "Javier Blanco-Romero",
            "Pedro Otero García",
            "Daniel Sobral-Blanco",
            "Florina Almenares Mendoza",
            "Ana Fernández Vilas",
            "Rebeca P. Díaz-Redondo"
        ],
        "published": "2025-03-10T11:24:38Z",
        "summary": "Quantum Key Distribution (QKD) promises information-theoretic security, yet integrating QKD into existing protocols like TLS remains challenging due to its fundamentally different operational model. In this paper, we propose a hybrid QKD-KEM protocol with two distinct integration approaches: a client-initiated flow compatible with both ETSI 004 and 014 specifications, and a server-initiated flow similar to existing work but limited to stateless ETSI 014 APIs. Unlike previous implementations, our work specifically addresses the integration of stateful QKD key exchange protocols (ETSI 004) which is essential for production QKD networks but has remained largely unexplored. By adapting OpenSSL's provider infrastructure to accommodate QKD's pre-distributed key model, we maintain compatibility with current TLS implementations while offering dual layers of security. Performance evaluations demonstrate the feasibility of our hybrid scheme with acceptable overhead, showing that robust security against quantum threats is achievable while addressing the unique requirements of different QKD API specifications.",
        "field": "Space-Based Banking Infrastructure",
        "link": "http://arxiv.org/abs/2503.07196v1"
    },
    {
        "id": "2503.05303v1",
        "title": "Robust Intrusion Detection System with Explainable Artificial Intelligence",
        "authors": [
            "Betül Güvenç Paltun",
            "Ramin Fuladi",
            "Rim El Malki"
        ],
        "published": "2025-03-07T10:31:59Z",
        "summary": "Machine learning (ML) models serve as powerful tools for threat detection and mitigation; however, they also introduce potential new risks. Adversarial input can exploit these models through standard interfaces, thus creating new attack pathways that threaten critical network operations. As ML advancements progress, adversarial strategies become more advanced, and conventional defenses such as adversarial training are costly in computational terms and often fail to provide real-time detection. These methods typically require a balance between robustness and model performance, which presents challenges for applications that demand instant response. To further investigate this vulnerability, we suggest a novel strategy for detecting and mitigating adversarial attacks using eXplainable Artificial Intelligence (XAI). This approach is evaluated in real time within intrusion detection systems (IDS), leading to the development of a zero-touch mitigation strategy. Additionally, we explore various scenarios in the Radio Resource Control (RRC) layer within the Open Radio Access Network (O-RAN) framework, emphasizing the critical need for enhanced mitigation techniques to strengthen IDS defenses against advanced threats and implement a zero-touch mitigation solution. Extensive testing across different scenarios in the RRC layer of the O-RAN infrastructure validates the ability of the framework to detect and counteract integrated RRC-layer attacks when paired with adversarial strategies, emphasizing the essential need for robust defensive mechanisms to strengthen IDS against complex threats.",
        "field": "Space-Based Banking Infrastructure",
        "link": "http://arxiv.org/abs/2503.05303v1"
    },
    {
        "id": "2503.04178v1",
        "title": "Unsupervised anomaly detection on cybersecurity data streams: a case with BETH dataset",
        "authors": [
            "Evgeniy Eremin"
        ],
        "published": "2025-03-06T07:45:48Z",
        "summary": "In modern world the importance of cybersecurity of various systems is increasing from year to year. The number of information security events generated by information security tools grows up with the development of the IT infrastructure. At the same time, the cyber threat landscape does not remain constant, and monitoring should take into account both already known attack indicators and those for which there are no signature rules in information security products of various classes yet. Detecting anomalies in large cybersecurity data streams is a complex task that, if properly addressed, can allow for timely response to atypical and previously unknown cyber threats. The possibilities of using of offline algorithms may be limited for a number of reasons related to the time of training and the frequency of retraining. Using stream learning algorithms for solving this task is capable of providing near-real-time data processing. This article examines the results of ten algorithms from three Python stream machine-learning libraries on BETH dataset with cybersecurity events, which contains information about the creation, cloning, and destruction of operating system processes collected using extended eBPF. ROC-AUC metric and total processing time of processing with these algorithms are presented. Several combinations of features and the order of events are considered. In conclusion, some mentions are given about the most promising algorithms and possible directions for further research are outlined.",
        "field": "Space-Based Banking Infrastructure",
        "link": "http://arxiv.org/abs/2503.04178v1"
    },
    {
        "id": "1910.10099v1",
        "title": "Mesoscale impact of trader psychology on stock markets: a multi-agent AI approach",
        "authors": [
            "J. Lussange",
            "S. Palminteri",
            "S. Bourgeois-Gironde",
            "B. Gutkin"
        ],
        "published": "2019-10-10T15:15:06Z",
        "summary": "Recent advances in the fields of machine learning and neurofinance have yielded new exciting research perspectives in practical inference of behavioural economy in financial markets and microstructure study. We here present the latest results from a recently published stock market simulator built around a multi-agent system architecture, in which each agent is an autonomous investor trading stocks by reinforcement learning (RL) via a centralised double-auction limit order book. The RL framework allows for the implementation of specific behavioural and cognitive traits known to trader psychology, and thus to study the impact of these traits on the whole stock market at the mesoscale. More precisely, we narrowed our agent design to three such psychological biases known to have a direct correspondence with RL theory, namely delay discounting, greed, and fear. We compared ensuing simulated data to real stock market data over the past decade or so, and find that market stability benefits from larger populations of agents prone to delay discounting and most astonishingly, to greed.",
        "field": "Neurofinance",
        "link": "http://arxiv.org/abs/1910.10099v1"
    },
    {
        "id": "2503.14279v1",
        "title": "Anti-Tamper Radio meets Reconfigurable Intelligent Surface for System-Level Tamper Detection",
        "authors": [
            "Maryam Shaygan Tabar",
            "Johannes Kortz",
            "Paul Staat",
            "Harald Elders-Boll",
            "Christof Paar",
            "Christian Zenger"
        ],
        "published": "2025-03-18T14:18:31Z",
        "summary": "Many computing systems need to be protected against physical attacks using active tamper detection based on sensors. One technical solution is to employ an ATR (Anti-Tamper Radio) approach, analyzing the radio wave propagation effects within a protected device to detect unauthorized physical alterations. However, ATR systems face key challenges in terms of susceptibility to signal manipulation attacks, limited reliability due to environmental noise, and regulatory constraints from wide bandwidth usage. In this work, we propose and experimentally evaluate an ATR system complemented by an RIS to dynamically reconfigure the wireless propagation environment. We show that this approach can enhance resistance against signal manipulation attacks, reduce bandwidth requirements from several~GHz down to as low as 20 MHz, and improve robustness to environmental disturbances such as internal fan movements. Our work demonstrates that RIS integration can strengthen the ATR performance to enhance security, sensitivity, and robustness, recognizing the potential of smart radio environments for ATR-based tamper detection",
        "field": "Blockchain-Based KYC Solutions",
        "link": "http://arxiv.org/abs/2503.14279v1"
    },
    {
        "id": "2503.14006v1",
        "title": "Securing Automated Insulin Delivery Systems: A Review of Security Threats and Protectives Strategies",
        "authors": [
            "Yuchen Niu",
            "Siew-Kei Lam"
        ],
        "published": "2025-03-18T08:11:19Z",
        "summary": "Automated insulin delivery (AID) systems have emerged as a significant technological advancement in diabetes care. These systems integrate a continuous glucose monitor, an insulin pump, and control algorithms to automate insulin delivery, reducing the burden of self-management and offering enhanced glucose control. However, the increasing reliance on wireless connectivity and software control has exposed AID systems to critical security risks that could result in life-threatening treatment errors. This review first presents a comprehensive examination of the security landscape, covering technical vulnerabilities, legal frameworks, and commercial product considerations, and an analysis of existing research on attack vectors, defence mechanisms, as well as evaluation methods and resources for AID systems. Despite recent advancements, several open challenges remain in achieving secure AID systems, particularly in standardising security evaluation frameworks and developing comprehensive, lightweight, and adaptive defence strategies. As one of the most widely adopted and extensively studied physiologic closed-loop control systems, this review serves as a valuable reference for understanding security challenges and solutions applicable to analogous medical systems.",
        "field": "Blockchain-Based KYC Solutions",
        "link": "http://arxiv.org/abs/2503.14006v1"
    },
    {
        "id": "2503.13994v1",
        "title": "TarPro: Targeted Protection against Malicious Image Editing",
        "authors": [
            "Kaixin Shen",
            "Ruijie Quan",
            "Jiaxu Miao",
            "Jun Xiao",
            "Yi Yang"
        ],
        "published": "2025-03-18T07:54:44Z",
        "summary": "The rapid advancement of image editing techniques has raised concerns about their misuse for generating Not-Safe-for-Work (NSFW) content. This necessitates a targeted protection mechanism that blocks malicious edits while preserving normal editability. However, existing protection methods fail to achieve this balance, as they indiscriminately disrupt all edits while still allowing some harmful content to be generated. To address this, we propose TarPro, a targeted protection framework that prevents malicious edits while maintaining benign modifications. TarPro achieves this through a semantic-aware constraint that only disrupts malicious content and a lightweight perturbation generator that produces a more stable, imperceptible, and robust perturbation for image protection. Extensive experiments demonstrate that TarPro surpasses existing methods, achieving a high protection efficacy while ensuring minimal impact on normal edits. Our results highlight TarPro as a practical solution for secure and controlled image editing.",
        "field": "Blockchain-Based KYC Solutions",
        "link": "http://arxiv.org/abs/2503.13994v1"
    },
    {
        "id": "2503.13419v1",
        "title": "Securing Virtual Reality Experiences: Unveiling and Tackling Cybersickness Attacks with Explainable AI",
        "authors": [
            "Ripan Kumar Kundu",
            "Matthew Denton",
            "Genova Mongalo",
            "Prasad Calyam",
            "Khaza Anuarul Hoque"
        ],
        "published": "2025-03-17T17:49:51Z",
        "summary": "The synergy between virtual reality (VR) and artificial intelligence (AI), specifically deep learning (DL)-based cybersickness detection models, has ushered in unprecedented advancements in immersive experiences by automatically detecting cybersickness severity and adaptively various mitigation techniques, offering a smooth and comfortable VR experience. While this DL-enabled cybersickness detection method provides promising solutions for enhancing user experiences, it also introduces new risks since these models are vulnerable to adversarial attacks; a small perturbation of the input data that is visually undetectable to human observers can fool the cybersickness detection model and trigger unexpected mitigation, thus disrupting user immersive experiences (UIX) and even posing safety risks. In this paper, we present a new type of VR attack, i.e., a cybersickness attack, which successfully stops the triggering of cybersickness mitigation by fooling DL-based cybersickness detection models and dramatically hinders the UIX. Next, we propose a novel explainable artificial intelligence (XAI)-guided cybersickness attack detection framework to detect such attacks in VR to ensure UIX and a comfortable VR experience. We evaluate the proposed attack and the detection framework using two state-of-the-art open-source VR cybersickness datasets: Simulation 2021 and Gameplay dataset. Finally, to verify the effectiveness of our proposed method, we implement the attack and the XAI-based detection using a testbed with a custom-built VR roller coaster simulation with an HTC Vive Pro Eye headset and perform a user study. Our study shows that such an attack can dramatically hinder the UIX. However, our proposed XAI-guided cybersickness attack detection can successfully detect cybersickness attacks and trigger the proper mitigation, effectively reducing VR cybersickness.",
        "field": "Blockchain-Based KYC Solutions",
        "link": "http://arxiv.org/abs/2503.13419v1"
    },
    {
        "id": "2503.12958v1",
        "title": "FedSDP: Explainable Differential Privacy in Federated Learning via Shapley Values",
        "authors": [
            "Yunbo Li",
            "Jiaping Gui",
            "Yue Wu"
        ],
        "published": "2025-03-17T09:14:19Z",
        "summary": "Federated learning (FL) enables participants to store data locally while collaborating in training, yet it remains vulnerable to privacy attacks, such as data reconstruction. Existing differential privacy (DP) technologies inject noise dynamically into the training process to mitigate the impact of excessive noise. However, this dynamic scheduling is often grounded in factors indirectly related to privacy, making it difficult to clearly explain the intricate relationship between dynamic noise adjustments and privacy requirements. To address this issue, we propose FedSDP, a novel and explainable DP-based privacy protection mechanism that guides noise injection based on privacy contribution. Specifically, FedSDP leverages Shapley values to assess the contribution of private attributes to local model training and dynamically adjusts the amount of noise injected accordingly. By providing theoretical insights into the injection of varying scales of noise into local training, FedSDP enhances interpretability. Extensive experiments demonstrate that FedSDP can achieve a superior balance between privacy preservation and model performance, surpassing state-of-the-art (SOTA) solutions.",
        "field": "Blockchain-Based KYC Solutions",
        "link": "http://arxiv.org/abs/2503.12958v1"
    },
    {
        "id": "2503.09823v1",
        "title": "Data Traceability for Privacy Alignment",
        "authors": [
            "Kevin Liao",
            "Shreya Thipireddy",
            "Daniel Weitzner"
        ],
        "published": "2025-03-12T20:42:23Z",
        "summary": "This paper offers a new privacy approach for the growing ecosystem of services--ranging from open banking to healthcare--dependent on sensitive personal data sharing between individuals and third-parties. While these services offer significant benefits, individuals want control over their data, transparency regarding how their data is used, and accountability from third-parties for misuse. However, existing legal and technical mechanisms are inadequate for supporting these needs. A comprehensive approach to the modern privacy challenges of accountable third-party data sharing requires a closer alignment of technical system architecture and legal institutional design. In order to achieve this privacy alignment, we extend traditional security threat modeling and analysis to encompass a broader range of privacy notions than has been typically considered. In particular, we introduce the concept of covert-accountability, which addresses adversaries that may act dishonestly but face potential identification and legal consequences. As a concrete instance of this design approach, we present the OTrace protocol, designed to provide traceable, accountable, consumer-control in third-party data sharing ecosystems. OTrace empowers consumers with the knowledge of where their data is, who has it, what it is being used for, and whom it is being shared with. By applying our alignment framework to OTrace, we demonstrate that OTrace's technical affordances can provide more confident, scalable regulatory oversight when combined with complementary legal mechanisms.",
        "field": "Quantum Machine Learning in Banking",
        "link": "http://arxiv.org/abs/2503.09823v1"
    },
    {
        "id": "2503.09586v1",
        "title": "Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot",
        "authors": [
            "Andrew Crossman",
            "Andrew R. Plummer",
            "Chandra Sekharudu",
            "Deepak Warrier",
            "Mohammad Yekrangian"
        ],
        "published": "2025-03-12T17:54:18Z",
        "summary": "We present Auspex - a threat modeling system built using a specialized collection of generative artificial intelligence-based methods that capture threat modeling tradecraft. This new approach, called tradecraft prompting, centers on encoding the on-the-ground knowledge of threat modelers within the prompts that drive a generative AI-based threat modeling system. Auspex employs tradecraft prompts in two processing stages. The first stage centers on ingesting and processing system architecture information using prompts that encode threat modeling tradecraft knowledge pertaining to system decomposition and description. The second stage centers on chaining the resulting system analysis through a collection of prompts that encode tradecraft knowledge on threat identification, classification, and mitigation. The two-stage process yields a threat matrix for a system that specifies threat scenarios, threat types, information security categorizations and potential mitigations. Auspex produces formalized threat model output in minutes, relative to the weeks or months a manual process takes. More broadly, the focus on bespoke tradecraft prompting, as opposed to fine-tuning or agent-based add-ons, makes Auspex a lightweight, flexible, modular, and extensible foundational system capable of addressing the complexity, resource, and standardization limitations of both existing manual and automated threat modeling processes. In this connection, we establish the baseline value of Auspex to threat modelers through an evaluation procedure based on feedback collected from cybersecurity subject matter experts measuring the quality and utility of threat models generated by Auspex on real banking systems. We conclude with a discussion of system performance and plans for enhancements to Auspex.",
        "field": "Quantum Machine Learning in Banking",
        "link": "http://arxiv.org/abs/2503.09586v1"
    },
    {
        "id": "2503.00070v1",
        "title": "Systematic Review of Cybersecurity in Banking: Evolution from Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain, Policies and Practice",
        "authors": [
            "Tue Nhi Tran"
        ],
        "published": "2025-02-27T14:17:06Z",
        "summary": "Throughout the history from pre-industry 4.0 to post-industry 4.0, cybersecurity at banks has undergone significant changes. Pre-industry 4.0 cyber security at banks relied on individual security methods that were highly manual and had low accuracy. When moving to post-industry 4.0, cybersecurity at banks had a major turning point with security methods that combined different technologies such as Artificial Intelligence (AI), Blockchain, IoT, automating necessary processes and significantly increasing the defence layer for banks. However, along with the development of new technologies, the current challenge of cybersecurity at banks lies in scalability, high costs and resources in both money and time for R&D of defence methods along with the threat of high-tech cybercriminals growing and expanding. This report goes from introducing the importance of cybersecurity at banks, analyzing their management, operational and business objectives, evaluating pre-industry 4.0 technologies used for cybersecurity at banks to assessing post-industry 4.0 technologies focusing on Artificial Intelligence and Blockchain, discussing current policies and practices and ending with discussing key advantages and challenges for 4.0 technologies and recommendations for further developing cybersecurity at banks.",
        "field": "Quantum Machine Learning in Banking",
        "link": "http://arxiv.org/abs/2503.00070v1"
    },
    {
        "id": "2503.10644v1",
        "title": "Combined climate stress testing of supply-chain networks and the financial system with nation-wide firm-level emission estimates",
        "authors": [
            "Zlata Tabachová",
            "Christian Diem",
            "Johannes Stangl",
            "András Borsos",
            "Stefan Thurner"
        ],
        "published": "2025-02-25T20:25:47Z",
        "summary": "On the way towards carbon neutrality, climate stress testing provides estimates for the physical and transition risks that climate change poses to the economy and the financial system. Missing firm-level CO2 emissions data severely impedes the assessment of transition risks originating from carbon pricing. Based on the individual emissions of all Hungarian firms (410,523), as estimated from their fossil fuel purchases, we conduct a stress test of both actual and hypothetical carbon pricing policies. Using a simple 1:1 economic ABM and introducing the new carbon-to-profit ratio, we identify firms that become unprofitable and default, and estimate the respective loan write-offs. We find that 45% of all companies are directly exposed to carbon pricing. At a price of 45 EUR/t, direct economic losses of 1.3% of total sales and bank equity losses of 1.2% are expected. Secondary default cascades in supply chain networks could increase these losses by 300% to 4000%, depending on firms' ability to substitute essential inputs. To reduce transition risks, firms should reduce their dependence on essential inputs from supply chains with high CO2 exposure. We discuss the implications of different policy implementations on these transition risks.",
        "field": "Quantum Machine Learning in Banking",
        "link": "http://arxiv.org/abs/2503.10644v1"
    },
    {
        "id": "2502.14766v2",
        "title": "Multi-Layer Deep xVA: Structural Credit Models, Measure Changes and Convergence Analysis",
        "authors": [
            "Kristoffer Andersson",
            "Alessandro Gnoatto"
        ],
        "published": "2025-02-20T17:41:55Z",
        "summary": "We propose a structural default model for portfolio-wide valuation adjustments (xVAs) and represent it as a system of coupled backward stochastic differential equations. The framework is divided into four layers, each capturing a key component: (i) clean values, (ii) initial margin and Collateral Valuation Adjustment (ColVA), (iii) Credit/Debit Valuation Adjustments (CVA/DVA) together with Margin Valuation Adjustment (MVA), and (iv) Funding Valuation Adjustment (FVA). Because these layers depend on one another through collateral and default effects, a naive Monte Carlo approach would require deeply nested simulations, making the problem computationally intractable. To address this challenge, we use an iterative deep BSDE approach, handling each layer sequentially so that earlier outputs serve as inputs to the subsequent layers. Initial margin is computed via deep quantile regression to reflect margin requirements over the Margin Period of Risk. We also adopt a change-of-measure method that highlights rare but significant defaults of the bank or counterparty, ensuring that these events are accurately captured in the training process. We further extend Han and Long's (2020) a posteriori error analysis to BSDEs on bounded domains. Due to the random exit from the domain, we obtain an order of convergence of $\\mathcal{O}(h^{1/4-\\epsilon})$ rather than the usual $\\mathcal{O}(h^{1/2})$. Numerical experiments illustrate that this method drastically reduces computational demands and successfully scales to high-dimensional, non-symmetric portfolios. The results confirm its effectiveness and accuracy, offering a practical alternative to nested Monte Carlo simulations in multi-counterparty xVA analyses.",
        "field": "Quantum Machine Learning in Banking",
        "link": "http://arxiv.org/abs/2502.14766v2"
    },
    {
        "id": "2503.09823v1",
        "title": "Data Traceability for Privacy Alignment",
        "authors": [
            "Kevin Liao",
            "Shreya Thipireddy",
            "Daniel Weitzner"
        ],
        "published": "2025-03-12T20:42:23Z",
        "summary": "This paper offers a new privacy approach for the growing ecosystem of services--ranging from open banking to healthcare--dependent on sensitive personal data sharing between individuals and third-parties. While these services offer significant benefits, individuals want control over their data, transparency regarding how their data is used, and accountability from third-parties for misuse. However, existing legal and technical mechanisms are inadequate for supporting these needs. A comprehensive approach to the modern privacy challenges of accountable third-party data sharing requires a closer alignment of technical system architecture and legal institutional design. In order to achieve this privacy alignment, we extend traditional security threat modeling and analysis to encompass a broader range of privacy notions than has been typically considered. In particular, we introduce the concept of covert-accountability, which addresses adversaries that may act dishonestly but face potential identification and legal consequences. As a concrete instance of this design approach, we present the OTrace protocol, designed to provide traceable, accountable, consumer-control in third-party data sharing ecosystems. OTrace empowers consumers with the knowledge of where their data is, who has it, what it is being used for, and whom it is being shared with. By applying our alignment framework to OTrace, we demonstrate that OTrace's technical affordances can provide more confident, scalable regulatory oversight when combined with complementary legal mechanisms.",
        "field": "Augmented Reality (AR) in Banking",
        "link": "http://arxiv.org/abs/2503.09823v1"
    },
    {
        "id": "2503.09586v1",
        "title": "Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot",
        "authors": [
            "Andrew Crossman",
            "Andrew R. Plummer",
            "Chandra Sekharudu",
            "Deepak Warrier",
            "Mohammad Yekrangian"
        ],
        "published": "2025-03-12T17:54:18Z",
        "summary": "We present Auspex - a threat modeling system built using a specialized collection of generative artificial intelligence-based methods that capture threat modeling tradecraft. This new approach, called tradecraft prompting, centers on encoding the on-the-ground knowledge of threat modelers within the prompts that drive a generative AI-based threat modeling system. Auspex employs tradecraft prompts in two processing stages. The first stage centers on ingesting and processing system architecture information using prompts that encode threat modeling tradecraft knowledge pertaining to system decomposition and description. The second stage centers on chaining the resulting system analysis through a collection of prompts that encode tradecraft knowledge on threat identification, classification, and mitigation. The two-stage process yields a threat matrix for a system that specifies threat scenarios, threat types, information security categorizations and potential mitigations. Auspex produces formalized threat model output in minutes, relative to the weeks or months a manual process takes. More broadly, the focus on bespoke tradecraft prompting, as opposed to fine-tuning or agent-based add-ons, makes Auspex a lightweight, flexible, modular, and extensible foundational system capable of addressing the complexity, resource, and standardization limitations of both existing manual and automated threat modeling processes. In this connection, we establish the baseline value of Auspex to threat modelers through an evaluation procedure based on feedback collected from cybersecurity subject matter experts measuring the quality and utility of threat models generated by Auspex on real banking systems. We conclude with a discussion of system performance and plans for enhancements to Auspex.",
        "field": "Augmented Reality (AR) in Banking",
        "link": "http://arxiv.org/abs/2503.09586v1"
    },
    {
        "id": "2503.00070v1",
        "title": "Systematic Review of Cybersecurity in Banking: Evolution from Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain, Policies and Practice",
        "authors": [
            "Tue Nhi Tran"
        ],
        "published": "2025-02-27T14:17:06Z",
        "summary": "Throughout the history from pre-industry 4.0 to post-industry 4.0, cybersecurity at banks has undergone significant changes. Pre-industry 4.0 cyber security at banks relied on individual security methods that were highly manual and had low accuracy. When moving to post-industry 4.0, cybersecurity at banks had a major turning point with security methods that combined different technologies such as Artificial Intelligence (AI), Blockchain, IoT, automating necessary processes and significantly increasing the defence layer for banks. However, along with the development of new technologies, the current challenge of cybersecurity at banks lies in scalability, high costs and resources in both money and time for R&D of defence methods along with the threat of high-tech cybercriminals growing and expanding. This report goes from introducing the importance of cybersecurity at banks, analyzing their management, operational and business objectives, evaluating pre-industry 4.0 technologies used for cybersecurity at banks to assessing post-industry 4.0 technologies focusing on Artificial Intelligence and Blockchain, discussing current policies and practices and ending with discussing key advantages and challenges for 4.0 technologies and recommendations for further developing cybersecurity at banks.",
        "field": "Augmented Reality (AR) in Banking",
        "link": "http://arxiv.org/abs/2503.00070v1"
    },
    {
        "id": "2503.10644v1",
        "title": "Combined climate stress testing of supply-chain networks and the financial system with nation-wide firm-level emission estimates",
        "authors": [
            "Zlata Tabachová",
            "Christian Diem",
            "Johannes Stangl",
            "András Borsos",
            "Stefan Thurner"
        ],
        "published": "2025-02-25T20:25:47Z",
        "summary": "On the way towards carbon neutrality, climate stress testing provides estimates for the physical and transition risks that climate change poses to the economy and the financial system. Missing firm-level CO2 emissions data severely impedes the assessment of transition risks originating from carbon pricing. Based on the individual emissions of all Hungarian firms (410,523), as estimated from their fossil fuel purchases, we conduct a stress test of both actual and hypothetical carbon pricing policies. Using a simple 1:1 economic ABM and introducing the new carbon-to-profit ratio, we identify firms that become unprofitable and default, and estimate the respective loan write-offs. We find that 45% of all companies are directly exposed to carbon pricing. At a price of 45 EUR/t, direct economic losses of 1.3% of total sales and bank equity losses of 1.2% are expected. Secondary default cascades in supply chain networks could increase these losses by 300% to 4000%, depending on firms' ability to substitute essential inputs. To reduce transition risks, firms should reduce their dependence on essential inputs from supply chains with high CO2 exposure. We discuss the implications of different policy implementations on these transition risks.",
        "field": "Augmented Reality (AR) in Banking",
        "link": "http://arxiv.org/abs/2503.10644v1"
    },
    {
        "id": "2502.14766v2",
        "title": "Multi-Layer Deep xVA: Structural Credit Models, Measure Changes and Convergence Analysis",
        "authors": [
            "Kristoffer Andersson",
            "Alessandro Gnoatto"
        ],
        "published": "2025-02-20T17:41:55Z",
        "summary": "We propose a structural default model for portfolio-wide valuation adjustments (xVAs) and represent it as a system of coupled backward stochastic differential equations. The framework is divided into four layers, each capturing a key component: (i) clean values, (ii) initial margin and Collateral Valuation Adjustment (ColVA), (iii) Credit/Debit Valuation Adjustments (CVA/DVA) together with Margin Valuation Adjustment (MVA), and (iv) Funding Valuation Adjustment (FVA). Because these layers depend on one another through collateral and default effects, a naive Monte Carlo approach would require deeply nested simulations, making the problem computationally intractable. To address this challenge, we use an iterative deep BSDE approach, handling each layer sequentially so that earlier outputs serve as inputs to the subsequent layers. Initial margin is computed via deep quantile regression to reflect margin requirements over the Margin Period of Risk. We also adopt a change-of-measure method that highlights rare but significant defaults of the bank or counterparty, ensuring that these events are accurately captured in the training process. We further extend Han and Long's (2020) a posteriori error analysis to BSDEs on bounded domains. Due to the random exit from the domain, we obtain an order of convergence of $\\mathcal{O}(h^{1/4-\\epsilon})$ rather than the usual $\\mathcal{O}(h^{1/2})$. Numerical experiments illustrate that this method drastically reduces computational demands and successfully scales to high-dimensional, non-symmetric portfolios. The results confirm its effectiveness and accuracy, offering a practical alternative to nested Monte Carlo simulations in multi-counterparty xVA analyses.",
        "field": "Augmented Reality (AR) in Banking",
        "link": "http://arxiv.org/abs/2502.14766v2"
    },
    {
        "id": "2503.09823v1",
        "title": "Data Traceability for Privacy Alignment",
        "authors": [
            "Kevin Liao",
            "Shreya Thipireddy",
            "Daniel Weitzner"
        ],
        "published": "2025-03-12T20:42:23Z",
        "summary": "This paper offers a new privacy approach for the growing ecosystem of services--ranging from open banking to healthcare--dependent on sensitive personal data sharing between individuals and third-parties. While these services offer significant benefits, individuals want control over their data, transparency regarding how their data is used, and accountability from third-parties for misuse. However, existing legal and technical mechanisms are inadequate for supporting these needs. A comprehensive approach to the modern privacy challenges of accountable third-party data sharing requires a closer alignment of technical system architecture and legal institutional design. In order to achieve this privacy alignment, we extend traditional security threat modeling and analysis to encompass a broader range of privacy notions than has been typically considered. In particular, we introduce the concept of covert-accountability, which addresses adversaries that may act dishonestly but face potential identification and legal consequences. As a concrete instance of this design approach, we present the OTrace protocol, designed to provide traceable, accountable, consumer-control in third-party data sharing ecosystems. OTrace empowers consumers with the knowledge of where their data is, who has it, what it is being used for, and whom it is being shared with. By applying our alignment framework to OTrace, we demonstrate that OTrace's technical affordances can provide more confident, scalable regulatory oversight when combined with complementary legal mechanisms.",
        "field": "Quantum Internet in Banking",
        "link": "http://arxiv.org/abs/2503.09823v1"
    },
    {
        "id": "2503.09586v1",
        "title": "Auspex: Building Threat Modeling Tradecraft into an Artificial Intelligence-based Copilot",
        "authors": [
            "Andrew Crossman",
            "Andrew R. Plummer",
            "Chandra Sekharudu",
            "Deepak Warrier",
            "Mohammad Yekrangian"
        ],
        "published": "2025-03-12T17:54:18Z",
        "summary": "We present Auspex - a threat modeling system built using a specialized collection of generative artificial intelligence-based methods that capture threat modeling tradecraft. This new approach, called tradecraft prompting, centers on encoding the on-the-ground knowledge of threat modelers within the prompts that drive a generative AI-based threat modeling system. Auspex employs tradecraft prompts in two processing stages. The first stage centers on ingesting and processing system architecture information using prompts that encode threat modeling tradecraft knowledge pertaining to system decomposition and description. The second stage centers on chaining the resulting system analysis through a collection of prompts that encode tradecraft knowledge on threat identification, classification, and mitigation. The two-stage process yields a threat matrix for a system that specifies threat scenarios, threat types, information security categorizations and potential mitigations. Auspex produces formalized threat model output in minutes, relative to the weeks or months a manual process takes. More broadly, the focus on bespoke tradecraft prompting, as opposed to fine-tuning or agent-based add-ons, makes Auspex a lightweight, flexible, modular, and extensible foundational system capable of addressing the complexity, resource, and standardization limitations of both existing manual and automated threat modeling processes. In this connection, we establish the baseline value of Auspex to threat modelers through an evaluation procedure based on feedback collected from cybersecurity subject matter experts measuring the quality and utility of threat models generated by Auspex on real banking systems. We conclude with a discussion of system performance and plans for enhancements to Auspex.",
        "field": "Quantum Internet in Banking",
        "link": "http://arxiv.org/abs/2503.09586v1"
    },
    {
        "id": "2503.00070v1",
        "title": "Systematic Review of Cybersecurity in Banking: Evolution from Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain, Policies and Practice",
        "authors": [
            "Tue Nhi Tran"
        ],
        "published": "2025-02-27T14:17:06Z",
        "summary": "Throughout the history from pre-industry 4.0 to post-industry 4.0, cybersecurity at banks has undergone significant changes. Pre-industry 4.0 cyber security at banks relied on individual security methods that were highly manual and had low accuracy. When moving to post-industry 4.0, cybersecurity at banks had a major turning point with security methods that combined different technologies such as Artificial Intelligence (AI), Blockchain, IoT, automating necessary processes and significantly increasing the defence layer for banks. However, along with the development of new technologies, the current challenge of cybersecurity at banks lies in scalability, high costs and resources in both money and time for R&D of defence methods along with the threat of high-tech cybercriminals growing and expanding. This report goes from introducing the importance of cybersecurity at banks, analyzing their management, operational and business objectives, evaluating pre-industry 4.0 technologies used for cybersecurity at banks to assessing post-industry 4.0 technologies focusing on Artificial Intelligence and Blockchain, discussing current policies and practices and ending with discussing key advantages and challenges for 4.0 technologies and recommendations for further developing cybersecurity at banks.",
        "field": "Quantum Internet in Banking",
        "link": "http://arxiv.org/abs/2503.00070v1"
    },
    {
        "id": "2503.10644v1",
        "title": "Combined climate stress testing of supply-chain networks and the financial system with nation-wide firm-level emission estimates",
        "authors": [
            "Zlata Tabachová",
            "Christian Diem",
            "Johannes Stangl",
            "András Borsos",
            "Stefan Thurner"
        ],
        "published": "2025-02-25T20:25:47Z",
        "summary": "On the way towards carbon neutrality, climate stress testing provides estimates for the physical and transition risks that climate change poses to the economy and the financial system. Missing firm-level CO2 emissions data severely impedes the assessment of transition risks originating from carbon pricing. Based on the individual emissions of all Hungarian firms (410,523), as estimated from their fossil fuel purchases, we conduct a stress test of both actual and hypothetical carbon pricing policies. Using a simple 1:1 economic ABM and introducing the new carbon-to-profit ratio, we identify firms that become unprofitable and default, and estimate the respective loan write-offs. We find that 45% of all companies are directly exposed to carbon pricing. At a price of 45 EUR/t, direct economic losses of 1.3% of total sales and bank equity losses of 1.2% are expected. Secondary default cascades in supply chain networks could increase these losses by 300% to 4000%, depending on firms' ability to substitute essential inputs. To reduce transition risks, firms should reduce their dependence on essential inputs from supply chains with high CO2 exposure. We discuss the implications of different policy implementations on these transition risks.",
        "field": "Quantum Internet in Banking",
        "link": "http://arxiv.org/abs/2503.10644v1"
    },
    {
        "id": "2502.14766v2",
        "title": "Multi-Layer Deep xVA: Structural Credit Models, Measure Changes and Convergence Analysis",
        "authors": [
            "Kristoffer Andersson",
            "Alessandro Gnoatto"
        ],
        "published": "2025-02-20T17:41:55Z",
        "summary": "We propose a structural default model for portfolio-wide valuation adjustments (xVAs) and represent it as a system of coupled backward stochastic differential equations. The framework is divided into four layers, each capturing a key component: (i) clean values, (ii) initial margin and Collateral Valuation Adjustment (ColVA), (iii) Credit/Debit Valuation Adjustments (CVA/DVA) together with Margin Valuation Adjustment (MVA), and (iv) Funding Valuation Adjustment (FVA). Because these layers depend on one another through collateral and default effects, a naive Monte Carlo approach would require deeply nested simulations, making the problem computationally intractable. To address this challenge, we use an iterative deep BSDE approach, handling each layer sequentially so that earlier outputs serve as inputs to the subsequent layers. Initial margin is computed via deep quantile regression to reflect margin requirements over the Margin Period of Risk. We also adopt a change-of-measure method that highlights rare but significant defaults of the bank or counterparty, ensuring that these events are accurately captured in the training process. We further extend Han and Long's (2020) a posteriori error analysis to BSDEs on bounded domains. Due to the random exit from the domain, we obtain an order of convergence of $\\mathcal{O}(h^{1/4-\\epsilon})$ rather than the usual $\\mathcal{O}(h^{1/2})$. Numerical experiments illustrate that this method drastically reduces computational demands and successfully scales to high-dimensional, non-symmetric portfolios. The results confirm its effectiveness and accuracy, offering a practical alternative to nested Monte Carlo simulations in multi-counterparty xVA analyses.",
        "field": "Quantum Internet in Banking",
        "link": "http://arxiv.org/abs/2502.14766v2"
    },
    {
        "id": "2503.14284v1",
        "title": "Entente: Cross-silo Intrusion Detection on Network Log Graphs with Federated Learning",
        "authors": [
            "Jiacen Xu",
            "Chenang Li",
            "Yu Zheng",
            "Zhou Li"
        ],
        "published": "2025-03-18T14:21:24Z",
        "summary": "Graph-based Network Intrusion Detection System (GNIDS) has gained significant momentum in detecting sophisticated cyber-attacks, like Advanced Persistent Threat (APT), in an organization or across organizations. Though achieving satisfying detection accuracy and adapting to ever-changing attacks and normal patterns, all prior GNIDSs assume the centralized data settings directly, but non-trivial data collection is not always practical under privacy regulations nowadays. We argue that training a GNIDS model has to consider privacy regulations, and propose to leverage federated learning (FL) to address this prominent challenge. Yet, directly applying FL to GNIDS is unlikely to succeed, due to issues like non-IID (independent and identically distributed) graph data over clients and the diverse design choices taken by different GNIDS. We address these issues with a set of novel techniques tailored to the graph datasets, including reference graph synthesis, graph sketching and adaptive contribution scaling, and develop a new system Entente. We evaluate Entente on the large-scale LANL, OpTC and Pivoting datasets. The result shows Entente outperforms the other baseline FL algorithms and sometimes even the non-FL GNIDS. We also evaluate Entente under FL poisoning attacks tailored to the GNIDS setting, and show Entente is able to bound the attack success rate to low values. Overall, our result suggests building cross-silo GNIDS is feasible and we hope to encourage more efforts in this direction.",
        "field": "Quantum-Enhanced Fraud Detection",
        "link": "http://arxiv.org/abs/2503.14284v1"
    },
    {
        "id": "2503.14281v1",
        "title": "XOXO: Stealthy Cross-Origin Context Poisoning Attacks against AI Coding Assistants",
        "authors": [
            "Adam Štorek",
            "Mukur Gupta",
            "Noopur Bhatt",
            "Aditya Gupta",
            "Janie Kim",
            "Prashast Srivastava",
            "Suman Jana"
        ],
        "published": "2025-03-18T14:20:54Z",
        "summary": "AI coding assistants are widely used for tasks like code generation, bug detection, and comprehension. These tools now require large and complex contexts, automatically sourced from various origins$\\unicode{x2014}$across files, projects, and contributors$\\unicode{x2014}$forming part of the prompt fed to underlying LLMs. This automatic context-gathering introduces new vulnerabilities, allowing attackers to subtly poison input to compromise the assistant's outputs, potentially generating vulnerable code, overlooking flaws, or introducing critical errors. We propose a novel attack, Cross-Origin Context Poisoning (XOXO), that is particularly challenging to detect as it relies on adversarial code modifications that are semantically equivalent. Traditional program analysis techniques struggle to identify these correlations since the semantics of the code remain correct, making it appear legitimate. This allows attackers to manipulate code assistants into producing incorrect outputs, including vulnerabilities or backdoors, while shifting the blame to the victim developer or tester. We introduce a novel, task-agnostic black-box attack algorithm GCGS that systematically searches the transformation space using a Cayley Graph, achieving an 83.09% attack success rate on average across five tasks and eleven models, including GPT-4o and Claude 3.5 Sonnet v2 used by many popular AI coding assistants. Furthermore, existing defenses, including adversarial fine-tuning, are ineffective against our attack, underscoring the need for new security measures in LLM-powered coding tools.",
        "field": "Quantum-Enhanced Fraud Detection",
        "link": "http://arxiv.org/abs/2503.14281v1"
    },
    {
        "id": "2503.14279v1",
        "title": "Anti-Tamper Radio meets Reconfigurable Intelligent Surface for System-Level Tamper Detection",
        "authors": [
            "Maryam Shaygan Tabar",
            "Johannes Kortz",
            "Paul Staat",
            "Harald Elders-Boll",
            "Christof Paar",
            "Christian Zenger"
        ],
        "published": "2025-03-18T14:18:31Z",
        "summary": "Many computing systems need to be protected against physical attacks using active tamper detection based on sensors. One technical solution is to employ an ATR (Anti-Tamper Radio) approach, analyzing the radio wave propagation effects within a protected device to detect unauthorized physical alterations. However, ATR systems face key challenges in terms of susceptibility to signal manipulation attacks, limited reliability due to environmental noise, and regulatory constraints from wide bandwidth usage. In this work, we propose and experimentally evaluate an ATR system complemented by an RIS to dynamically reconfigure the wireless propagation environment. We show that this approach can enhance resistance against signal manipulation attacks, reduce bandwidth requirements from several~GHz down to as low as 20 MHz, and improve robustness to environmental disturbances such as internal fan movements. Our work demonstrates that RIS integration can strengthen the ATR performance to enhance security, sensitivity, and robustness, recognizing the potential of smart radio environments for ATR-based tamper detection",
        "field": "Quantum-Enhanced Fraud Detection",
        "link": "http://arxiv.org/abs/2503.14279v1"
    },
    {
        "id": "2503.13419v1",
        "title": "Securing Virtual Reality Experiences: Unveiling and Tackling Cybersickness Attacks with Explainable AI",
        "authors": [
            "Ripan Kumar Kundu",
            "Matthew Denton",
            "Genova Mongalo",
            "Prasad Calyam",
            "Khaza Anuarul Hoque"
        ],
        "published": "2025-03-17T17:49:51Z",
        "summary": "The synergy between virtual reality (VR) and artificial intelligence (AI), specifically deep learning (DL)-based cybersickness detection models, has ushered in unprecedented advancements in immersive experiences by automatically detecting cybersickness severity and adaptively various mitigation techniques, offering a smooth and comfortable VR experience. While this DL-enabled cybersickness detection method provides promising solutions for enhancing user experiences, it also introduces new risks since these models are vulnerable to adversarial attacks; a small perturbation of the input data that is visually undetectable to human observers can fool the cybersickness detection model and trigger unexpected mitigation, thus disrupting user immersive experiences (UIX) and even posing safety risks. In this paper, we present a new type of VR attack, i.e., a cybersickness attack, which successfully stops the triggering of cybersickness mitigation by fooling DL-based cybersickness detection models and dramatically hinders the UIX. Next, we propose a novel explainable artificial intelligence (XAI)-guided cybersickness attack detection framework to detect such attacks in VR to ensure UIX and a comfortable VR experience. We evaluate the proposed attack and the detection framework using two state-of-the-art open-source VR cybersickness datasets: Simulation 2021 and Gameplay dataset. Finally, to verify the effectiveness of our proposed method, we implement the attack and the XAI-based detection using a testbed with a custom-built VR roller coaster simulation with an HTC Vive Pro Eye headset and perform a user study. Our study shows that such an attack can dramatically hinder the UIX. However, our proposed XAI-guided cybersickness attack detection can successfully detect cybersickness attacks and trigger the proper mitigation, effectively reducing VR cybersickness.",
        "field": "Quantum-Enhanced Fraud Detection",
        "link": "http://arxiv.org/abs/2503.13419v1"
    },
    {
        "id": "2503.13572v1",
        "title": "VeriContaminated: Assessing LLM-Driven Verilog Coding for Data Contamination",
        "authors": [
            "Zeng Wang",
            "Minghao Shao",
            "Jitendra Bhandari",
            "Likhitha Mankali",
            "Ramesh Karri",
            "Ozgur Sinanoglu",
            "Muhammad Shafique",
            "Johann Knechtel"
        ],
        "published": "2025-03-17T12:26:49Z",
        "summary": "Large Language Models (LLMs) have revolutionized code generation, achieving exceptional results on various established benchmarking frameworks. However, concerns about data contamination - where benchmark data inadvertently leaks into pre-training or fine-tuning datasets - raise questions about the validity of these evaluations. While this issue is known, limiting the industrial adoption of LLM-driven software engineering, hardware coding has received little to no attention regarding these risks. For the first time, we analyze state-of-the-art (SOTA) evaluation frameworks for Verilog code generation (VerilogEval and RTLLM), using established methods for contamination detection (CCD and Min-K% Prob). We cover SOTA commercial and open-source LLMs (CodeGen2.5, Minitron 4b, Mistral 7b, phi-4 mini, LLaMA-{1,2,3.1}, GPT-{2,3.5,4o}, Deepseek-Coder, and CodeQwen 1.5), in baseline and fine-tuned models (RTLCoder and Verigen). Our study confirms that data contamination is a critical concern. We explore mitigations and the resulting trade-offs for code quality vs fairness (i.e., reducing contamination toward unbiased benchmarking).",
        "field": "Quantum-Enhanced Fraud Detection",
        "link": "http://arxiv.org/abs/2503.13572v1"
    },
    {
        "id": "2503.14462v1",
        "title": "Blockchain with proof of quantum work",
        "authors": [
            "Mohammad H. Amin",
            "Jack Raymond",
            "Daniel Kinn",
            "Firas Hamze",
            "Kelsey Hamer",
            "Joel Pasvolsky",
            "William Bernoudy",
            "Andrew D. King",
            "Samuel Kortas"
        ],
        "published": "2025-03-18T17:37:22Z",
        "summary": "We propose a blockchain architecture in which mining requires a quantum computer. The consensus mechanism is based on proof of quantum work, a quantum-enhanced alternative to traditional proof of work that leverages quantum supremacy to make mining intractable for classical computers. We have refined the blockchain framework to incorporate the probabilistic nature of quantum mechanics, ensuring stability against sampling errors and hardware inaccuracies. To validate our approach, we implemented a prototype blockchain on four D-Wave$^{\\rm TM}$ quantum annealing processors geographically distributed within North America, demonstrating stable operation across hundreds of thousands of quantum hashing operations. Our experimental protocol follows the same approach used in the recent demonstration of quantum supremacy [1], ensuring that classical computers cannot efficiently perform the same computation task. By replacing classical machines with quantum systems for mining, it is possible to significantly reduce the energy consumption and environmental impact traditionally associated with blockchain mining. Beyond serving as a proof of concept for a meaningful application of quantum computing, this work highlights the potential for other near-term quantum computing applications using existing technology.",
        "field": "Quantum-Resistant Ledger Technologies",
        "link": "http://arxiv.org/abs/2503.14462v1"
    },
    {
        "id": "2503.14006v1",
        "title": "Securing Automated Insulin Delivery Systems: A Review of Security Threats and Protectives Strategies",
        "authors": [
            "Yuchen Niu",
            "Siew-Kei Lam"
        ],
        "published": "2025-03-18T08:11:19Z",
        "summary": "Automated insulin delivery (AID) systems have emerged as a significant technological advancement in diabetes care. These systems integrate a continuous glucose monitor, an insulin pump, and control algorithms to automate insulin delivery, reducing the burden of self-management and offering enhanced glucose control. However, the increasing reliance on wireless connectivity and software control has exposed AID systems to critical security risks that could result in life-threatening treatment errors. This review first presents a comprehensive examination of the security landscape, covering technical vulnerabilities, legal frameworks, and commercial product considerations, and an analysis of existing research on attack vectors, defence mechanisms, as well as evaluation methods and resources for AID systems. Despite recent advancements, several open challenges remain in achieving secure AID systems, particularly in standardising security evaluation frameworks and developing comprehensive, lightweight, and adaptive defence strategies. As one of the most widely adopted and extensively studied physiologic closed-loop control systems, this review serves as a valuable reference for understanding security challenges and solutions applicable to analogous medical systems.",
        "field": "Quantum-Resistant Ledger Technologies",
        "link": "http://arxiv.org/abs/2503.14006v1"
    },
    {
        "id": "2503.13419v1",
        "title": "Securing Virtual Reality Experiences: Unveiling and Tackling Cybersickness Attacks with Explainable AI",
        "authors": [
            "Ripan Kumar Kundu",
            "Matthew Denton",
            "Genova Mongalo",
            "Prasad Calyam",
            "Khaza Anuarul Hoque"
        ],
        "published": "2025-03-17T17:49:51Z",
        "summary": "The synergy between virtual reality (VR) and artificial intelligence (AI), specifically deep learning (DL)-based cybersickness detection models, has ushered in unprecedented advancements in immersive experiences by automatically detecting cybersickness severity and adaptively various mitigation techniques, offering a smooth and comfortable VR experience. While this DL-enabled cybersickness detection method provides promising solutions for enhancing user experiences, it also introduces new risks since these models are vulnerable to adversarial attacks; a small perturbation of the input data that is visually undetectable to human observers can fool the cybersickness detection model and trigger unexpected mitigation, thus disrupting user immersive experiences (UIX) and even posing safety risks. In this paper, we present a new type of VR attack, i.e., a cybersickness attack, which successfully stops the triggering of cybersickness mitigation by fooling DL-based cybersickness detection models and dramatically hinders the UIX. Next, we propose a novel explainable artificial intelligence (XAI)-guided cybersickness attack detection framework to detect such attacks in VR to ensure UIX and a comfortable VR experience. We evaluate the proposed attack and the detection framework using two state-of-the-art open-source VR cybersickness datasets: Simulation 2021 and Gameplay dataset. Finally, to verify the effectiveness of our proposed method, we implement the attack and the XAI-based detection using a testbed with a custom-built VR roller coaster simulation with an HTC Vive Pro Eye headset and perform a user study. Our study shows that such an attack can dramatically hinder the UIX. However, our proposed XAI-guided cybersickness attack detection can successfully detect cybersickness attacks and trigger the proper mitigation, effectively reducing VR cybersickness.",
        "field": "Quantum-Resistant Ledger Technologies",
        "link": "http://arxiv.org/abs/2503.13419v1"
    },
    {
        "id": "2503.13255v1",
        "title": "Zero-Knowledge Proof-Based Consensus for Blockchain-Secured Federated Learning",
        "authors": [
            "Tianxing Fu",
            "Jia Hu",
            "Geyong Min",
            "Zi Wang"
        ],
        "published": "2025-03-17T15:13:10Z",
        "summary": "Federated learning (FL) enables multiple participants to collaboratively train machine learning models while ensuring their data remains private and secure. Blockchain technology further enhances FL by providing stronger security, a transparent audit trail, and protection against data tampering and model manipulation. Most blockchain-secured FL systems rely on conventional consensus mechanisms: Proof-of-Work (PoW) is computationally expensive, while Proof-of-Stake (PoS) improves energy efficiency but risks centralization as it inherently favors participants with larger stakes. Recently, learning-based consensus has emerged as an alternative by replacing cryptographic tasks with model training to save energy. However, this approach introduces potential privacy vulnerabilities, as the training process may inadvertently expose sensitive information through gradient sharing and model updates. To address these challenges, we propose a novel Zero-Knowledge Proof of Training (ZKPoT) consensus mechanism. This method leverages the zero-knowledge succinct non-interactive argument of knowledge proof (zk-SNARK) protocol to validate participants' contributions based on their model performance, effectively eliminating the inefficiencies of traditional consensus methods and mitigating the privacy risks posed by learning-based consensus. We analyze our system's security, demonstrating its capacity to prevent the disclosure of sensitive information about local models or training data to untrusted parties during the entire FL process. Extensive experiments demonstrate that our system is robust against privacy and Byzantine attacks while maintaining accuracy and utility without trade-offs, scalable across various blockchain settings, and efficient in both computation and communication.",
        "field": "Quantum-Resistant Ledger Technologies",
        "link": "http://arxiv.org/abs/2503.13255v1"
    },
    {
        "id": "2503.12958v1",
        "title": "FedSDP: Explainable Differential Privacy in Federated Learning via Shapley Values",
        "authors": [
            "Yunbo Li",
            "Jiaping Gui",
            "Yue Wu"
        ],
        "published": "2025-03-17T09:14:19Z",
        "summary": "Federated learning (FL) enables participants to store data locally while collaborating in training, yet it remains vulnerable to privacy attacks, such as data reconstruction. Existing differential privacy (DP) technologies inject noise dynamically into the training process to mitigate the impact of excessive noise. However, this dynamic scheduling is often grounded in factors indirectly related to privacy, making it difficult to clearly explain the intricate relationship between dynamic noise adjustments and privacy requirements. To address this issue, we propose FedSDP, a novel and explainable DP-based privacy protection mechanism that guides noise injection based on privacy contribution. Specifically, FedSDP leverages Shapley values to assess the contribution of private attributes to local model training and dynamically adjusts the amount of noise injected accordingly. By providing theoretical insights into the injection of varying scales of noise into local training, FedSDP enhances interpretability. Extensive experiments demonstrate that FedSDP can achieve a superior balance between privacy preservation and model performance, surpassing state-of-the-art (SOTA) solutions.",
        "field": "Quantum-Resistant Ledger Technologies",
        "link": "http://arxiv.org/abs/2503.12958v1"
    },
    {
        "id": "2503.10269v1",
        "title": "Targeted Data Poisoning for Black-Box Audio Datasets Ownership Verification",
        "authors": [
            "Wassim Bouaziz",
            "El-Mahdi El-Mhamdi",
            "Nicolas Usunier"
        ],
        "published": "2025-03-13T11:25:25Z",
        "summary": "Protecting the use of audio datasets is a major concern for data owners, particularly with the recent rise of audio deep learning models. While watermarks can be used to protect the data itself, they do not allow to identify a deep learning model trained on a protected dataset. In this paper, we adapt to audio data the recently introduced data taggants approach. Data taggants is a method to verify if a neural network was trained on a protected image dataset with top-$k$ predictions access to the model only. This method relies on a targeted data poisoning scheme by discreetly altering a small fraction (1%) of the dataset as to induce a harmless behavior on out-of-distribution data called keys. We evaluate our method on the Speechcommands and the ESC50 datasets and state of the art transformer models, and show that we can detect the use of the dataset with high confidence without loss of performance. We also show the robustness of our method against common data augmentation techniques, making it a practical method to protect audio datasets.",
        "field": "Digital Identity Verification",
        "link": "http://arxiv.org/abs/2503.10269v1"
    },
    {
        "id": "2503.10171v1",
        "title": "Verifiable, Efficient and Confidentiality-Preserving Graph Search with Transparency",
        "authors": [
            "Qiuhao Wang",
            "Xu Yang",
            "Yiwei Liu",
            "Saiyu Qi",
            "Hongguang Zhao",
            "Ke Li",
            "Yong Qi"
        ],
        "published": "2025-03-13T08:53:53Z",
        "summary": "Graph databases have garnered extensive attention and research due to their ability to manage relationships between entities efficiently. Today, many graph search services have been outsourced to a third-party server to facilitate storage and computational support. Nevertheless, the outsourcing paradigm may invade the privacy of graphs. PeGraph is the latest scheme achieving encrypted search over social graphs to address the privacy leakage, which maintains two data structures XSet and TSet motivated by the OXT technology to support encrypted conjunctive search. However, PeGraph still exhibits limitations inherent to the underlying OXT. It does not provide transparent search capabilities, suffers from expensive computation and result pattern leakages, and it fails to support search over dynamic encrypted graph database and results verification. In this paper, we propose SecGraph to address the first two limitations, which adopts a novel system architecture that leverages an SGX-enabled cloud server to provide users with secure and transparent search services since the secret key protection and computational overhead have been offloaded to the cloud server. Besides, we design an LDCF-encoded XSet based on the Logarithmic Dynamic Cuckoo Filter to facilitate efficient plaintext computation in trusted memory, effectively mitigating the risks of result pattern leakage and performance degradation due to exceeding the limited trusted memory capacity. Finally, we design a new dynamic version of TSet named Twin-TSet to enable conjunctive search over dynamic encrypted graph database. In order to support verifiable search, we further propose VSecGraph, which utilizes a procedure-oriented verification method to verify all data structures loaded into the trusted memory, thus bypassing the computational overhead associated with the client's local verification.",
        "field": "Digital Identity Verification",
        "link": "http://arxiv.org/abs/2503.10171v1"
    },
    {
        "id": "2503.09513v1",
        "title": "RESTRAIN: Reinforcement Learning-Based Secure Framework for Trigger-Action IoT Environment",
        "authors": [
            "Md Morshed Alam",
            "Lokesh Chandra Das",
            "Sandip Roy",
            "Sachin Shetty",
            "Weichao Wang"
        ],
        "published": "2025-03-12T16:23:14Z",
        "summary": "Internet of Things (IoT) platforms with trigger-action capability allow event conditions to trigger actions in IoT devices autonomously by creating a chain of interactions. Adversaries exploit this chain of interactions to maliciously inject fake event conditions into IoT hubs, triggering unauthorized actions on target IoT devices to implement remote injection attacks. Existing defense mechanisms focus mainly on the verification of event transactions using physical event fingerprints to enforce the security policies to block unsafe event transactions. These approaches are designed to provide offline defense against injection attacks. The state-of-the-art online defense mechanisms offer real-time defense, but extensive reliability on the inference of attack impacts on the IoT network limits the generalization capability of these approaches. In this paper, we propose a platform-independent multi-agent online defense system, namely RESTRAIN, to counter remote injection attacks at runtime. RESTRAIN allows the defense agent to profile attack actions at runtime and leverages reinforcement learning to optimize a defense policy that complies with the security requirements of the IoT network. The experimental results show that the defense agent effectively takes real-time defense actions against complex and dynamic remote injection attacks and maximizes the security gain with minimal computational overhead.",
        "field": "Digital Identity Verification",
        "link": "http://arxiv.org/abs/2503.09513v1"
    },
    {
        "id": "2503.09433v1",
        "title": "CASTLE: Benchmarking Dataset for Static Code Analyzers and LLMs towards CWE Detection",
        "authors": [
            "Richard A. Dubniczky",
            "Krisztofer Zoltán Horvát",
            "Tamás Bisztray",
            "Mohamed Amine Ferrag",
            "Lucas C. Cordeiro",
            "Norbert Tihanyi"
        ],
        "published": "2025-03-12T14:30:05Z",
        "summary": "Identifying vulnerabilities in source code is crucial, especially in critical software components. Existing methods such as static analysis, dynamic analysis, formal verification, and recently Large Language Models are widely used to detect security flaws. This paper introduces CASTLE (CWE Automated Security Testing and Low-Level Evaluation), a benchmarking framework for evaluating the vulnerability detection capabilities of different methods. We assess 13 static analysis tools, 10 LLMs, and 2 formal verification tools using a hand-crafted dataset of 250 micro-benchmark programs covering 25 common CWEs. We propose the CASTLE Score, a novel evaluation metric to ensure fair comparison. Our results reveal key differences: ESBMC (a formal verification tool) minimizes false positives but struggles with vulnerabilities beyond model checking, such as weak cryptography or SQL injection. Static analyzers suffer from high false positives, increasing manual validation efforts for developers. LLMs perform exceptionally well in the CASTLE dataset when identifying vulnerabilities in small code snippets. However, their accuracy declines, and hallucinations increase as the code size grows. These results suggest that LLMs could play a pivotal role in future security solutions, particularly within code completion frameworks, where they can provide real-time guidance to prevent vulnerabilities. The dataset is accessible at https://github.com/CASTLE-Benchmark.",
        "field": "Digital Identity Verification",
        "link": "http://arxiv.org/abs/2503.09433v1"
    },
    {
        "id": "2503.14006v1",
        "title": "Securing Automated Insulin Delivery Systems: A Review of Security Threats and Protectives Strategies",
        "authors": [
            "Yuchen Niu",
            "Siew-Kei Lam"
        ],
        "published": "2025-03-18T08:11:19Z",
        "summary": "Automated insulin delivery (AID) systems have emerged as a significant technological advancement in diabetes care. These systems integrate a continuous glucose monitor, an insulin pump, and control algorithms to automate insulin delivery, reducing the burden of self-management and offering enhanced glucose control. However, the increasing reliance on wireless connectivity and software control has exposed AID systems to critical security risks that could result in life-threatening treatment errors. This review first presents a comprehensive examination of the security landscape, covering technical vulnerabilities, legal frameworks, and commercial product considerations, and an analysis of existing research on attack vectors, defence mechanisms, as well as evaluation methods and resources for AID systems. Despite recent advancements, several open challenges remain in achieving secure AID systems, particularly in standardising security evaluation frameworks and developing comprehensive, lightweight, and adaptive defence strategies. As one of the most widely adopted and extensively studied physiologic closed-loop control systems, this review serves as a valuable reference for understanding security challenges and solutions applicable to analogous medical systems.",
        "field": "Quantum-Driven Risk Management",
        "link": "http://arxiv.org/abs/2503.14006v1"
    },
    {
        "id": "2503.13056v1",
        "title": "Deep Hedging of Green PPAs in Electricity Markets",
        "authors": [
            "Richard Biegler-König",
            "Daniel Oeltz"
        ],
        "published": "2025-03-17T11:02:23Z",
        "summary": "In power markets, Green Power Purchase Agreements have become an important contractual tool of the energy transition from fossil fuels to renewable sources such as wind or solar radiation. Trading Green PPAs exposes agents to price risks and weather risks. Also, developed electricity markets feature the so-called cannibalisation effect : large infeeds induce low prices and vice versa. As weather is a non-tradable entity the question arises how to hedge and risk-manage in this highly incom-plete setting. We propose a ''deep hedging'' framework utilising machine learning methods to construct hedging strategies. The resulting strategies outperform static and dynamic benchmark strategies with respect to different risk measures.",
        "field": "Quantum-Driven Risk Management",
        "link": "http://arxiv.org/abs/2503.13056v1"
    },
    {
        "id": "2503.13544v1",
        "title": "Semi-Decision-Focused Learning with Deep Ensembles: A Practical Framework for Robust Portfolio Optimization",
        "authors": [
            "Juhyeong Kim"
        ],
        "published": "2025-03-16T10:57:45Z",
        "summary": "I propose Semi-Decision-Focused Learning, a practical adaptation of Decision-Focused Learning for portfolio optimization. Rather than directly optimizing complex financial metrics, I employ simple target portfolios (Max-Sortino or One-Hot) and train models with a convex, cross-entropy loss. I further incorporate Deep Ensemble methods to reduce variance and stabilize performance. Experiments on two universes (one upward-trending and another range-bound) show consistent outperformance over baseline portfolios, demonstrating the effectiveness and robustness of my approach. Code is available at https://github.com/sDFLwDE/sDFLwDE",
        "field": "Quantum-Driven Risk Management",
        "link": "http://arxiv.org/abs/2503.13544v1"
    },
    {
        "id": "2503.11963v1",
        "title": "Effective and Efficient Cross-City Traffic Knowledge Transfer A Privacy-Preserving Perspective",
        "authors": [
            "Zhihao Zeng",
            "Ziquan Fang",
            "Yuting Huang",
            "Lu Chen",
            "Yunjun Gao"
        ],
        "published": "2025-03-15T02:26:24Z",
        "summary": "Traffic prediction targets forecasting future traffic conditions using historical traffic data, serving a critical role in urban computing and transportation management. To mitigate the scarcity of traffic data while maintaining data privacy, numerous Federated Traffic Knowledge Transfer (FTT) approaches have been developed, which use transfer learning and federated learning to transfer traffic knowledge from data-rich cities to data-scarce cities, enhancing traffic prediction capabilities for the latter. However, current FTT approaches face challenges such as privacy leakage, cross-city data distribution discrepancies, low data quality, and inefficient knowledge transfer, limiting their privacy protection, effectiveness, robustness, and efficiency in real-world applications. To this end, we propose FedTT, an effective, efficient, and privacy-aware cross-city traffic knowledge transfer framework that transforms the traffic data domain from the data-rich cities and trains traffic models using the transformed data for the data-scarce cities. First, to safeguard data privacy, we propose a traffic secret transmission method that securely transmits and aggregates traffic domain-transformed data from source cities using a lightweight secret aggregation approach. Second, to mitigate the impact of traffic data distribution discrepancies on model performance, we introduce a traffic domain adapter to uniformly transform traffic data from the source cities' domains to that of the target city. Third, to improve traffic data quality, we design a traffic view imputation method to fill in and predict missing traffic data. Finally, to enhance transfer efficiency, FedTT is equipped with a federated parallel training method that enables the simultaneous training of multiple modules. Extensive experiments using 4 real-life datasets demonstrate that FedTT outperforms the 14 state-of-the-art baselines.",
        "field": "Quantum-Driven Risk Management",
        "link": "http://arxiv.org/abs/2503.11963v1"
    },
    {
        "id": "2503.11940v1",
        "title": "Vote Delegation in DeFi Governance",
        "authors": [
            "Dion Bongaerts",
            "Thomas Lambert",
            "Daniel Liebau",
            "Peter Roosenboom"
        ],
        "published": "2025-03-15T01:15:08Z",
        "summary": "We investigate the drivers of vote delegation in Decentralized Autonomous Organizations (DAOs), using the Uniswap governance DAO as a laboratory. We show that parties with fewer self-owned votes and those affiliated with the controlling venture capital firm, Andreesen Horowitz (a16z), receive more vote delegations. These patterns suggest that while the Uniswap ecosystem values decentralization, a16z may engage in window-dressing around it. Moreover, we find that an active and successful track record in submitting improvement proposals, especially in the final stage, leads to more vote delegations, indicating that delegation in DAOs is at least partly reputation- or merit-based. Combined, our findings provide new insights into how governance and decentralization operate in DeFi.",
        "field": "Quantum-Driven Risk Management",
        "link": "http://arxiv.org/abs/2503.11940v1"
    }
]